[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "geocompx",
    "section": "",
    "text": "Geocomputation with R\n\n\n Geocomputation with Python"
  },
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "geocompx",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nAutomatic website with the Geocomputation with R solutions\n\n\n\n\n\n\n\nvignette\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nFeb 24, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\nProgress update: Geocomputation with R Second Edition Part 1\n\n\n\n\n\n\n\nupdates\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nJan 27, 2022\n\n\n4 min\n\n\n\n\n\n\n\n\nGeocomputation with R: Second Edition feedback\n\n\n\n\n\n\n\nannouncement\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nSep 6, 2021\n\n\n1 min\n\n\n\n\n\n\n\n\nConversions between different spatial classes in R\n\n\n\n\n\n\n\nvignette\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nJun 17, 2021\n\n\n5 min\n\n\n\n\n\n\n\n\nRecent changes in R spatial and how to be ready for them\n\n\n\n\n\n\n\nannouncement\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2020\n\n\n1 min\n\n\n\n\n\n\n\n\nDemo of reproducible geographic data analysis: mapping Covid-19 data with R\n\n\n\n\n\n\n\ndemo\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2020\n\n\n9 min\n\n\n\n\n\n\n\n\nInstalling spatial R packages on Ubuntu\n\n\n\n\n\n\n\nsetup\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nMar 30, 2020\n\n\n11 min\n\n\n\n\n\n\n\n\nInset maps with ggplot2\n\n\n\n\n\n\n\nvignette\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2019\n\n\n8 min\n\n\n\n\n\n\n\n\nMap coloring: the color scale styles available in the tmap package\n\n\n\n\n\n\n\nvignette\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2019\n\n\n9 min\n\n\n\n\n\n\n\n\nGrids and graticules in the tmap package\n\n\n\n\n\n\n\nvignette\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2019\n\n\n2 min\n\n\n\n\n\n\n\n\nGeographic projections and transformations\n\n\n\n\n\n\n\nvignette\n\n\nrstats\n\n\n\n\n\n\n\n\n\n\n\nAug 21, 2019\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "post/2019/crs-projections-transformations/index.html",
    "href": "post/2019/crs-projections-transformations/index.html",
    "title": "Geographic projections and transformations",
    "section": "",
    "text": "Introduction\nThis workbook outlines key concepts and functions related to map projections — also referred to as coordinate reference systems (CRSs) — and transformation of geographic data from one projection to another. It is based on the open source book Geocomputation with R, and Chapter 6 in particular.\nIt was developed for the ‘CASA Summer School’, or the Doctoral Summer School for Advanced Spatial Modelling: Skills Workshop and Hackathon, 21st to 23rd August 2019, for its full name! It should be of use to anyone interested in projections, beyond the summer school, so we posted it on our newly updated website for maximum benefit.\n\n\nPrerequisites\nBefore you get started, make sure you have the packages installed:\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(spData)\n\n\n\nIntroduction\nBefore we get started, why use R for geographic data?\nSimple answer: because it works, excels at spatial statistics and visualisation and has a huge user community.\nIt can be used for a wide range of things such as:\n\nBook on Geocomputation: https://geocompr.robinlovelace.net/\nPropensity to Cycle Tool: https://pct.bike/\n\nGeographic data relies on a frame of reference. There are two main types of CRS:\n\nGeographic, where the frame of reference is the globe and how many degrees north or east from the position (0, 0) you are\nProjected, where the frame of reference is a flat representation of part of the Earth’s surface\n\n\n\n\n\n\nIllustration of vector (point) data in which location of London (the red X) is represented with reference to an origin (the blue circle). The left plot represents a geographic CRS with an origin at 0° longitude and latitude. The right plot represents a projected CRS with an origin located in the sea west of the South West Peninsula.\n\n\n\n\n\n\n\nIllustration of vector (point) data in which location of London (the red X) is represented with reference to an origin (the blue circle). The left plot represents a geographic CRS with an origin at 0° longitude and latitude. The right plot represents a projected CRS with an origin located in the sea west of the South West Peninsula.\n\n\n\n\n\n\nTLDR\nThe ‘too long didn’t read’ (TLDR) take away messages from this text are:\n\nBe aware of projections\nDecide an appropriate CRS for your project and ensure everything is in that CRS\nUse a projected CRS when doing geometric operations\nEPSG codes such as 27700 and 4326 refer to specific coordinate systems\nIt is worth checking if there is an officially supported projection for the region — that is often a good option\n\nIn R, you can check, set and transform CRS with st_crs() and st_transform() as follows:\n\nzones_london = lnd\nst_crs(zones_london)                                         # find out the CRS\n#> Coordinate Reference System:\n#>   User input: EPSG:4326 \n#>   wkt:\n#> GEOGCS[\"WGS 84\",\n#>     DATUM[\"WGS_1984\",\n#>         SPHEROID[\"WGS 84\",6378137,298.257223563,\n#>             AUTHORITY[\"EPSG\",\"7030\"]],\n#>         AUTHORITY[\"EPSG\",\"6326\"]],\n#>     PRIMEM[\"Greenwich\",0,\n#>         AUTHORITY[\"EPSG\",\"8901\"]],\n#>     UNIT[\"degree\",0.0174532925199433,\n#>         AUTHORITY[\"EPSG\",\"9122\"]],\n#>     AUTHORITY[\"EPSG\",\"4326\"]]\nzones_london_projected = st_transform(zones_london, 27700)   # transform CRS\nst_crs(zones_london) = NA                                    # set CRS\nst_crs(zones_london) = 4326                                  # set CRS\n\nIf you ignore CRSs, bad things can happen.\n\n\nWhy are projections needed?\nWithout a coordinate system, we have no context:\n\n\n\n\n\nWhich country is it?\nLocation on Earth is measured in degrees so, even when axes are equal, maps are highly distorted representations of reality far from the equator:\n\n\n\n\n\nAlong which axis is the image over-represented?\nEven when we compensate for this, the cylindrical projection is misleading:\n\n\n\n\n\nBy default, most software (including R) plots data with geographic lon/lat coordinates a cylindrical projection, leading to maps like this:\n\nplot(canada$geom)\n\n\n\nplot(world)\n#> Warning: plotting the first 9 out of 10 attributes; use max.plot = 10 to plot\n#> all\n\n\n\n\nAcross the whole world, the results look like this:\n\nplot(st_geometry(world), col = \"grey\")\n\n\n\n\nThere is no single ‘correct’ CRS that can represent everywhere well: it is physically impossible to ‘peal’ the surface of the Earth onto a flat screen (credit: Awar Jahfar):\n\n\n\n\n\n\nAt best we can comply with two out of three spatial properties (distance, area, direction). Therefore, the task at hand determines which projection to choose. For instance, if we are interested in a density (points per grid cell or inhabitants per grid cell), we should use an equal-area projection.\n\n\nThere is also a fourth property, shape.\n\n\n\nWhich projection to use?\nA range of CRSs is available:\n\nA Lambert azimuthal equal-area (LAEA) projection for a custom local projection (set lon_0 and lat_0 to the center of the study area), which is an equal-area projection at all locations but distorts shapes beyond thousands of kilometers.\nAzimuthal equidistant (AEQD) projections for a specifically accurate straight-line distance between a point and the center point of the local projection.\nLambert conformal conic (LCC) projections for regions covering thousands of kilometers, with the cone set to keep distance and area properties reasonable between the secant lines.\nStereographic (STERE) projections for polar regions, but taking care not to rely on area and distance calculations thousands of kilometers from the center.\n\nThis is how it works in R:\n\nworld_laea1_g = world %>%\n  st_transform(\"+proj=laea +x_0=0 +y_0=0 +lon_0=0 +lat_0=0\") %>% \n  st_geometry()\nplot(world_laea1_g)\n\n\n\n\n\nworld %>%\n  st_transform(\"+proj=aeqd +x_0=0 +y_0=0 +lon_0=0 +lat_0=0\") %>% \n  st_geometry() %>% \n  plot()\n\n\n\n\n\nworld %>%\n  st_transform(\"+proj=moll\") %>% \n  st_geometry() %>% \n  plot()\n\n\n\n\nHow to add graticules?\n\nworld %>%\n  st_transform(\"+proj=moll\") %>% \n  st_geometry() %>% \n  plot()\ng = st_graticule(x = world) %>% \n  st_transform(\"+proj=moll\") %>% \n  st_geometry()\nplot(g, add = TRUE)\n\n\n\n\n\ncanada_centroid = st_coordinates(st_centroid(canada))\n#> Warning in st_centroid.sf(canada): st_centroid assumes attributes are constant\n#> over geometries of x\ncanada_laea_crs = paste0(\"+proj=laea +x_0=0 +y_0=0 +lon_0=\",\n                         canada_centroid[1],\n                         \" +lat_0=\",\n                         canada_centroid[2])\ncanada_laea = st_transform(canada, crs = canada_laea_crs)\nworld_laea = st_transform(world, crs = canada_laea_crs)\nplot(st_geometry(canada_laea))\nplot(world_laea, add = TRUE)\n#> Warning in plot.sf(world_laea, add = TRUE): ignoring all but the first attribute\n\n\n\n\n\ncanada_centroid = st_coordinates(st_centroid(canada))\n#> Warning in st_centroid.sf(canada): st_centroid assumes attributes are constant\n#> over geometries of x\ncanada_laea_crs = paste0(\"+proj=laea +x_0=0 +y_0=0 +lon_0=\",\n                         canada_centroid[1],\n                         \" +lat_0=\",\n                         canada_centroid[2])\ncanada_laea = st_transform(canada, crs = canada_laea_crs)\nworld_laea = st_transform(world, crs = canada_laea_crs)\nplot(st_geometry(canada_laea))\nplot(world_laea, add = TRUE)\n#> Warning in plot.sf(world_laea, add = TRUE): ignoring all but the first attribute\n\n\n\n\n\n\nEPSG codes\nEPSG codes are standard codes for projections. See them in R with:\n\nepsg_codes = rgdal::make_EPSG()\n# View(epsg_codes) # open in interactive spreadsheet\n\nIn the UK, the EPSG code of official data is 27700.\n\n\nGeographic data in R\n\nlondon_df = data.frame(name = \"london\", population = 1e7,\n                       lon = -0.1, lat = 51.5)\nclass(london_df)\n#> [1] \"data.frame\"\nlondon = st_as_sf(london_df, coords = c(\"lon\", \"lat\"))\nclass(london)\n#> [1] \"sf\"         \"data.frame\"\nst_is_longlat(london)\n#> [1] NA\nplot(zones_london_projected$geometry)\nplot(london$geometry, add = TRUE, pch = 9) # not there\n\n\n\n\n\n\nIssues with geometric operations\n\nlondon_buff1 = st_buffer(london, 0.1)\nplot(london_buff1)\n\n\n\nplot(zones_london$geometry)\nplot(london_buff1, add = T)\n#> Warning in plot.sf(london_buff1, add = T): ignoring all but the first attribute\n\n\n\n\n\nst_crs(london) = 4326\nlondon_projected = st_transform(london, 27700)\nlondon_buff2 = st_buffer(london_projected, 10000)\nst_is_longlat(london_projected)\n#> [1] FALSE\nplot(zones_london_projected$geometry)\nplot(london_buff2, add = TRUE)\n#> Warning in plot.sf(london_buff2, add = TRUE): ignoring all but the first\n#> attribute\n\n\n\n\n\n\nFurther reading\nIf you’re interested in learning more on this, check out Geocomputations with R.\n\n\n\n\n\nMore specific resources on projections include:\n\nExcellent tutorial on coordinate systems on the Manifold website: http://www.manifold.net/doc/mfd9/projections_tutorial.htm\nAn introduction to vector geographic data in Geocomputation with R (Section 2.2)\nAn introduction to CRSs in R (Section 2.4)\nThe contents and exercises of Chapter 6, solutions to which you can find at https://geocompr.github.io/geocompkg/articles/index.html\nFor a fun take on projections, see https://xkcd.com/977/\nChapter in upcoming book on CRSs by Edzer Pebesma and Roger Bivand: https://github.com/edzer/sdsr\n\nCheck out the questions in the exercises section of Chapter 6 of Geocomputation with R.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{lovelace2019,\n  author = {Robin Lovelace},\n  title = {Geographic Projections and Transformations},\n  date = {2019-08-21},\n  url = {https://geocompx.org//post/2019/crs-projections-transformations},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobin Lovelace. 2019. “Geographic Projections and\nTransformations.” August 21, 2019. https://geocompx.org//post/2019/crs-projections-transformations."
  },
  {
    "objectID": "post/2019/ggplot2-inset-maps/index.html",
    "href": "post/2019/ggplot2-inset-maps/index.html",
    "title": "Inset maps with ggplot2",
    "section": "",
    "text": "Inset maps enable multiple places to be shown in the same geographic data visualisation, as described in the Inset maps section (8.2.7) of our open source book Geocomputation with R. The topic of inset maps has gained attention and recently Enrico Spinielli asked inset maps could be created for data in unusual coordinate.\nR’s flexibility allows inset maps to be created in various ways, using different approaches and packages. However, the main idea stays the same: we need to create at least two maps: a larger one, called the main map, that shows the central story and a smaller one, called the inset map, that puts the main map in context.\nThis blog post shows how to create inset maps with ggplot2 for visualization. The approach also uses the sf package for spatial data reading and handling, cowplot to arrange inset maps, and rcartocolor for additional color palettes. To reproduce the results on your own computer, after installing them, these packages can be attached as follows:"
  },
  {
    "objectID": "post/2019/ggplot2-inset-maps/index.html#data-preparation",
    "href": "post/2019/ggplot2-inset-maps/index.html#data-preparation",
    "title": "Inset maps with ggplot2",
    "section": "Data preparation",
    "text": "Data preparation\nThe first step is to read and prepare the data we want to visualize. We use the us_states data from the spData package as the source of the inset map, and north_carolina from the sf package as the source of the main map.\n\nlibrary(spData)\ndata(\"us_states\", package = \"spData\")\nnorth_carolina = read_sf(system.file(\"shape/nc.shp\", package = \"sf\"))\n\nBoth objects should have the same coordinate reference system (crs). Here, we use crs = 2163, which represents the US National Atlas Equal Area projection.\n\nus_states_2163 = st_transform(us_states, crs = 2163)\nnorth_carolina_2163 = st_transform(north_carolina, crs = 2163)\n\nWe also need to have the borders of the area we want to highlight (use in the main map). This can be done by extracting the bounding box of our north_carolina_2163 object.\n\nnorth_carolina_2163_bb = st_as_sfc(st_bbox(north_carolina_2163))"
  },
  {
    "objectID": "post/2019/ggplot2-inset-maps/index.html#maps-creation",
    "href": "post/2019/ggplot2-inset-maps/index.html#maps-creation",
    "title": "Inset maps with ggplot2",
    "section": "Maps creation",
    "text": "Maps creation\nThe second step is to create both inset and main maps independently. The inset map should show the context (larger area) and highlight the area of interest.\n\nggm1 = ggplot() + \n  geom_sf(data = us_states_2163, fill = \"white\") + \n  geom_sf(data = north_carolina_2163_bb, fill = NA, color = \"red\", size = 1.2) +\n  theme_void()\n\nggm1\n\n\n\n\nThe main map’s role is to tell the story. Here we show the number of births between 1974 and 1978 in the North Carolina counties (the BIR74 variable) using the Mint color palette from the rcartocolor palette. We also customize the legend position and size - this way, the legend is a part of the map, instead of being somewhere outside the map frame.\n\nggm2 = ggplot() + \n  geom_sf(data = north_carolina_2163, aes(fill = BIR74)) +\n  scale_fill_carto_c(palette = \"Mint\") +\n  theme_void() +\n  theme(legend.position = c(0.4, 0.05),\n        legend.direction = \"horizontal\",\n        legend.key.width = unit(10, \"mm\"))\n\nggm2"
  },
  {
    "objectID": "post/2019/ggplot2-inset-maps/index.html#maps-joining",
    "href": "post/2019/ggplot2-inset-maps/index.html#maps-joining",
    "title": "Inset maps with ggplot2",
    "section": "Maps joining",
    "text": "Maps joining\nThe final step is to join two maps. This can be done using functions from the cowplot package. We create an empty ggplot layer using ggdraw(), fill it with out main map (draw_plot(ggm2)), and add an inset map by specifing its position and size:\n\ngg_inset_map1 = ggdraw() +\n  draw_plot(ggm2) +\n  draw_plot(ggm1, x = 0.05, y = 0.65, width = 0.3, height = 0.3)\n\ngg_inset_map1\n\n\n\n\nThe final map can be saved using the ggsave() function.\n\nggsave(filename = \"01_gg_inset_map.png\", \n       plot = gg_inset_map1,\n       width = 8, \n       height = 4,\n       dpi = 150)"
  },
  {
    "objectID": "post/2019/ggplot2-inset-maps/index.html#data-preparation-1",
    "href": "post/2019/ggplot2-inset-maps/index.html#data-preparation-1",
    "title": "Inset maps with ggplot2",
    "section": "Data preparation",
    "text": "Data preparation\nThis map will use the US states borders (states()) as the source of the inset map and the Kentucky Senate legislative districts (state_legislative_districts()) as the main map.\n\nlibrary(tigris)\noptions(tigris_use_cache = TRUE)\nus_states = states(cb = FALSE, class = \"sf\")\nky_districts = state_legislative_districts(\"KY\", house = \"upper\",\n                                           cb = FALSE, class = \"sf\")\n\nThe states() function, in addition to the 50 states, also returns the District of Columbia, Puerto Rico, American Samoa, the Commonwealth of the Northern Mariana Islands, Guam, and the US Virgin Islands. For our purpose, we are interested in the continental 48 states and the District of Columbia only; therefore, we remove the rest of the divisions using subset().\n\nus_states = subset(us_states, \n                   !NAME %in% c(\n                     \"United States Virgin Islands\",\n                     \"Commonwealth of the Northern Mariana Islands\",\n                     \"Guam\",\n                     \"American Samoa\",\n                     \"Puerto Rico\",\n                     \"Alaska\",\n                     \"Hawaii\"\n                   ))\n\nThe same as in the example above, we transform both objects to have the same projection.\n\nky_districts_2163 = st_transform(ky_districts, crs = 2163)\nus_states_2163 = st_transform(us_states, crs = 2163)\n\nWe also extract the bounding box of the main object here. However, instead of using it directly, we add a buffer of 10,000 meters around it. This output will be handy in both inset and main maps.\n\nky_districts_2163_bb = st_as_sfc(st_bbox(ky_districts_2163))\nky_districts_2163_bb = st_buffer(ky_districts_2163_bb, dist = 10000)\n\nThe ky_districts_2163 object does not have any interesting variables to visualize, so we create some random values here. However, we could also join the districts’ data with another dataset in this step.\n\nky_districts_2163$values = runif(nrow(ky_districts_2163))"
  },
  {
    "objectID": "post/2019/ggplot2-inset-maps/index.html#map-creation",
    "href": "post/2019/ggplot2-inset-maps/index.html#map-creation",
    "title": "Inset maps with ggplot2",
    "section": "Map creation",
    "text": "Map creation\nThe inset map should be as clear and simple as possible.\n\nggm3 = ggplot() + \n  geom_sf(data = us_states_2163, fill = \"white\", size = 0.2) + \n  geom_sf(data = ky_districts_2163_bb, fill = NA, color = \"blue\", size = 1.2) +\n  theme_void()\n\nggm3\n\n\n\n\nOn the other hand, the main map looks better when we provide some additional context to our data. One of the ways to achieve it is to add the borders of the neighboring states.\nImportantly, we also need to limit the extent of our main map to the range of the frame in the inset map. This can be done with the coord_sf() function.\n\nggm4 = ggplot() + \n  geom_sf(data = us_states_2163, fill = \"#F5F5DC\") +\n  geom_sf(data = ky_districts_2163, aes(fill = values)) +\n  scale_fill_carto_c(palette = \"Sunset\") +\n  theme_void() +\n  theme(legend.position = c(0.5, 0.07),\n        legend.direction = \"horizontal\",\n        legend.key.width = unit(10, \"mm\"),\n        plot.background = element_rect(fill = \"#BFD5E3\")) +\n  coord_sf(xlim = st_bbox(ky_districts_2163_bb)[c(1, 3)],\n           ylim = st_bbox(ky_districts_2163_bb)[c(2, 4)])\n\nggm4\n\n\n\n\nFinally, we draw two maps together, trying to find the best location and size for the inset map.\n\ngg_inset_map2 = ggdraw() +\n  draw_plot(ggm4) +\n  draw_plot(ggm3, x = 0.02, y = 0.65, width = 0.35, height = 0.35)\n\ngg_inset_map2\n\n\n\n\nThe final map can be saved using the ggsave() function.\n\nggsave(filename = \"02_gg_inset_map.png\", \n       plot = gg_inset_map2,\n       width = 7.05, \n       height = 4,\n       dpi = 150)"
  },
  {
    "objectID": "post/2019/tmap-grid/index.html",
    "href": "post/2019/tmap-grid/index.html",
    "title": "Grids and graticules in the tmap package",
    "section": "",
    "text": "This vignette builds on the making maps chapter of the Geocomputation with R book. Its goal is to demonstrate how to set and modify grids and graticules in the tmap package."
  },
  {
    "objectID": "post/2019/tmap-grid/index.html#prerequisites",
    "href": "post/2019/tmap-grid/index.html#prerequisites",
    "title": "Grids and graticules in the tmap package",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe examples below assume the following packages are attached:\n\n\n\n\nlibrary(spData) # example datasets\nlibrary(tmap)   # map creation (>=2.3)\nlibrary(sf)     # spatial data classes"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html",
    "href": "post/2019/tmap-styles/index.html",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "",
    "text": "This vignette builds on the making maps chapter of the Geocomputation with R book. Its goal is to demonstrate all possible map styles available in the tmap package."
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#prerequisites",
    "href": "post/2019/tmap-styles/index.html#prerequisites",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe examples below assume the following packages are attached:\n\nlibrary(spData) # example datasets\nlibrary(tmap)   # map creation\nlibrary(sf)     # spatial data reprojection\n\nThe world object containing a world map data from Natural Earth and information about countries’ names, regions, and subregions they belong to, areas, life expectancies, and populations. This object is in geographical coordinates using the WGS84 datum, however, for mapping purposes, the Mollweide projection is a better alternative (learn more in the modifying map projections section). The st_tranform function from the sf package allows for quick reprojection to the selected coordinate reference system (e.g., \"+proj=moll\" represents the Mollweide projection).\n\nworld_moll = st_transform(world, crs = \"+proj=moll\")"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#pretty",
    "href": "post/2019/tmap-styles/index.html#pretty",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Pretty",
    "text": "Pretty\nWhen the variable provided as the col argument is numeric, tmap will use the \"pretty\" style as a default. In other words, it runs tm_polygons(col = \"lifeExp\", style = \"pretty\") invisibly to the user. This style rounds breaks into whole numbers where possible and spaces them evenly.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE) \n\n\n\n\nA histogram is added using legend.hist = TRUE in this and several next examples to show how the selected map style relates to the distribution of values.\nIt is possible to indicate a preferred number of classes using the n argument. Importantly, not every n is possible depending on the range of the values in the data.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              legend.hist = TRUE,\n              n = 4) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#fixed",
    "href": "post/2019/tmap-styles/index.html#fixed",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Fixed",
    "text": "Fixed\nThe \"fixed\" style allows for a manual selection of the breaks in conjunction with the breaks argument.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\", \n              style = \"fixed\",\n              breaks = c(45, 60, 75, 90),\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE) \n\n\n\n\nAdditionally, the default labels can be overwritten using the labels argument.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\", \n              style = \"fixed\",\n              breaks = c(45, 60, 75, 90),\n              labels = c(\"low\", \"medium\", \"high\"),\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#breaks-based-on-the-standard-deviation-value",
    "href": "post/2019/tmap-styles/index.html#breaks-based-on-the-standard-deviation-value",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Breaks based on the standard deviation value",
    "text": "Breaks based on the standard deviation value\nThe \"sd\" style calculates a standard deviation of a given variable, and next use this value as the break width.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\", \n              style = \"sd\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#fisher-algorithm",
    "href": "post/2019/tmap-styles/index.html#fisher-algorithm",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Fisher algorithm",
    "text": "Fisher algorithm\nThe \"fisher\" style creates groups with maximalized homogeneity.1\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              style = \"fisher\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#jenks-natural-breaks",
    "href": "post/2019/tmap-styles/index.html#jenks-natural-breaks",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Jenks natural breaks",
    "text": "Jenks natural breaks\nThe \"jenks\" style identifies groups of similar values in the data and maximizes the differences between categories.2\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              style = \"jenks\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#hierarchical-clustering",
    "href": "post/2019/tmap-styles/index.html#hierarchical-clustering",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nIn the \"hclust\" style, breaks are created using hierarchical clustering.3\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              style = \"hclust\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#bagged-clustering",
    "href": "post/2019/tmap-styles/index.html#bagged-clustering",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Bagged clustering",
    "text": "Bagged clustering\nThe \"bclust\" style uses the bclust function to generate the breaks using bagged clustering.4\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              style = \"bclust\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE) \n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#k-means-clustering",
    "href": "post/2019/tmap-styles/index.html#k-means-clustering",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "k-means clustering",
    "text": "k-means clustering\nThe \"kmeans\" style uses the kmeans function to generate the breaks.5\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\", \n              style = \"kmeans\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#quantile-breaks",
    "href": "post/2019/tmap-styles/index.html#quantile-breaks",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Quantile breaks",
    "text": "Quantile breaks\nThe \"quantile\" style creates breaks with an equal number of features (polygons).\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\", \n              style = \"quantile\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#equal-breaks",
    "href": "post/2019/tmap-styles/index.html#equal-breaks",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Equal breaks",
    "text": "Equal breaks\nThe \"equal\" style divides input values into bins of equal range and is appropriate for variables with a uniform distribution. It is not recommended for variables with a skewed distribution as the resulting map may end-up having little color diversity.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\", \n              style = \"equal\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE) \n\n\n\n\nLearn more about the implementation of discrete scales in the classInt package’s documentation."
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#continuous",
    "href": "post/2019/tmap-styles/index.html#continuous",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Continuous",
    "text": "Continuous\nThe \"cont\" style presents a large number of colors over the continuous color field.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              style = \"cont\") +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#order",
    "href": "post/2019/tmap-styles/index.html#order",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Order",
    "text": "Order\nThe \"order\" style also presents a large number of colors over the continuous color field. However, this style is suited to visualize skewed distributions; notice that the values on the legend do not change linearly.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              style = \"order\") +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2020/installing-r-spatial-packages-linux/index.html",
    "href": "post/2020/installing-r-spatial-packages-linux/index.html",
    "title": "Installing spatial R packages on Ubuntu",
    "section": "",
    "text": "This post explains how to quickly get key R packages for geographic research installed on Ubuntu, a popular Linux distribution.\n\nA recent thread on the r-spatial GitHub organization alludes to many considerations when choosing a Linux set-up for work with geographic data, ranging from the choice of Linux distribution (distro) to the use of binary vs or compiled versions (binaries are faster to install). This post touches on some of these things. Its main purpose, though, is to provide advice on getting R’s key spatial packages up-and-running on a future-proof Linux operating system (Ubuntu).\nNow is a good time to be thinking about your R set-up because changes are in the pipeline and getting set-up (or preparing to get set-up) now could save hours in the future. These imminent changes include:\n\nThe next major release of R (4.0.0), scheduled for the 24th April (2020-04-24)\nThe next major release of Ubuntu (20.04), a Long Term Support (LTS) version that will be used by millions of servers and research computers worldwide for years to come. Coincidentally, Ubuntu 20.04 will be released a day earlier than R 4.0.0, on 23rd April (2020-04-23).\nOngoing changes to the OSGeo stack on which key geographic R packages depend, as documented in r-spatial repos and a recent blog post on how recent versions of PROJ enable more precise coordinate reference system definitions.\n\nTo keep-up with these changes, this post will be updated in late April when some of the dust has settled around these changes. However, the advice presented here should be future-proof. Upgrading Ubuntu is covered in the next section.\nThere many ways of getting Ubuntu set-up for spatial R packages. A benefit of Linux operating systems is that they offer choice and prevent ‘lock-in’. However, the guidance in the next section should reduce set-up time and improve maintainability (with updates managed by Ubuntu) compared with other ways of doing things, especially for beginners. If you’re planning to switch to Linux as the basis of your geographic work, this advice may be particularly useful. (The post was written in response to colleagues asking me how to set-up R on their new Ubuntu computers. If you would like a a computer running Ubuntu, check out companies that support open source operating systems and guides on installing Ubuntu on an existing machine).\nBy ‘key packages’ I mean the following, which enable the majority of day-to-day geographic data processing and visualization tasks:\n\nsf for reading, writing and working with a range geographic vector file formats and geometry types\nraster, a mature package for working with geographic raster data (see the terra for an in-development replacement for raster)\ntmap, a flexible package for making static and interactive maps\n\nThe focus is on Ubuntu because that’s what I’ve got most experience with and it is well supported by the community. Links for installing geographic R packages on other distros are provided in section 3.\n\n1. Installing spatial R packages on Ubuntu\n\n\nR’s spatial packages can be installed from source on the latest version of this popular operating system, once the appropriate repository has been set-up, meaning faster install times (only a few minutes including the installation of upstream dependencies). The following bash commands should install key geographic R packages on Ubuntu 19.10:\n# add a repository that ships the latest version of R:\nsudo add-apt-repository ppa:marutter/rrutter3.5\n# update the repositories so the software can be found:\nsudo apt update\n# install system dependencies:\nsudo apt install libudunits2-dev libgdal-dev libgeos-dev libproj-dev libfontconfig1-dev\n# binary versions of key R packages:\nsudo apt install r-base-dev r-cran-sf r-cran-raster r-cran-rjava\nTo test your installation of R has worked, try running R in an IDE such as RStudio or in the terminal by entering R. You should be able to run the following commands without problem:\nlibrary(sf)\n#> Linking to GEOS 3.7.2, GDAL 2.4.2, PROJ 5.2.0\ninstall.packages(\"tmap\")\nIf you are using an older version of Ubuntu and don’t want to upgrade to 19.10, which will upgrade to (20.04) by the end of April 2020, see instructions at github.com/r-spatial/sf and detailed instructions on the blog rtask.thinkr.fr, which contains this additional shell command:\n# for Ubuntu 18.04\nsudo add-apt-repository ppa:marutter/c2d4u3.5\nThat adds a repository that ships hundreds of binary versions of R packages, meaning faster install times for packages (see the Binary package section of the open source book R Packages for more on binary packages). An updated repository, called c2d4u4.0 or similar, will be available for Ubuntu 20.04 in late April.\n\n\nIf you have issues with the instructions in this post here, you can find a wealth of answers on site such as StackOverflow, the sf issue tracker, r-sig-geo and Debian special interest group (SIG) email lists (the latter of which provided input into this blog post, thanks to Dirk Eddelbuettel and Michael Rutter).\n\n\n2. Updating R packages and upstream dependencies\nLinux operating systems allow you to customize your set-up in myriad ways. This can be enlightening but it can also be wasteful, so it’s worth considering the stability/cutting-edge continuum before diving into a particular set-up and potentially wasting time (if the previous section hasn’t already made-up your mind).\n\n\nA reliable way to keep close (but not too close) to the cutting edge on the R side on any operating system is simply to keep your packages up-to-date. Running the following command (or using the Tools menu in RStudio) every week or so will ensure you have up-to-date package versions:\n\nupdate.packages()\n\nKeeping system dependencies, software that R relies on but that is not maintained by R developers, is also important but can be tricky, especially for large and complex libraries like GDAL. On Ubuntu dependencies are managed by apt, and the following commands will update the ‘OSGeo stack’, composed of PROJ, GEOS and GDAL, if changes are detected in the default repositories (from 18.10 onwards):\nsudo apt update # see if things have changed\nsudo apt upgrade # install changes\nThe following commands will upgrade to a newer version of Ubuntu (it may be worth waiting until the point release of Ubuntu 20.04 — 20.04.1 — is released in summer before upgrading if you’re currently running Ubuntu 18.04 if high stability and low set-up times are priorities; also see instructions here):\napt dist-upgrade\nTo get more up-to-date upstream geographic libraries than provided in the default Ubuntu repositories, you can add the ubuntugis repository as follows. This is a pre-requisite on Ubuntu 18.04 and earlier but also works with later versions (warning, adding this repository could cause complications if you already have software such as QGIS that uses a particular version of GDAL installed):\nsudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable\nsudo apt update\nsudo apt upgrade\nThat will give you more up-to-date versions of GDAL, GEOS and PROJ which may offer some performance improvements. Note: if you do update dependencies such as GDAL you will need to re-install the relevant packages, e.g. with install.packages(\"sf\"). You can revert that change with the following little-known command:\nsudo add-apt-repository --remove ppa:ubuntugis/ubuntugis-unstable\nIf you also want the development versions of key R packages, e.g. to test new features and support development efforts, you can install them from GitHub, e.g. as follows:\n\nremotes::install_github(\"r-spatial/sf\")\nremotes::install_github(\"rspatial/raster\")\nremotes::install_github(\"mtennekes/tmaptools\") # required for dev version of tmap\nremotes::install_github(\"mtennekes/tmap\")\n\n\n\n3. Installing geographic R packages on other Linux operating systems\nIf you are in the fortunate position of switching to Linux and being able to choose the distribution that best fits your needs, it’s worth thinking about which distribution will be both user-friendly (more on that soon), performant and future-proof. Ubuntu is a solid choice, with a large user community and repositories such as ‘ubuntugis’ providing more up-to-date versions of upstream geographic libraries such as GDAL.\nQGIS is also well-supported on Ubuntu.\nHowever, you can install R and key geographic packages on other operating systems, although it may take longer. Useful links on installing R and geographic libraries are provided below for reference:\n\nInstalling R on Debian is covered on the CRAN website. Upstream dependencies such as GDAL can be installed on recent versions of Debian, such as buster, with commands such as apt install libgdal-dev as per instructions on the rocker/geospatial.\nInstalling R on Fedora/Red Hat is straightforward, as outlined on CRAN. GDAL and other spatial libraries can be installed from Fedora’s dnf package manager, e.g. as documented here for sf.\nArch Linux has a growing R community. Information on installing and setting-up R can be found on the ArchLinux wiki. Installing upstream dependencies such as GDAL on Arch is also relatively straightforward. There is also a detailed guide for installing R plus geographic packages by Patrick Schratz.\n\n\n\n4. Geographic R packages on Docker\n\n\n\n\n\n\n\nThe Ubuntu installation instructions outlined above provide such an easy and future-proof set-up. But if you want an even easier way to get the power of key geographic packages running on Linux, and have plenty of RAM and HD space, running R on the ‘Docker Engine’ may be an attractive option.\nAdvantages of using Docker include reproducibility (code will always run the same on any given image, and images can be saved), portability (Docker can run on Linux, Windows and Mac) and scalability (Docker provides a platform for scaling-up computations across multiple nodes).\nFor an introduction to using R/RStudio in Docker, see the Rocker project.\nUsing that approach, I recommend the following Docker images for using R as a basis for geographic research:\n\nrocker/geospatial which contains key geographic packages, including those listed above\nrobinlovelace/geocompr which contains all the packages needed to reproduce the contents of the book, and which you can run with the following command in a shell in which Docker is installed:\n\ndocker run -e PASSWORD=yourpassword --rm -p 8787:8787 robinlovelace/geocompr\nTo test-out the Ubuntu 19.10 set-up recommended above I created a Dockerfile and associated image on Dockerhub that you can test-out as follows:\ndocker run -it robinlovelace/geocompr:ubuntu-eoan\nR\nlibrary(sf)\n#> Linking to GEOS 3.7.2, GDAL 2.4.2, PROJ 5.2.0\nlibrary(raster)\nlibrary(tmap) \nThe previous commands should take you to a terminal inside the docker container where you try out the Linux command line and R. If you want to use more cutting-edge versions of the geographic libraries, you can use the ubuntu-bionic image (note the more recent version numbers, with PROJ 7.0.0 for example):\nsudo docker run -it robinlovelace/geocompr:ubuntu-bionic\nR\nlibrary(sf)\n#> Linking to GEOS 3.8.0, GDAL 3.0.4, PROJ 7.0.0\nThese images do not currently contain all the dependencies needed to reproduce the code in Geocomputation with R. \nHowever, as documented in issue 476 in the geocompr GitHub repo, there is a plan to provide Docker images with this full ‘R-spatial’ stack installed, building on strong foundations such as rocker/geospatial and the ubuntugis repositories, to support different versions of GDAL and other dependencies. We welcome any comments or tech support to help make this happen. Suggested changes to this post are also welcome, see the source code here.\n\n\n5. Fin\nR is an open-source language heavily inspired by Unix/Linux so it should come as no surprise that it runs well on a variety of Linux distributions, Ubuntu (covered in this post) in particular. The guidance in this post should get geographic R packages set-up quickly in a future-proof way. A sensible next step is to sharpen you system administration (sysadmin) and shell coding skills, e.g. with reference to Ubuntu wiki pages and Chapter 2 of the open source book Data Science at the Command Line.\nThis will take time but, building on OSGeo libraries, a well set-up Linux machine is an ideal platform to install, run and develop key geographic R packages in a performant, stable and future-proof way. \n\nBe the FOSS4G change you want to see in the world!\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{lovelace2020,\n  author = {Robin Lovelace},\n  title = {Installing Spatial {R} Packages on {Ubuntu}},\n  date = {2020-03-30},\n  url = {https://geocompx.org//post/2020/installing-r-spatial-packages-linux},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobin Lovelace. 2020. “Installing Spatial R Packages on\nUbuntu.” March 30, 2020. https://geocompx.org//post/2020/installing-r-spatial-packages-linux."
  },
  {
    "objectID": "post/2020/r-spatial-demo-covid-19/index.html",
    "href": "post/2020/r-spatial-demo-covid-19/index.html",
    "title": "Demo of reproducible geographic data analysis: mapping Covid-19 data with R",
    "section": "",
    "text": "Introduction\nThe coronavirus pandemic is a global phenomenon that will affect the lives of the majority of the world’s population for years to come. Impacts range from physical distancing measures already affecting more than half of Earth’s population and knock-on impacts such as changes in air quality to potentially life threatening illness, with peer reviewed estimates of infection fatality rates showing the disease disproportionately affects the elderly and people with underlying health conditions.\nLike other global phenomena such as climate change, the impacts of the pandemic vary greatly by geographic location, with effective and early implementation of physical distancing measures and effective contact tracing associated with lower death rates, according to preliminary research, as illustrated in the animation below (source: Washington Post).\n\n\n\n\n\nThis article demonstrates how to download and map open data on the evolving coronavirus pandemic, using reproducible R code. The aim is not to provide scientific analysis of the data, but to demonstrate how ‘open science’ enables public access to important international datasets. It also provides an opportunity to demonstrate how techniques taught in Geocomputation with R can be applied to real-world datasets.\nBefore undertaking geographic analysis of ‘rate’ data, such as the number Covid-19 infections per unit area, it is worth acknowledging caveats at the outset. Simple graphics of complex phenomena can be misleading. This is well-illustrated in the figure below by Will Geary, which shows how the ecological fallacy can affect interpretations of geographical analysis of areal units such countries that we will be using in this research.\n\n\n\n\n\nThe post is intended more as a taster of geographic visualisation in R than as a gateway to scientific analysis of Covid-19 data. See resources such as the eRum2020 CovidR contest and lists of online resources for pointers on how to usefully contribute to data-driven efforts to tackle the crisis.\n\n\nSet-up\nTo reproduce the results presented in this article you will need to have an R installation with up-to-date versions of the following packages installed and loaded. (See the geocompr/docker repo and Installing R on Ubuntu article for more on setting up your computer to work with R).\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.2, PROJ 9.0.1; sf_use_s2() is TRUE\n\n\n\nlibrary(tmap)\nlibrary(dplyr)\n\n\n\nGetting international Covid-19 data\nTo get data on official Covid-19 statistics, we will use the COVID19 R package.\n\nThis package provides daily updated data on a variety of variables related to the coronavirus pandemic at national, regional and city levels. Install it as follows:\n\ninstall.packages(\"COVID19\")\n\nAfter the package is installed, you can get up-to-date country-level data as follows:\n\nd = COVID19::covid19()\n\n\n\n\nTo minimise dependencies for reproducing the results in this article, we uploaded a copy of the data, which can be downloaded as follows (re-run the code above to get up-to-date data):\n\nd = readr::read_csv(\"https://git.io/covid-19-2020-04-23\")\nclass(d)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nThe previous code chunk read a .csv file from online and confirmed, we have loaded a data frame (we will see how to join this with geographic data in the next section). We can get a sense of the contents of the data as follows:\n\nncol(d)\n\n[1] 24\n\nnrow(d)\n\n[1] 17572\n\nnames(d)\n\n [1] \"id\"             \"date\"           \"deaths\"         \"confirmed\"     \n [5] \"tests\"          \"recovered\"      \"hosp\"           \"icu\"           \n [9] \"vent\"           \"driving\"        \"walking\"        \"transit\"       \n[13] \"country\"        \"state\"          \"city\"           \"lat\"           \n[17] \"lng\"            \"pop\"            \"pop_14\"         \"pop_15_64\"     \n[21] \"pop_65\"         \"pop_age\"        \"pop_density\"    \"pop_death_rate\"\n\n\n\n\nGetting geographic data\nWe will use a dataset representing countries worldwide from the rnaturalearth package. Assuming you have the package installed you can get the geographic data as follows (see the subsequent code chunk if not):\n\nworld_rnatural = rnaturalearth::ne_download(returnclass = \"sf\")\n# names(world_iso) # variables available\nworld_iso = world_rnatural %>% \n  select(NAME_LONG, ISO_A3_EH, POP_EST, GDP_MD_EST, CONTINENT)\n\n\n\n\nThe result of the previous code block, an object representing the world and containing only variables of interest, was uploaded to GitHub and can be loaded from a GeoJSON file as follows:\n\nworld_iso = sf::read_sf(\"https://git.io/JfICT\") \n\nTo see what’s in the world_iso object we can plot it, with the default setting in sf showing the geographic distribution of each variable:\n\nplot(world_iso)\n\n\n\n\n\n\nTransforming geographic data\nAn issue with the result from a data visualisation perspective is that this unprojected visualisation distorts the world: countries such as Greenland at high latitudes appear bigger than the actually are. To overcome this issue we will project the object as follows (see Chapter 6 of Geocomputation with R and a recent article on the r-spatial website for more on coordinate systems):\n\nworld_projected = world_iso %>% \n  st_transform(\"+proj=moll\")\n\nWe can plot just the geometry of the updated object as follows, noting that the result is projected in a way that preserves the true area of countries (noting also that all projections introduce distortions):\n\nplot(st_geometry(world_projected))\n\n\n\n\n\n\nAttribute joins\nAs outlined in Chapter 3 of Geocomputation with R, attribute joins can be used to add additional variables to geographic data via a ‘key variable’ shared between the geographic and non-geographic objects. In this case the shared variables are ISO_A3_EH in the geographic object and id in the Covid-19 dataset d. We will be concise and call the dataset resulting from this join operation w.\n\nw = dplyr::left_join(world_projected, d, by = c(\"ISO_A3_EH\"= \"id\"))\nclass(w)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnrow(w)\n\n[1] 14919\n\n\n\n\nCalculating area\nThe package sf provides a wide range of functions for calculating geographic variables such as object centroid, bounding boxes, lengths and, as demonstrated below, area. We use this area data to calculate the population density of each country as follows:\n\nw$Area_km = as.numeric(st_area(w)) / 1e6\nw$`Pop/km2` = as.numeric(w$POP_EST) / w$Area_km \n\n\n\nPlotting international Covid-19 data for a single day\nThe class of w shows that it has geometries for each row. Notice that it has many more rows of data than the original world object: geometries are repeated for every year. This is not an efficient way to store data, as it means lots of duplicate geometries. On a small dataset that doesn’t matter, but it’s something to be aware of. To check that the join has worked, we will take a subset of rows representing the global situation yesterday relative to the date of data access:\n\nw_yesterday = w %>% \n  filter(date == max(date, na.rm = T) - 1)\nplot(w_yesterday)\n\n\n\n\nThe plot method for sf objects is fast and flexible, as documented in sf’s Plotting Simple Features vignette, which can be accessed with vignette(\"sf5\") from the R console. We can set the breaks to better show the difference between countries with no reported deaths and countries with few reported deaths as follows:\n\nplot(w_yesterday[\"deaths\"])\n\n\n\nb = c(0, 10, 100, 1000, 10000, 100000)\nplot(w_yesterday[\"deaths\"], breaks = b)\n\n\n\n\nTo plot the other Covid-19 variables, reporting number of confirmed cases, number of tests and number of people who have recovered, we can subset the relevant variables and pipe the result to the plot() function (noting the caveat that code containing pipes may be hard to debug) as follows:\n\nw_yesterday %>%\n  dplyr::select(deaths, confirmed, tests, recovered) %>% \n  plot()\n\n\n\n\n\n\nMaking maps with tmap\nThe mapping chapter of Geocomputation with R shows how the tmap package enables publication-quality maps to be created with concise and relatively commands, such as:\n\ntm_shape(w_yesterday) +\n  tm_polygons(c(\"deaths\", \"recovered\"))\n\n\n\n\nWe can modify the palette and scale as follows:\n\ntm_shape(w_yesterday) +\n  tm_polygons(\n    c(\"deaths\", \"recovered\"),\n    palette = \"viridis\",\n    style = \"order\"\n    ) \n\n\n\n\nThe map can be further improved by adding graticules representing the curvature of the Earth, created as follows:\n\ng = st_graticule(w_yesterday)\n\nIt’s also worth moving the legend:\n\ntm_shape(g) +\n  tm_lines(col = \"grey\") +\n  tm_shape(w_yesterday) +\n  tm_polygons(\n    c(\"deaths\", \"recovered\"),\n    palette = \"viridis\",\n    style = \"order\"\n    ) +\n  tm_layout(legend.position = c(0.01, 0.25))\n\n\n\n\nA problem with choropleth maps is that they can under-represent small areas. To overcome this issue we can use dot size instead of color to represent number:\n\ntm_shape(g) +\n  tm_lines(col = \"grey\") +\n  tm_shape(w_yesterday) +\n  tm_polygons() +\n  tm_layout(legend.position = c(0.01, 0)) +\n  tm_shape(w_yesterday) +\n  tm_dots(\n    col = c(\"red\", \"green\"),\n    size = c(\"deaths\", \"recovered\"),\n    palette = \"viridis\"\n    )\n\n\n\n\nOne question I have here: make the size legend have style = \"log10_pretty\" also?\n\n\nMaking animated maps\nThe animation at the beginning of this article shows how dynamic graphics can communicate change effectively. Animated maps are therefore useful for showing evolving geographic phenomena, such as the spread of Covid-19 worldwide. As covered in section 8.3 of Geocomputation with R, animated maps can be created with tmap by extending the tm_facet() functionality. So let’s start by creating a facetted map showing the total number of deaths on the first day of each month in our data:\n\nw$Date = as.character(w$date)\ntm_shape(g) +\n  tm_lines(col = \"grey\") +\n  tm_shape(w_yesterday) +\n  tm_polygons(\n    \"Pop/km2\",\n    palette = \"viridis\",\n    style = \"log10_pretty\",\n    n = 3\n    ) +\n  tm_shape(w %>% filter(grepl(pattern = \"01$\", date))) +\n  tm_dots(size = \"deaths\", col = \"red\") +\n  tm_facets(\"Date\", nrow = 1, free.scales.fill = FALSE) +\n  tm_layout(\n    legend.outside.position = \"bottom\",\n    legend.stack = \"horizontal\"\n    )\n\n\n\n\nTo create an animated map, following instructions in Chapter 8 of Geocomputation with R, we need to make some small changes to the code above:\n\nm = tm_shape(g) +\n  tm_lines(col = \"grey\") +\n  tm_shape(w_yesterday) +\n  tm_polygons(\n    \"Pop/km2\",\n    palette = \"viridis\",\n    style = \"log10_pretty\",\n    n = 3\n    ) +\n  tm_shape(w %>% filter(grepl(pattern = \"01$\", date))) +\n  tm_dots(size = \"deaths\", col = \"red\") +\n  tm_facets(along = \"Date\", free.coords = FALSE) +\n  tm_layout(legend.outside = TRUE)\ntmap_animation(m, \"covid-19-animated-map-test.gif\", width = 800)\nbrowseURL(\"covid-19-animated-map-test.gif\")\n\n\n\n\n\n\nWe made an animated map! The first version is rarely the best though, and the map above clearly could benefit from some adjustments before we plot the results for the whole dataset:\n\nw$Date = paste0(\"Total deaths from 22nd January 2020 to \", w$date)\n\nm = tm_shape(g) +\n  tm_lines(col = \"grey\") +\n  tm_shape(w) +\n  tm_polygons(\n    \"Pop/km2\",\n    palette = \"viridis\",\n    style = \"log10_pretty\",\n    lwd = 0.5\n    ) +\n  tm_shape(w) +\n  tm_dots(size = \"deaths\", col = \"red\") +\n  tm_facets(along = \"Date\", free.coords = FALSE) +\n  tm_layout(\n    main.title.size = 0.5,\n    legend.outside = TRUE\n    )\ntmap_animation(m, \"covid-19-animated-map.gif\", width = 1400, height = 600)\nbrowseURL(\"covid-19-animated-map.gif\")\n\n\n\n\n\nConclusion\nThis article has demonstrated how to work with and map geographic data using the free and open source statistical programming language R. It demonstrates that by representing analysis in code, research can be made reproducible and more accessible to others, encouraging transparent and open science. This has multiple advantages, from education and citizen engagement with the evidence to increased trust in the evidence on which important, life-or-death, decisions are made.\nAlthough the research did not address any policy issues, it could be extended to do so, and we encourage readers to check-out the following resources for ideas for future research:\n\nA reproducible geographic analysis of Covid-19 data in Spain by Antonio Paez and others (challenge: reproduce their findings)\nThe eRum2020 CovidR competition (challenge: enter the contest!)\nTry downloading the the city-level data with this command and exploring the geographic distribution of the outbreak at the city level:\n\n\nd_city = COVID19::covid19(level = 3)\n\nFor further details on geographic data analysis in R in general, we recommend checkout out in-depth materials such as Geocomputation with R and the in-progress open source book Spatial Data Science.\nThere is also an online talk on the subject on YouTube.\n\n\nSession info\n\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       Fedora Linux 37 (Thirty Seven)\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Warsaw\n date     2022-12-20\n pandoc   2.19.2 @ /usr/lib/rstudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version   date (UTC) lib source\n P abind          1.4-5     2016-07-21 [?] CRAN (R 4.2.2)\n P base64enc      0.1-3     2015-07-28 [?] CRAN (R 4.2.2)\n P bit            4.0.5     2022-11-15 [?] CRAN (R 4.2.2)\n P bit64          4.0.5     2020-08-30 [?] CRAN (R 4.2.2)\n P class          7.3-20    2022-01-16 [?] CRAN (R 4.2.2)\n P classInt       0.4-8     2022-09-29 [?] CRAN (R 4.2.2)\n P cli            3.4.1     2022-09-23 [?] CRAN (R 4.2.2)\n P codetools      0.2-18    2020-11-04 [?] CRAN (R 4.2.2)\n P crayon         1.5.2     2022-09-29 [?] CRAN (R 4.2.2)\n P crosstalk      1.2.0     2021-11-04 [?] CRAN (R 4.2.2)\n P curl           4.3.3     2022-10-06 [?] CRAN (R 4.2.2)\n P DBI            1.1.3     2022-06-18 [?] CRAN (R 4.2.2)\n P dichromat      2.0-0.1   2022-05-02 [?] CRAN (R 4.2.2)\n P digest         0.6.31    2022-12-11 [?] CRAN (R 4.2.2)\n P dplyr        * 1.0.10    2022-09-01 [?] CRAN (R 4.2.2)\n P e1071          1.7-12    2022-10-24 [?] CRAN (R 4.2.2)\n P ellipsis       0.3.2     2021-04-29 [?] CRAN (R 4.2.2)\n P evaluate       0.19      2022-12-13 [?] CRAN (R 4.2.2)\n P fansi          1.0.3     2022-03-24 [?] CRAN (R 4.2.2)\n P fastmap        1.1.0     2021-01-25 [?] CRAN (R 4.2.2)\n P generics       0.1.3     2022-07-05 [?] CRAN (R 4.2.2)\n P glue           1.6.2     2022-02-24 [?] CRAN (R 4.2.2)\n P hms            1.1.2     2022-08-19 [?] CRAN (R 4.2.2)\n P htmltools      0.5.4     2022-12-07 [?] CRAN (R 4.2.2)\n P htmlwidgets    1.6.0     2022-12-15 [?] CRAN (R 4.2.2)\n P jsonlite       1.8.4     2022-12-06 [?] CRAN (R 4.2.2)\n P KernSmooth     2.23-20   2021-05-03 [?] CRAN (R 4.2.2)\n P knitr          1.41      2022-11-18 [?] CRAN (R 4.2.2)\n P lattice        0.20-45   2021-09-22 [?] CRAN (R 4.2.2)\n P leafem         0.2.0     2022-04-16 [?] CRAN (R 4.2.2)\n P leaflet        2.1.1     2022-03-23 [?] CRAN (R 4.2.2)\n P leafsync       0.1.0     2019-03-05 [?] CRAN (R 4.2.2)\n P lifecycle      1.0.3     2022-10-07 [?] CRAN (R 4.2.2)\n P lwgeom         0.2-10    2022-11-19 [?] CRAN (R 4.2.2)\n P magrittr       2.0.3     2022-03-30 [?] CRAN (R 4.2.2)\n P pillar         1.8.1     2022-08-19 [?] CRAN (R 4.2.2)\n P pkgconfig      2.0.3     2019-09-22 [?] CRAN (R 4.2.2)\n P png            0.1-8     2022-11-29 [?] CRAN (R 4.2.2)\n P proxy          0.4-27    2022-06-09 [?] CRAN (R 4.2.2)\n P R6             2.5.1     2021-08-19 [?] CRAN (R 4.2.2)\n P raster         3.6-11    2022-11-28 [?] CRAN (R 4.2.2)\n P RColorBrewer   1.1-3     2022-04-03 [?] CRAN (R 4.2.2)\n P Rcpp           1.0.9     2022-07-08 [?] CRAN (R 4.2.2)\n P readr          2.1.3     2022-10-01 [?] CRAN (R 4.2.2)\n   renv           0.16.0    2022-09-29 [2] CRAN (R 4.2.2)\n P rlang          1.0.6     2022-09-24 [?] CRAN (R 4.2.2)\n P rmarkdown      2.19      2022-12-15 [?] CRAN (R 4.2.2)\n P rstudioapi     0.14      2022-08-22 [?] CRAN (R 4.2.2)\n P sessioninfo    1.2.2     2021-12-06 [?] CRAN (R 4.2.2)\n P sf           * 1.0-9     2022-11-08 [?] CRAN (R 4.2.2)\n P sp             1.5-1     2022-11-07 [?] CRAN (R 4.2.2)\n P stars          0.6-0     2022-11-21 [?] CRAN (R 4.2.2)\n P stringi        1.7.8     2022-07-11 [?] CRAN (R 4.2.2)\n P stringr        1.5.0     2022-12-02 [?] CRAN (R 4.2.2)\n P terra          1.6-47    2022-12-02 [?] CRAN (R 4.2.2)\n P tibble         3.1.8     2022-07-22 [?] CRAN (R 4.2.2)\n P tidyselect     1.2.0     2022-10-10 [?] CRAN (R 4.2.2)\n P tmap         * 3.3-3     2022-03-02 [?] CRAN (R 4.2.2)\n P tmaptools      3.1-1     2021-01-19 [?] CRAN (R 4.2.2)\n P tzdb           0.3.0     2022-03-28 [?] CRAN (R 4.2.2)\n P units          0.8-1     2022-12-10 [?] CRAN (R 4.2.2)\n P utf8           1.2.2     2021-07-24 [?] CRAN (R 4.2.2)\n P vctrs          0.5.1     2022-11-16 [?] CRAN (R 4.2.2)\n P viridisLite    0.4.1     2022-08-22 [?] CRAN (R 4.2.2)\n P vroom          1.6.0     2022-09-30 [?] CRAN (R 4.2.2)\n P withr          2.5.0     2022-03-03 [?] CRAN (R 4.2.2)\n P xfun           0.35      2022-11-16 [?] CRAN (R 4.2.2)\n P XML            3.99-0.13 2022-12-04 [?] CRAN (R 4.2.2)\n P yaml           2.3.6     2022-10-18 [?] CRAN (R 4.2.2)\n\n [1] /tmp/Rtmp2F9iQn/renv-library-6dbab52f30514\n [2] /home/jn/Science/geocompr/geocompr.github.io/renv/library/R-4.2/x86_64-redhat-linux-gnu\n [3] /home/jn/Science/geocompr/geocompr.github.io/renv/sandbox/R-4.2/x86_64-redhat-linux-gnu/60c4e220\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{lovelace2020,\n  author = {Robin Lovelace},\n  title = {Demo of Reproducible Geographic Data Analysis: Mapping\n    {Covid-19} Data with {R}},\n  date = {2020-04-23},\n  url = {https://geocompx.org//post/2020/r-spatial-demo-covid-19},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobin Lovelace. 2020. “Demo of Reproducible Geographic Data\nAnalysis: Mapping Covid-19 Data with R.” April 23, 2020. https://geocompx.org//post/2020/r-spatial-demo-covid-19."
  },
  {
    "objectID": "post/2020/recent-changes-in-r-spatial-and-how-to-be-ready-for-them/index.html",
    "href": "post/2020/recent-changes-in-r-spatial-and-how-to-be-ready-for-them/index.html",
    "title": "Recent changes in R spatial and how to be ready for them",
    "section": "",
    "text": "Currently, hundreds of R packages are related to spatial data analysis. They range from ecology and earth observation, hydrology and soil science, to transportation and demography. These packages support various stages of analysis, including data preparation, visualization, modeling, or communicating the results. One common feature of most R spatial packages is that they are built upon some of the main representations of spatial data in R, available in key geographic R packages such as:\n\nsf, which replaces sp\nterra, which aims to replace raster\nstars\n\nThose packages are also not entirely independent. They are using external libraries, namely GEOS for spatial data operations, GDAL for reading and writing spatial data, and PROJ for conversions of spatial coordinates.\nTherefore, R spatial packages are interwoven with each other and depend partially on external software developments. This has several positives, including the ability to use cutting-edge features and algorithms. On the other hand, it also makes R spatial packages vulnerable to changes in the upstream packages and libraries.\nIn the first part of the talk, we showcase several recent advances in R packages. It includes the largest recent change related to the developments in the PROJ library. We explain why the changes happened and how they impact R users. The second part focus on how to prepare for the changes, including computer set-up and running R spatial packages using Docker (briefly covered in a previous post and outlined in the new geocompr/docker repo). We outline important considerations when setting-up operating systems for geographic R packages. To reduce set-up times you can use geographic R packages Docker, a flexible and scalable technology containerization technology. Docker can run on modern computers and on your browser via services such as Binder, greatly reducing set-up times. Discussing these set-up options, and questions of compatibility between geographic R packages and paradigms such as the tidyverse and data.table, ensure that after the talk everyone can empower themselves with open source software for geographic data analysis in a powerful and flexible statistical programming environment.\nYou can find the slides for the talk at https://nowosad.github.io/whyr_webinar004/.\n\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{nowosad,robinlovelace2020,\n  author = {Jakub Nowosad, Robin Lovelace},\n  title = {Recent Changes in {R} Spatial and How to Be Ready for Them},\n  date = {2020-04-25},\n  url = {https://geocompx.org//post/2020/recent-changes-in-r-spatial-and-how-to-be-ready-for-them},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nJakub Nowosad, Robin Lovelace. 2020. “Recent Changes in R Spatial\nand How to Be Ready for Them.” April 25, 2020. https://geocompx.org//post/2020/recent-changes-in-r-spatial-and-how-to-be-ready-for-them."
  },
  {
    "objectID": "post/2021/geocompr2-bp1/index.html",
    "href": "post/2021/geocompr2-bp1/index.html",
    "title": "Geocomputation with R: Second Edition feedback",
    "section": "",
    "text": "TL;DR: Please help us by filling out the survey at https://forms.gle/nq9RmbxJyZXQgc948.\nIt’s been almost 3 years since the first edition of Geocomputation with R was published back in 2019. It’s been an amazing journey for this open source book on geographic data analysis, visualization and modeling since then. We have reached almost 500k people via the website at https://geocompr.robinlovelace.net/ and the physical book. The book now contributes to many university courses, lectures, and personal development as outlined at https://geocompr.github.io/guestbook/ - where you can add you own comments.\nBuilding on the success of the first edition, and motivated by the need for the material to adapt as spatial ecosystem evolves, we have decided that it is time for a Second Edition. We plan to start work on it over the next few months, aiming for publication around summer 2022. Second edition will be available at https://geocompr.robinlovelace.net/, while you can find the first edition at https://bookdown.org/robinlovelace/geocompr/.\nWe already have plenty of changes, updates, and improvements in mind, as documented in the book’s issue tracker. However, we want to get feedback from the community, to ensure we’re not miss something key and to find out what you most want from a 2nd edition. We’re asking for your help in guiding the future of Geocomputation with R!\nThe book is already much stronger thanks to community, with 50+ people contributing to the codebase already and many more supporting in the issue tracker and the guestbook. Please take a few minutes to let us know your thoughts on the current edition, and suggestions for the next one using the Google form.\nThanks from the Geocomputation with R team,\nRobin Lovelace, Jakub Nowosad, and Jannes Muenchow\n\n\n\nReusehttps://creativecommons.org/licenses/by/4.0/CitationBibTeX citation:@online{lovelace,jakubnowosad,jannesmuenchow2021,\n  author = {Robin Lovelace, Jakub Nowosad, Jannes Muenchow},\n  title = {Geocomputation with {R:} {Second} {Edition} Feedback},\n  date = {2021-09-06},\n  url = {https://geocompx.org//post/2021/geocompr2-bp1},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRobin Lovelace, Jakub Nowosad, Jannes Muenchow. 2021.\n“Geocomputation with R: Second Edition Feedback.” September\n6, 2021. https://geocompx.org//post/2021/geocompr2-bp1."
  },
  {
    "objectID": "post/2021/spatial-classes-conversion/index.html",
    "href": "post/2021/spatial-classes-conversion/index.html",
    "title": "Conversions between different spatial classes in R",
    "section": "",
    "text": "The R programming language has, over the past two decades, evolved substantial spatial data analysis capabilities, and is now one of the most powerful environments for undertaking geographic research using a reproducible command line interface. Currently, dedicated R packages allow to read spatial data and apply a plethora of different kinds of spatial methods in a reproducible fashion.\nThere are two main1 spatial data models - spatial vector data and spatial raster data. Natively R does not support spatial data and does not have a definition of spatial classes. Therefore, there had been a need to create R tools able to represent spatial vector and raster data. Spatial classes are slightly different from regular R objects, such as data frames or matrices, as they need to not only store values, but also information about spatial locations and their coordinate reference systems.\nNowadays, the most prominent packages to represent spatial vector data are sf (Pebesma 2021a) and its predecessor sp (Pebesma and Bivand 2021), however, the terra (Hijmans 2021b) package also has its own spatial class for vector data. Spatial raster data can be stored as objects from terra (Hijmans 2021b) and its predecessor raster (Hijmans 2021a), or alternatively the stars package (Pebesma 2021b).\nAs you could see in our Why R? webinar talk, the spatial capabilities of R constantly expand, but also evolve. New packages are being developed, while old ones are modified or superseded. In this process, new methods are created, higher performance code is added, and possible workflows are expanded. Alternative approaches allow for a (hopefully) healthy competition, resulting in better packages. Of course, having more than one package (with its own spatial class/es) for a vector or raster data model could be problematic, especially for new or inexperienced users.\nFirst, it takes time to understand how different spatial classes are organized. To illustrate this, let’s read the same spatial data, srtm.tif from the spDataLarge package (Nowosad and Lovelace 2021), using raster and stars. The raster object:\nThe stars object:\nSecondly, other packages with methods we want to use, could only accept one specific spatial class, but not the other. For example, the current version of the sabre package (Nowosad and Stepinski 2019) (0.3.2) accepts objects from the raster package, but not ones from terra or stars2. The partitions1 and partitions2 objects are of the RasterLayer class from raster, so the vmeasure_calc() function works correctly.\nHowever, when the input object (representing the same spatial data!) is of the SpatRaster class from terra, the calculation results in error.\nSome packages, such as tmap (Tennekes 2021), accept many R spatial classes, however, this takes a lot of effort from package creators to make it possible and to maintain it. Gladly, a number of functions exist that allow to convert between different R spatial classes. Using them, we can work with our favorite spatial data representation, switch to some other representation just for a certain calculation, and then convert the result back into our class. The next two sections showcase how to move between different spatial vector and raster data classes in R."
  },
  {
    "objectID": "post/2021/spatial-classes-conversion/index.html#spatial-vector-data",
    "href": "post/2021/spatial-classes-conversion/index.html#spatial-vector-data",
    "title": "Conversions between different spatial classes in R",
    "section": "Spatial vector data",
    "text": "Spatial vector data\nThe world.gpkg file from the spData (Bivand, Nowosad, and Lovelace 2020) contains spatial vector data with world countries.\n\nworld_path = system.file(\"shapes/world.gpkg\", package = \"spData\")\n\nNow, we can read this file, for example, as an sf object, and convert it into other spatial vector data classes.\n\nlibrary(sf)\nlibrary(sp)\nlibrary(terra)\n\n# read as sf\nworld = read_sf(world_path)\n\n# sf to sp\nworld_sp1 = as(world, \"Spatial\")\n\n# sf to terra vect\nworld_terra1 = vect(world)\n\n# sp to terra vect\nworld_terra2 = vect(world_sp1)\n\n# sp to sf\nworld_sf2 = st_as_sf(world_sp1)\n\n# terra vect to sf\nworld_sf3 = st_as_sf(world_terra1)\n\n# terra vect to sp\nworld_sp2 = as(world_terra1, \"Spatial\")\n\nIn summary, st_as_sf() converts other classes into sf, vect() transform other classes into terra’s SpatVector, and with as(x, \"Spatial\") it is possible to get sp’s vectors.\n\n\n\n\n \n  \n    FROM/TO \n    sf \n    sp \n    terra \n  \n \n\n  \n    sf \n     \n    as(x, \"Spatial\") \n    vect() \n  \n  \n    sp \n    st_as_sf() \n     \n    vect() \n  \n  \n    terra \n    st_as_sf() \n    as(x, \"Spatial\")"
  },
  {
    "objectID": "post/2021/spatial-classes-conversion/index.html#spatial-raster-data",
    "href": "post/2021/spatial-classes-conversion/index.html#spatial-raster-data",
    "title": "Conversions between different spatial classes in R",
    "section": "Spatial raster data",
    "text": "Spatial raster data\nThe srtm.tif file from the spDataLarge (Nowosad and Lovelace 2021) contains a raster elevation model for the Zion National Park in the USA.\n\nsrtm_path = system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\n\nNow, we can read this file, for example, as a raster object, and convert it into other spatial vector data classes.\n\nlibrary(raster)\nlibrary(stars)\nlibrary(terra)\n\nsrtm_raster1 = raster(srtm_path)\n\n# raster to terra\nsrtm_terra1 = rast(srtm_raster1)\n\n# terra to raster\nsrtm_raster2 = raster(srtm_terra1)\n\n# raster to stars\nsrtm_stars1 = st_as_stars(srtm_raster1)\n\n# stars to raster\nsrtm_raster2 = as(srtm_stars1, \"Raster\")\n\n# terra to stars\nsrtm_stars2 = st_as_stars(srtm_terra1)\n\n# stars to terra\nsrtm_terra1a = as(srtm_stars1, \"SpatRaster\")\n\nAs you can see - in most cases, we can just use one function to move from one class to another.\n\n\n\n\n \n  \n    FROM/TO \n    raster \n    terra \n    stars \n  \n \n\n  \n    raster \n     \n    rast() \n    st_as_stars() \n  \n  \n    terra \n    raster() \n     \n    st_as_stars() \n  \n  \n    stars \n    raster() \n    as(x, \"SpatRaster\")"
  },
  {
    "objectID": "post/2021/spatial-classes-conversion/index.html#summary",
    "href": "post/2021/spatial-classes-conversion/index.html#summary",
    "title": "Conversions between different spatial classes in R",
    "section": "Summary",
    "text": "Summary\nThis blog post summarizes how to move between different R spatial vector and raster classes. All of the functions mentioned above have one role: to change classes of input objects. They do not, however, change geometries or underlining values in the data.\nAdditionally, switching from the spatial vector data model to the spatial raster data model (and vice versa) is also possible. These operations are known as rasterization and vectorization, and they could impact spatial and nonspatial information in the input data. To learn more about them, read the Raster-Vector Interactions section in Geocomputation with R."
  },
  {
    "objectID": "post/2022/geocompr-solutions/index.html",
    "href": "post/2022/geocompr-solutions/index.html",
    "title": "Automatic website with the Geocomputation with R solutions",
    "section": "",
    "text": "Geocomputation with R is a book on geographic data analysis, visualization, and modeling. Each chapter1 ends with a set of exercises allowing the readers to test how well they understand each presented concepts and approaches. However, exercises are often not enough without their solutions, which provides a way to compare readers’ answers with an expected one. This leads to technical questions: how to provide solutions to the exercises without making the book too long, and to minimise duplication of words/effort (for the questions and answers)? Also, how to test, at the same time, if the solutions are still working?\nFor the first edition of the book we added solutions as external set of vignettes. This approach had, however, several flaws. Most importantly, solutions in the vignettes were detached from the exercises in the book, meaning that if we decided to change an exercise, we would need to go to a different repository and update it there as well. Additionally, the solution vignettes had a different style from the book and were harder to find. We decided to try a different approach for the second edition of the book. This time, we created an external bookdown-based website for solutions, which you can find at https://geocompr.github.io/solutions/. The main goal of this blog post is to explain the new setup in some detail."
  },
  {
    "objectID": "post/2022/geocompr-solutions/index.html#new-solutions-website",
    "href": "post/2022/geocompr-solutions/index.html#new-solutions-website",
    "title": "Automatic website with the Geocomputation with R solutions",
    "section": "New solutions website",
    "text": "New solutions website\nFirstly, let’s take a look at the bigger picture. Exercises are shown at the end of the book chapters, for example, exercises for chapter 6 can be seen at https://geocompr.robinlovelace.net/raster-vector.html#exercises-4. You may notice that this exercises section starts with a description of prerequisites, and then several questions are listed. However, what to do when I want to find solutions to these problems or compare my results with those provided by the book authors? Then, we can visit the corresponding chapter in the solution website at https://geocompr.github.io/solutions/raster-vector.html. Note that the solution website contains the same content as the exercises section, and extends it with the solution code, code results, and newly created figures."
  },
  {
    "objectID": "post/2022/geocompr-solutions/index.html#solutions",
    "href": "post/2022/geocompr-solutions/index.html#solutions",
    "title": "Automatic website with the Geocomputation with R solutions",
    "section": "Solutions",
    "text": "Solutions\nNext, let’s dive deeper to see how it works. We can look at the source code of the solution website first – https://github.com/geocompr/solutions, and solutions for chapter 6 are at https://github.com/geocompr/solutions/blob/main/06-raster-vector.Rmd. If you open the last link, you should see a code looking like that:\n```{r setup, echo=FALSE, results='hide'}\nex = tempfile(fileext = \".Rmd\")\non.exit(unlink(ex))\ndownload.file(\"https://raw.githubusercontent.com/Robinlovelace/geocompr/main/_06-ex.Rmd\", ex)\n```\n\n```{r, echo=FALSE, results='asis'}\nres = knitr::knit_child(ex, quiet = TRUE, options = list(include = TRUE))\ncat(res, sep = '\\n')\n```\nWhat is going on here? In short, we have two code chunks. The first (hidden) code chunk downloads a _06-ex.Rmd file from the Robinlovelace/geocompr repository. Ok, but how does this _06-ex.Rmd document looks like? You can see its content at https://raw.githubusercontent.com/Robinlovelace/geocompr/main/_06-ex.Rmd. It is a mix of text (mainly exercises questions) and R code chunks. Note here that most code chunks do not have any code chunks options set, except the first one. We wanted the first code chunk to be visible on the main book and the solution websites, so we used include = TRUE.\nThe second code chunk takes the downloaded document and knits it with a new global option, include = TRUE. This option make sure that we will show all possible outputs from _06-ex.Rmd, including numerical results and plots."
  },
  {
    "objectID": "post/2022/geocompr-solutions/index.html#exercises",
    "href": "post/2022/geocompr-solutions/index.html#exercises",
    "title": "Automatic website with the Geocomputation with R solutions",
    "section": "Exercises",
    "text": "Exercises\nThe above section explained how we could see exercises and solutions on the solution website. However, how is it possible that we can only see the exercise text in the book? Let’s look at the bottom of the source code of the 6th chapter at https://github.com/Robinlovelace/geocompr/blob/main/06-raster-vector.Rmd:\n## Exercises\n\n```{r, echo=FALSE, results='asis'}\nres = knitr::knit_child('_06-ex.Rmd', quiet = TRUE, options = list(include = FALSE, eval = FALSE))\ncat(res, sep = '\\n')\n```\n\nThere is only one short code chunk – it knits the same document, _06-ex.Rmd, however using different global options, include = FALSE, eval = FALSE. This assures that the book’s readers would not see any solutions’ code chunks or their outputs."
  },
  {
    "objectID": "post/2022/geocompr-solutions/index.html#workflow",
    "href": "post/2022/geocompr-solutions/index.html#workflow",
    "title": "Automatic website with the Geocomputation with R solutions",
    "section": "Workflow",
    "text": "Workflow\nNow we know all of the pieces of the puzzle. The last step is to connect them together with GitHub Actions.\nOur workflow is as follows:\n\nWe add/update/modify exercises and their solutions stored in a file per chapter, e.g., _06-ex.Rmd\nWe push the changes to the main book repository on GitHub (Robinlovelace/geocompr)\nThe book is automatically built using a GitHub Actions workflow, including exercises (but not solutions!)\nOnce a week, another GitHub Actions workflow runs in the solutions repository.\n\nThis approach ensures that the solutions are always up-to-date, reproducible, and in sync. with the manuscript. It creates a resources dedicated to the exercises and their solutions, helping people learn. It should also encourage others to get involved: can you think of another exercise or an alternative solution to existing exercises? You have all the info you need to contribute now. Let us know your comments or suggestions at https://github.com/geocompr/solutions! Pull Requests are also welcome!"
  },
  {
    "objectID": "post/2022/geocompr2-bp2/index.html",
    "href": "post/2022/geocompr2-bp2/index.html",
    "title": "Progress update: Geocomputation with R Second Edition Part 1",
    "section": "",
    "text": "Geocomputation with R is a book on geographic data analysis, visualization, and modeling. The First Edition was first published as a physical book in 2019 and we have reached 500k people through via the website at https://geocompr.robinlovelace.net/ and the physical book since then.\nThe book has also become a key part of many university courses, lectures, and has been endorsed by many people who have found it vital for their work and for learning new geographic skills, as outlined in our guestbook at geocompr.github.io/guestbook/. Tools and methods for geographic data analysis are evolving quickly, especially in the R-Spatial ecosystem. Because of these changes, and demand from our readers and the wider community, we decided in September 2021 to start working on the second edition of the book. The second edition (work in progress) is available at https://geocompr.robinlovelace.net/, while you can find the first edition at https://bookdown.org/robinlovelace/geocompr/.\nAt the time of writing (January 2022), we have already made many changes to first part of the book called Foundations. In this post we list and explain the changes."
  },
  {
    "objectID": "post/2022/geocompr2-bp2/index.html#major-changes",
    "href": "post/2022/geocompr2-bp2/index.html#major-changes",
    "title": "Progress update: Geocomputation with R Second Edition Part 1",
    "section": "Major changes",
    "text": "Major changes\nSeveral major changes have happened to #rspatial since the release of the first version of the book. Most importantly:1\n\nPROJ library was refactored mainly, which have had an impact on a plethora of GIS software, including R spatial packages\nThe raster package, highlighted in the 1st edition, is now being replaced by its successor terra\n\n\nBoth of these changes are now addressed in Geocomputation with R. Chapter “Reprojecting geographic data” has undergone numerous modifications, including an explanation of the currently recommended WKT coordinate reference systems representation, information on how to get and set CRSs, or a much-improved section on creating custom projections. Similarly, we already replaced raster’s descriptions and functions with terra in the book’s first eight chapters."
  },
  {
    "objectID": "post/2022/geocompr2-bp2/index.html#changes-to-specific-chapters",
    "href": "post/2022/geocompr2-bp2/index.html#changes-to-specific-chapters",
    "title": "Progress update: Geocomputation with R Second Edition Part 1",
    "section": "Changes to specific chapters",
    "text": "Changes to specific chapters\nAdditionally, almost all chapters received significant edits. Section The history of R-spatial in 1st chapter was updated with summary of various new #rspatial developments, including terra, stars, lidR, rayshader, ggspatial, mapsf, etc. We also used this section to highlight that packages rgdal, rgeos, and maptools will be retired at the end of 2023 and that anyone still using these packages should “transition to sf/stars/terra functions using GDAL and PROJ at your earliest convenience”.”\nSecond chapter got an information about s2, an interface to spherical geometry library already used by the sf package, and a comparison between terra and stars.\nIn Chapter 4, we added improved communication of binary spatial predicates in Chapter 4 and a new section on the dimensionally extended 9 intersection model (DE-9IM).\n\nThe vector part of Chapter 5 received a new section on the links between subsetting and clipping, while the raster part got a much improved part on resampling methods. We also decided to split out the text related raster-vector interactions into a new 6th chapter.\nFinally, Chapter 8 got a bunch of improvements, including mentions of more data packages, alternative ways of reading-in OSM data, a part about geocoding, and information about Cloud Optimized GeoTIFF (COG). We also spent some time to show how to read just a part of vector data file using OGR SQL queries and WKT filters, and extract only a portion of a COG raster file."
  },
  {
    "objectID": "post/2022/geocompr2-bp2/index.html#book-infrastructure",
    "href": "post/2022/geocompr2-bp2/index.html#book-infrastructure",
    "title": "Progress update: Geocomputation with R Second Edition Part 1",
    "section": "Book infrastructure",
    "text": "Book infrastructure\nWe also decided that the 2nd edition is a good time to improve the book infrastructure, i.e., data packages, storage of the exercises and solutions, automatic book builds, etc.\nThe most visible change is the visual theme of the book website. Now it uses the Bootstrap 4 framework through the bookdown::bs4_book theme with slightly customized elements, such as font families or a text blocks style.\nNext, we created an external bookdown-based website for solutions of the Geocomputation with R exercises. You can find its early version at https://geocompr.github.io/solutions/, and we plan to describe it in more detail in a future blog post.\nThe book still uses two external packages, spData and spDataLarge, to store example datasets. However, we made an important change here – we saved some R spatial objects to files (e.g., raster objects to GeoTIFF) – this way, we can read data using raster/terra/stars, instead of just loading one package’s class object into R memory.\nWe refactored build settings, so the book builds on Docker images in the geocompr/docker repo, and improved the experience of using the book in Binder (ideal for trying out the code before installing or updating the necessary R packages), as documented in issue #691 (thanks to yuvipanda)."
  },
  {
    "objectID": "post/2022/geocompr2-bp2/index.html#minor-changes",
    "href": "post/2022/geocompr2-bp2/index.html#minor-changes",
    "title": "Progress update: Geocomputation with R Second Edition Part 1",
    "section": "Minor changes",
    "text": "Minor changes\nWe reworked several plots using the tmap package to improve the visual consistency of the book.\n\nThe book also received (almost) countless minor changes, including rephrasing many concepts and reordering of our prose to make some ideas easier to understand."
  },
  {
    "objectID": "post/2022/geocompr2-bp2/index.html#outro",
    "href": "post/2022/geocompr2-bp2/index.html#outro",
    "title": "Progress update: Geocomputation with R Second Edition Part 1",
    "section": "Outro",
    "text": "Outro\nOur plan now is to switch attention to the second part of the book called Extensions, while Part 1 is sent for peer review. We will work on the enhanced Making maps chapter, updates on Bridges to GIS software and Statistical learning.\nThe work and updates on the second edition of Geocomputation with R would not be possible without the interest and activity of our readers! We want to thank all of you, including authors of recent Pull Requests (ec-nebi, e-clin, Oliver Leroy defuneste, Ivan Dubrovin iod-ine, Floris Vanderhaeghe florisvdh, Hasmukh K Mistry hasukmistry, John S. Erickson olyerickson) and people raising new GitHub issues. We are still actively working on the book, and thus – let us know if you have any issues or suggestions and feel free to create any Pull Request from fixing typos, clarifying unclear sentences, to changes to the code and prose. You can also contact us on the Geocomputation with R discord channel."
  },
  {
    "objectID": "r.html#translation",
    "href": "r.html#translation",
    "title": "geocompx",
    "section": "Translation",
    "text": "Translation"
  },
  {
    "objectID": "r.html#presentations",
    "href": "r.html#presentations",
    "title": "R resources",
    "section": "Presentations",
    "text": "Presentations\n\n“GIS and mapping ecosystem of R”\nEcole thématique SIGR2021, Saint-Pierre-d’Oléron, 2021-06-29\n\nslides\n\n\n\n“Recent changes in R spatial and how to be ready for them”\nWhy R? Webinar, remotely, 2020-04-23\n\nslides\nvideo\n\n\n\n“How to win friends and write an open-source book”\nuseR! 2019, Toulouse, France, 2019-07-12\n\nslides\n\n\n\n“Reproducible spatial data analysis: An example of transportation analysis for Bristol”\nCollegium Da Vinci. Poznan, Poland, 2019-01-29\n\nslides\n\n\n\n“Geocomputation with R: Data science, open source software and geo* data”\nNorthwest Universities R Day. Manchester, UK, 2018-10-31\n\nslides\n\n\n\n“Spatial data and the tidyverse”\nGEOSTAT2018. Prague, Czech Republic, 2018-08-22\n\nslides\n\n\n\n“Geocomputation with R: An overview of the field and introduction to the book”\nGEOSTAT2018. Prague, Czech Republic, 2018-08-20\n\nslides\n\n\n\n“Introduction to geocomputation with R”\nCinDay RUG. Mason, Ohio, 2018-05-22\n\nslides\n\n\n\n“Spatial data and the tidyverse”\nGeocomputation 2017. Leeds, UK, 2017-09-04\n\nslides"
  },
  {
    "objectID": "r.html#workshops",
    "href": "r.html#workshops",
    "title": "R resources",
    "section": "Workshops",
    "text": "Workshops\n\n“GIS and mapping ecosystem of R” and “Image processing and raster analysis with R”\nEcole thématique SIGR2021, Saint-Pierre-d’Oléron, 2021-06-29\n\nAll slides and additional information\n\n\n\nGeocomputation with R –\nEuropean Geosciences Union. Vienna, Austria, 2019-04-10\nTalk and practical workshop introducing packages for handling geographic data, based on chapters 2, 3 and 8 in the book\n\nIntroductory slides\nGeographic vector data in R\nGeographic raster data in R\nMaking maps with R: from static maps towards web applications\n\n\n\n“Geocomputation with R” –\neRum 2018. Budapest, Hungary, 2018-05-14\nSlides:\n\nBasics\nVector\nRaster\nViz\nrqgis\n\n\n\n“GIS with R: how to start?”\nGIS Learning Community. Cincinnati, Ohio, 2017-11-01\n\nGitHub repo"
  },
  {
    "objectID": "r.html#vignettes",
    "href": "r.html#vignettes",
    "title": "R resources",
    "section": "Vignettes",
    "text": "Vignettes\n\nThese vignettes provide extended examples for several methods and functions mentioned in Geocomputation with R\n\nSpatial Joins Extended\nMaking inset maps of the USA\nAlgorithms Extended\nPoint Pattern analysis, spatial interpolation and heatmaps\nDesire Lines Extended\nSpatial data and the tidyverse: pitfalls to avoid"
  },
  {
    "objectID": "r.html",
    "href": "r.html",
    "title": "R resources",
    "section": "",
    "text": "Geocomputation with R (geocompr) is for people who want to analyze, visualize and model geographic data with open source software. It is based on R, a statistical programming language that has powerful data processing, visualization, and geospatial capabilities. The book equips you with the knowledge and skills to tackle a wide range of issues manifested in geographic data, including those with scientific, societal, and environmental implications. The online version of the book is hosted at r.geocompx.org/.\nYou can buy it from:\n\nRoutledge.com\nAmazon.com\nAmazon.co.uk"
  },
  {
    "objectID": "r.html#solutions",
    "href": "r.html#solutions",
    "title": "R resources",
    "section": "Solutions",
    "text": "Solutions\nYou can find all the solutions to the book’s exercises at r.geocompx.org/solutions."
  },
  {
    "objectID": "r.html#translations",
    "href": "r.html#translations",
    "title": "R resources",
    "section": "Translations",
    "text": "Translations\nThe Geocomputation with R book has a few community translations:\n\nGeocomputación con R (in Spanish). Translation lead: Mireia Camacho\nGeocomputation avec R (in French). Translation lead: Olivier Leroy\n\nAdditionally, the first edition of the book has been officially translated into Japanese and Korean (check your local bookstore)."
  },
  {
    "objectID": "py.html",
    "href": "py.html",
    "title": "Python resources",
    "section": "",
    "text": "Geocomputation with Python (geocompy) is motivated by the need for an introductory, yet rigorous and up-to-date, resource geographic data with the most popular programming language in the world. A unique selling point of the book is its cohesive and joined-up coverage of both vector and raster geographic data models and consistent learning curve. We aim to minimize surprises, with each section and chapter building on the previous. If you’re just starting out with Python for working with geographic data, this book is an excellent place to start.\nInspired by the Free and Open Source Software for Geospatial (FOSS4G) movement this is an open source book. Find the code underlying the geocompy project on GitHub, ensuring that the content is reproducible, transparent, and accessible. Making the book open source allows you or anyone else, to interact with the project by opening issues, making typo fixes and more, for the benefit of everyone.\nThe online version of the book is hosted at py.geocompx.org/."
  },
  {
    "objectID": "static/presentations/libs/leaflet-binding/lib/leaflet-providers/rstudio_install.html",
    "href": "static/presentations/libs/leaflet-binding/lib/leaflet-providers/rstudio_install.html",
    "title": "geocompx",
    "section": "",
    "text": "Location\n\nfrom: github.com/schloerke/leaflet-providers@urlProtocol\nInspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‘old’ relative url protocols and to ‘upgrade’ them at js runtime.\n\n\n\nNotes…\n\nCopy/paste provider information into providers.json\n\nvar providers = L.TileLayer.Provider.providers;\nJSON.stringify(providers, null, \"  \");\n\n./data-raw/providerNames.R was re-ran to update to the latest providers\nSome providers had their protocols turned into ‘//’.\n\nThis allows browsers to pick the protocol\nTo stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize"
  },
  {
    "objectID": "static/presentations/libs/leaflet-providers/rstudio_install.html",
    "href": "static/presentations/libs/leaflet-providers/rstudio_install.html",
    "title": "geocompx",
    "section": "",
    "text": "Location\n\nfrom: github.com/schloerke/leaflet-providers@urlProtocol\nInspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‘old’ relative url protocols and to ‘upgrade’ them at js runtime.\n\n\n\nNotes…\n\nCopy/paste provider information into providers.json\n\nvar providers = L.TileLayer.Provider.providers;\nJSON.stringify(providers, null, \"  \");\n\n./data-raw/providerNames.R was re-ran to update to the latest providers\nSome providers had their protocols turned into ‘//’.\n\nThis allows browsers to pick the protocol\nTo stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize"
  },
  {
    "objectID": "static/workshops/erum2018/libs/leaflet-binding/lib/leaflet-providers/rstudio_install.html",
    "href": "static/workshops/erum2018/libs/leaflet-binding/lib/leaflet-providers/rstudio_install.html",
    "title": "geocompx",
    "section": "",
    "text": "Location\n\nfrom: github.com/schloerke/leaflet-providers@urlProtocol\nInspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‘old’ relative url protocols and to ‘upgrade’ them at js runtime.\n\n\n\nNotes…\n\nCopy/paste provider information into providers.json\n\nvar providers = L.TileLayer.Provider.providers;\nJSON.stringify(providers, null, \"  \");\n\n./data-raw/providerNames.R was re-ran to update to the latest providers\nSome providers had their protocols turned into ‘//’.\n\nThis allows browsers to pick the protocol\nTo stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize"
  },
  {
    "objectID": "static/workshops/erum2018/libs/leaflet-providers/rstudio_install.html",
    "href": "static/workshops/erum2018/libs/leaflet-providers/rstudio_install.html",
    "title": "geocompx",
    "section": "",
    "text": "Location\n\nfrom: github.com/schloerke/leaflet-providers@urlProtocol\nInspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‘old’ relative url protocols and to ‘upgrade’ them at js runtime.\n\n\n\nNotes…\n\nCopy/paste provider information into providers.json\n\nvar providers = L.TileLayer.Provider.providers;\nJSON.stringify(providers, null, \"  \");\n\n./data-raw/providerNames.R was re-ran to update to the latest providers\nSome providers had their protocols turned into ‘//’.\n\nThis allows browsers to pick the protocol\nTo stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize"
  },
  {
    "objectID": "static/workshops/libs/leaflet-binding/lib/leaflet-providers/rstudio_install.html",
    "href": "static/workshops/libs/leaflet-binding/lib/leaflet-providers/rstudio_install.html",
    "title": "geocompx",
    "section": "",
    "text": "Location\n\nfrom: github.com/schloerke/leaflet-providers@urlProtocol\nInspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‘old’ relative url protocols and to ‘upgrade’ them at js runtime.\n\n\n\nNotes…\n\nCopy/paste provider information into providers.json\n\nvar providers = L.TileLayer.Provider.providers;\nJSON.stringify(providers, null, \"  \");\n\n./data-raw/providerNames.R was re-ran to update to the latest providers\nSome providers had their protocols turned into ‘//’.\n\nThis allows browsers to pick the protocol\nTo stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize"
  },
  {
    "objectID": "static/workshops/libs/leaflet-providers/rstudio_install.html",
    "href": "static/workshops/libs/leaflet-providers/rstudio_install.html",
    "title": "geocompx",
    "section": "",
    "text": "Location\n\nfrom: github.com/schloerke/leaflet-providers@urlProtocol\nInspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‘old’ relative url protocols and to ‘upgrade’ them at js runtime.\n\n\n\nNotes…\n\nCopy/paste provider information into providers.json\n\nvar providers = L.TileLayer.Provider.providers;\nJSON.stringify(providers, null, \"  \");\n\n./data-raw/providerNames.R was re-ran to update to the latest providers\nSome providers had their protocols turned into ‘//’.\n\nThis allows browsers to pick the protocol\nTo stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize"
  },
  {
    "objectID": "guestbook.html#geocomputation-with-r",
    "href": "guestbook.html#geocomputation-with-r",
    "title": "Guestbook",
    "section": "Geocomputation with R",
    "text": "Geocomputation with R\n\nModules using the book\n\nCASA0005 Geographic Information Systems and Science, Andy MacLachlan and Adam Dennett, Centre for Advanced Spatial Analysis, Univeristy College London\nECS530 Analysing spatial data, Roger Bivand, Norwegian School of Economics\nGEOG 495 and 595: Geographic Data Analysis, University of Oregon\nESPM 288: Reproducible and Collaborative Data Science, Berkeley University of California \nENVR_SCI 390-0 “R Data Science” - Special Topics in Environmental Sciences Northwestern University\nPD-15 - R for Geospatial Analysis and Mapping, University Consortium for Geographic Information Science \nCRD 298: Spatial Methods in Community Research, Professor Noli Brazil, University of California, Davis\nCP6521 Advanced GIS, Yongsun Lee, Georgia Tech\nUrban Data Analytics in R, Nikhil Kaza, University of North Carolina at Chapel Hill\nGeospatial Analytics, taught by Dr Nick Tate as part of the MSc in Geographical Information Science at the Unversity of Leicester, UK.\nGEOG 28402 & 28403: Geographic Information Science II & III, Marynia Kolak at the Center for Spatial Data Science, Unversity of Chicago\n\n\n\nOther teaching courses/lectures/guides using the book\n\nData Science for Economists, EC 607\n11th International Summer School2019: Spatial and digital Epidemiology: Leveraging geo-referenced social media in the context of urban health\nData Carpentry: Introduction to Geospatial Raster and Vector Data with R\nR: Mapping and Geospatial guide, Duke University\n\n\n\nBook reviews\n\nI first read the first edition around 3 years ago and was astounded that everything I had been doing piecemeal in Q, Arc, and with some sf functions could be done in an orderly manner and finally the geospatial libraries were accessibly explained. Sadly at the time I did not get past the first part and thought “this is great! but need to move on!”. A month or so ago I sat down to reread the book in its entirety, with the move to terra etc.\nSee the rest of this review and maybe add your own review here.\n\n\n\nSocial media"
  }
]