{
  "hash": "f8cf8f57a203003f3641aa48ab3a3ae1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Specialized R packages for spatial cross-validation: sperrorest and blockCV\"\nauthor:\n  - name: Jakub Nowosad\n    url: https://jakubnowosad.com\n    orcid:  0000-0002-1057-3721\ndate: \"\"\nslug: sml-bp6\ncategories:\n  - rstats\n  - sml\ntags: [caret, CAST, mlr3, tidymodels, machine-learning, spatial-machine-learning]\ndraft: true\nmermaid-format: png\nknitr:\n  opts_chunk:\n    message: false\n---\n\n\n\n\n::: {.callout-note appearance=\"simple\"}\n\nThis is the sixth part of a blog post series on spatial machine learning with R.\n\nYou can find the list of other blog posts in this series [in part one](post/2025/sml-bp1/).\n\n:::\n\nThis document provides an overview of two R packages, **sperrorest** and **blockCV**, that can be used for spatial cross validation, but are outside of standard machine learning frameworks like **caret**, **tidymodels**, or **mlr3**.\n\nAll of the examples below use the same dataset, which includes the temperature measurements in Spain, a set of covariates, and the spatial coordinates of the temperature measurements.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspain <- sf::read_sf(\"https://github.com/LOEK-RS/FOSSGIS2025-examples/raw/refs/heads/main/data/spain.gpkg\")\ncovariates <- terra::rast(\"https://github.com/LOEK-RS/FOSSGIS2025-examples/raw/refs/heads/main/data/predictors.tif\")\ntemperature <- sf::read_sf(\"https://github.com/LOEK-RS/FOSSGIS2025-examples/raw/refs/heads/main/data/temp_train.gpkg\")\n\ntemperature <- terra::extract(covariates, temperature, bind = TRUE) |>\n    sf::st_as_sf()\n```\n:::\n\n\n\n\n# sperrorest\n\nThe **sperrorest** (<https://doi.org/10.32614/CRAN.package.sperrorest>) package is designed for spatial error estimation and variable importance assessment for predictive models.\nThe package itself does not fit the models but provides a set of functions for spatial cross-validation, including data partitioning and model cross-validation.\n\nWhile the **sperrorest** package has many functions (including a set of functions for data partitioning), its main function is `sperrorest()`.\nIt performs spatial cross-validation for spatial prediction models, including variable importance assessment and prediction error estimation.\nTo use this function, we need to provide the formula, the data, the coordinates, the model function, the model arguments, the prediction function, the sampling function, and the sampling arguments.\n\nLet's do it step by step.\nFirst, we need to prepare the data by extracting the coordinates and creating a data frame with the dependent variable, covariates, and coordinates.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(sperrorest)\nlibrary(ranger)\n\ncoordinates <- sf::st_coordinates(temperature)\ntemperature_df <- sf::st_drop_geometry(temperature)\ntemperature_df$x <- coordinates[, 1]\ntemperature_df$y <- coordinates[, 2]\n```\n:::\n\n\n\n\nSecond, we need to define the formula for the model and the prediction function.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nresponse_name <- \"temp\"\ncovariate_names <- colnames(temperature_df)[2:(ncol(temperature_df) - 7)]\nfo <- as.formula(paste(\n    response_name,\n    \"~\",\n    paste(covariate_names, collapse = \" + \")\n))\n```\n:::\n\n\n\n\nThird, we need to define the custom prediction function.\nThe **sperrorest** package works with many model functions, but it requires a custom prediction function to extract the predictions from the model object.\nIn this example, we use the `ranger` model, so we need to define a custom prediction function that extracts the predictions from the `ranger` model object.\nThe `predict()` function from the `ranger` package returns a list with several elements, so we need to extract the predictions from this list.^[More information on the custom prediction functions is at <https://cran.r-project.org/web/packages/sperrorest/vignettes/custom-pred-and-model-functions.html>.]\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmypred <- function(object, newdata) {\n    predict(object, newdata)$predictions\n}\n```\n:::\n\n\n\n\nFourth, we can perform the spatial cross-validation using the `sperrorest()` function.\nWe just need to provide previously prepared data, the formula, the model function, and the prediction function.\nMoreover, we can also define some additional parameters of the model, such as the number of trees in the `ranger` model.\nFinally, the important part is to define the sampling function (`smp_fun`) and its arguments (`smp_args`).\nThe sampling function is used to partition the data into training and testing sets: here, we use the `partition_kmeans()` function to partition the data spatially into folds using k-means clustering of the coordinates.^[There are several other partition functions available in the package, including `partition_disc()`, `partition_tiles()`, and  `partition_cv()`.]\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Spatial cross-validation\nset.seed(6)\nsp_res <- sperrorest(\n    formula = fo,\n    data = temperature_df,\n    coords = c(\"x\", \"y\"),\n    model_fun = ranger,\n    model_args = list(num.trees = 100),\n    pred_fun = mypred,\n    smp_fun = partition_kmeans,\n    smp_args = list(repetition = 1:2, nfold = 3),\n    progress = FALSE\n)\n```\n:::\n\n\n\n\nThe result is a list with several components, including the error at the repetition and fold levels, the resampling object, the variable importance (only when `importance = TRUE`), the benchmark, and the package version.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(sp_res$error_rep)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n                      mean           sd        median          IQR\ntrain_bias     0.003667002 0.0224183696   0.003667002 0.0158521812\ntrain_stddev   0.399217628 0.0044832845   0.399217628 0.0031701609\ntrain_rmse     0.399038491 0.0042677806   0.399038491 0.0030177766\ntrain_mad      0.316892003 0.0078406248   0.316892003 0.0055441590\ntrain_median   0.033006028 0.0001897285   0.033006028 0.0001341583\ntrain_iqr      0.427813987 0.0083456683   0.427813987 0.0059012787\ntrain_count  390.000000000 0.0000000000 390.000000000 0.0000000000\ntest_bias     -0.009090677 0.0742284999  -0.009090677 0.0524874756\ntest_stddev    1.447676903 0.0934364402   1.447676903 0.0660695405\ntest_rmse      1.444959463 0.0926651037   1.444959463 0.0655241232\ntest_mad       1.341466967 0.1144815717   1.341466967 0.0809506957\ntest_median    0.101932523 0.0950544902   0.101932523 0.0672136746\ntest_iqr       1.876642239 0.1533036116   1.876642239 0.1084020233\ntest_count   195.000000000 0.0000000000 195.000000000 0.0000000000\n```\n\n\n:::\n:::\n\n\n\n\nWe can contrast the obtained results with the non-spatial cross-validation by changing the sampling function to `partition_cv()`.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Non-spatial cross-validation\nset.seed(11)\nnsp_res <- sperrorest(\n    formula = fo,\n    data = temperature_df,\n    coords = c(\"x\", \"y\"),\n    model_fun = ranger,\n    model_args = list(num.trees = 100),\n    pred_fun = mypred,\n    smp_fun = partition_cv,\n    smp_args = list(repetition = 1:2, nfold = 3),\n    progress = FALSE\n)\n```\n:::\n\n\n\n\nTo compare both results, we can plot the RMSE values for the training and testing sets of both spatial and non-spatial cross-validation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n# Extract train/test RMSE from spatial CV\nsp_train_rmse <- sp_res$error_rep$train_rmse\nsp_test_rmse <- sp_res$error_rep$test_rmse\n# Extract train/test RMSE from non-spatial CV\nnsp_train_rmse <- nsp_res$error_rep$train_rmse\nnsp_test_rmse <- nsp_res$error_rep$test_rmse\n# Build data frame\nrmse_df <- data.frame(\n    CV_Type = rep(c(\"Spatial\", \"Non-Spatial\"), each = 4),\n    Set = rep(c(\"Train\", \"Test\"), each = 2),\n    RMSE = c(sp_train_rmse, sp_test_rmse, nsp_train_rmse, nsp_test_rmse)\n)\nggplot(rmse_df, aes(x = CV_Type, y = RMSE, fill = Set)) +\n    geom_boxplot() +\n    facet_wrap(~Set) +\n    labs(title = \"RMSE Comparison\", x = \"CV Method\", y = \"RMSE\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/misc-scv-8-1.png){width=672}\n:::\n:::\n\n\n\n\nThe results show that the estimation using the spatial-cross validation is less optimistic than the non-spatial cross-validation for the test set.\n\nMore examples of the package use can be found at <https://giscience-fsu.github.io/sperrorest/articles/spatial-modeling-use-case.html>/\n\n# blockCV\n\nThe **blockCV** (<https://doi.org/10.1111/2041-210X.13107>) package provides a set of functions for block cross-validation, spatial and environmental clustering, and spatial autocorrelation estimation.\nThe package itself does not fit the models.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(blockCV)\n```\n:::\n\n\n\n\nCross-validation strategies separate the data into training and testing sets to evaluate the model's performance.\nThe **blockCV** package provides several cross-validation strategies, including block cross-validation, spatial clustering, environmental clustering, buffering LOO, and Nearest Neighbour Distance Matching (NNDM) LOO.\n\nThe block cross-validation is performed using the `cv_spatial()` function.\nIt assigns blocks to the training and testing folds randomly, systematically or in a checkerboard pattern (the `selection` argument).\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(61)\nsb1 <- cv_spatial(\n    x = temperature,\n    k = 10, # number of folds\n    size = 300000, # size of the blocks in meters\n    selection = \"random\", # random blocks-to-fold\n    iteration = 50, # find evenly dispersed folds\n    progress = FALSE,\n    biomod2 = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   train test\n1    170   25\n2    169   26\n3    171   24\n4    180   15\n5    181   14\n6    183   12\n7    182   13\n8    165   30\n9    179   16\n10   175   20\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/misc-scv-10-1.png){width=672}\n:::\n:::\n\n\n\n\nThe result is a list with several components, including the folds list, the folds IDs, the biomod table, the number of folds, the input size, the column name, the blocks, and the records.\nFor example, we can check the structure of the folds list with the `str()` function.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(sb1$folds_list)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 10\n $ :List of 2\n  ..$ : int [1:170] 117 87 103 84 105 99 119 101 98 97 ...\n  ..$ : int [1:25] 23 31 57 39 58 60 21 22 32 27 ...\n $ :List of 2\n  ..$ : int [1:169] 117 87 103 84 105 99 119 101 98 97 ...\n  ..$ : int [1:26] 153 155 93 108 95 91 88 154 94 140 ...\n $ :List of 2\n  ..$ : int [1:171] 117 87 103 84 105 99 119 101 98 97 ...\n  ..$ : int [1:24] 12 45 175 176 166 20 177 19 167 16 ...\n $ :List of 2\n  ..$ : int [1:180] 117 87 103 84 105 99 119 101 98 97 ...\n  ..$ : int [1:15] 134 138 136 133 187 192 174 173 189 190 ...\n $ :List of 2\n  ..$ : int [1:181] 117 87 103 84 105 99 119 101 98 97 ...\n  ..$ : int [1:14] 161 142 143 150 160 149 146 147 158 151 ...\n $ :List of 2\n  ..$ : int [1:183] 117 87 103 84 105 99 119 101 98 97 ...\n  ..$ : int [1:12] 4 8 5 2 193 1 6 194 164 3 ...\n $ :List of 2\n  ..$ : int [1:182] 117 87 103 84 105 99 119 101 98 97 ...\n  ..$ : int [1:13] 128 124 129 130 127 132 131 120 126 125 ...\n $ :List of 2\n  ..$ : int [1:165] 117 87 103 84 105 99 119 101 98 97 ...\n  ..$ : int [1:30] 61 70 75 52 77 83 79 86 50 48 ...\n $ :List of 2\n  ..$ : int [1:179] 23 31 57 39 58 60 21 22 32 27 ...\n  ..$ : int [1:16] 117 87 103 84 105 99 119 101 98 97 ...\n $ :List of 2\n  ..$ : int [1:175] 117 87 103 84 105 99 119 101 98 97 ...\n  ..$ : int [1:20] 163 181 159 157 186 67 71 41 178 179 ...\n```\n\n\n:::\n:::\n\n\n\n\nThe `cv_plot()` function additionally allows for the visualization of cross-validation results.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_plot(sb1, temperature)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/misc-scv-12-1.png){width=672}\n:::\n:::\n\n\n\n\nLet's compare the results of the block cross-validation with systematic and checkerboard patterns.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(16)\nsb2 <- cv_spatial(\n    x = temperature,\n    k = 10,\n    rows_cols = c(4, 6),\n    hexagon = FALSE,\n    selection = \"systematic\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   train test\n1    172   23\n2    180   15\n3    177   18\n4    169   26\n5    178   17\n6    182   13\n7    180   15\n8    162   33\n9    178   17\n10   177   18\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/misc-scv-13-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncv_plot(sb2, temperature)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/misc-scv-13-2.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12)\nsb3 <- cv_spatial(\n    x = temperature,\n    k = 10,\n    size = 300000,\n    hexagon = FALSE,\n    selection = \"checkerboard\"\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  train test\n1    98   97\n2    97   98\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/misc-scv-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncv_plot(sb3, temperature)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/misc-scv-14-2.png){width=672}\n:::\n:::\n\n\n\n\nThe clustering strategies (`cv_cluster()`) are used to group the data into clusters based on spatial or environmental similarity.\nThe spatial similarity is based only on the clustering of the spatial coordinates. \n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(7)\nscv <- cv_cluster(x = temperature, k = 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   train test\n1    169   26\n2    169   26\n3    173   22\n4    178   17\n5    179   16\n6    182   13\n7    176   19\n8    171   24\n9    178   17\n10   180   15\n```\n\n\n:::\n\n```{.r .cell-code}\ncv_plot(scv, temperature)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/misc-scv-15-1.png){width=672}\n:::\n:::\n\n\n\n\nThe environmental clustering, on the other hand, is based on the clustering of the values of the covariates extracted from the raster data.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2)\necv <- cv_cluster(x = temperature, r = covariates, k = 5, scale = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  train test\n1   164   31\n2   190    5\n3   182   13\n4   154   41\n5    90  105\n```\n\n\n:::\n\n```{.r .cell-code}\ncv_plot(ecv, temperature)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/misc-scv-16-1.png){width=672}\n:::\n:::\n\n\n\n\nThe next cross-validation strategy is buffering LOO (also known as Spatial LOO).\nIt is performed using the `cv_buffer()` function, which selects a buffer around each point  (test point) and uses the points outside the buffer as the testing set.^[This approach is a form of leave-one-out cross-validation.]\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\nbloo <- cv_buffer(x = temperature, size = 300000, progress = FALSE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     train            test  \n Min.   : 97.0   Min.   :1  \n Mean   :132.7   Mean   :1  \n Max.   :170.0   Max.   :1  \n```\n\n\n:::\n\n```{.r .cell-code}\ncv_plot(bloo, temperature, num_plots = c(1, 50, 100))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/misc-scv-17-1.png){width=672}\n:::\n:::\n\n\n\n\nNote that above, we plot only the first, 50th, and 100th points to avoid overplotting.\n\nThe last cross-validation strategy implemented in the **blockCV** package is the Nearest Neighbour Distance Matching (NNDM) LOO.\nIt is performed using the `cv_nndm()` function, which tries to match the nearest neighbor distance distribution function between the test and training data to the nearest neighbor distance distribution function between the target prediction and training points.\nThus, in this base, we need to provide more arguments, including a raster with the covariates, the number of samples, the sampling strategy, and the minimum training size.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12)\nnncv <- cv_nndm(\n    x = temperature,\n    r = covariates,\n    size = 300000,\n    num_sample = 5000,\n    sampling = \"regular\",\n    min_train = 0.1,\n    plot = TRUE\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n     train            test  \n Min.   :192.0   Min.   :1  \n Mean   :192.9   Mean   :1  \n Max.   :193.0   Max.   :1  \n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/misc-scv-18-1.png){width=672}\n:::\n\n```{.r .cell-code}\ncv_plot(nncv, temperature, num_plots = c(1, 50, 100))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/misc-scv-18-2.png){width=672}\n:::\n:::\n\n\n\n\nLet's now use the block cross-validation to fit and evaluate a model.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# define formula\nresponse_name <- \"temp\"\ncovariate_names <- colnames(temperature_df)[2:(ncol(temperature_df) - 7)]\nfo <- as.formula(paste(\n    response_name,\n    \"~\",\n    paste(covariate_names, collapse = \" + \")\n))\n\n# extract the folds\nfolds <- sb1$folds_list\n\nmodel_rmse <- data.frame(fold = seq_along(folds), rmse = rep(NA, length(folds)))\n\nfor (k in seq_along(folds)) {\n    trainSet <- unlist(folds[[k]][1]) # training set indices; first element\n    testSet <- unlist(folds[[k]][2]) # testing set indices; second element\n    rf <- ranger(fo, temperature_df[trainSet, ], num.trees = 100) # model fitting on training set\n    pred <- predict(rf, temperature_df[testSet, ])$predictions # predict the test set\n    model_rmse[k, \"rmse\"] <- sqrt(mean(\n        (temperature_df[testSet, response_name] - pred)^2\n    )) # calculate RMSE\n}\nmodel_rmse\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   fold      rmse\n1     1 1.1401856\n2     2 0.7848595\n3     3 0.9219486\n4     4 0.8007453\n5     5 0.6485726\n6     6 1.2989341\n7     7 0.8140209\n8     8 1.2050918\n9     9 0.9696801\n10   10 0.8681127\n```\n\n\n:::\n:::\n\n\n\n\nThe **blockCV** package also provides functions for checking the similarity between the folds (`cv_similarity()`) and estimating the effective range of spatial autocorrelation (`cv_spatial_autocor()`).\nThe first function is used to check the similarity between the folds in the cross-validation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_similarity(cv = sb1, x = temperature, r = covariates, progress = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: The following aesthetics were dropped during statistical transformation:\ncolour.\nℹ This can happen when ggplot fails to infer the correct grouping structure in\n  the data.\nℹ Did you forget to specify a `group` aesthetic or to convert a numerical\n  variable into a factor?\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/misc-scv-20-1.png){width=672}\n:::\n:::\n\n\n\n\nThe second function is used to estimate the effective range of spatial autocorrelation of all input raster layers or the response data -- its role is to help to determine the size of the blocks in the block cross-validation.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncv_spatial_autocor(r = covariates, num_sample = 5000, progress = FALSE)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/misc-scv-21-1.png){width=672}\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"cv_spatial_autocor\"\n```\n\n\n:::\n:::\n\n\n\n\nMore examples of the package's use can be found at <https://cran.r-project.org/web/packages/blockCV/vignettes/tutorial_2.html>.\n\n<!-- # Closing remarks\n\nother tools, methods, packages\ndeep learning is not covered in this series\nmore in the future -->\n\n::: {.callout-note appearance=\"simple\"}\n\nThis blog post was originally written as a supplement to the poster \"An Inventory of Spatial Machine Learning Packages in R\" presented at the FOSSGIS 2025 conference in Muenster, Germany.\nThe poster is available at <https://doi.org/10.5281/zenodo.15088973>.\n\n:::",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}