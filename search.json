[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "geocompx",
    "section": "",
    "text": "Here you will find content, including books and beyond, on reproducible geographic data analysis, modeling, and visualization with open source software.\nThe geocompx project is a community-driven effort to provide resources for learning and teaching about geocomputation in multiple programming languages. It is about eXchanging information about geocomputation, cross (X) pollination of ideas from one programming language for working with geographic data to another, and hosting additional content on geocomputation with (X) other languages.\n\n\n\n\n\nGeocomputation with R\n\n\n\n\n\nGeocomputation with Python\n\n\n\nThe project and this website evolved from the website originally created to host content around Geocomputation with R. Since the publication of that book in 2019, the community around the book has grown and now includes a nascent ‘geocompy’ community and the associated Geocomputation with Python open source book project. It became clear that the ‘geocompr’ name was no longer appropriate for the more multi-lingual nature of the project."
  },
  {
    "objectID": "index.html#welcome-to-geocompx",
    "href": "index.html#welcome-to-geocompx",
    "title": "geocompx",
    "section": "",
    "text": "Here you will find content, including books and beyond, on reproducible geographic data analysis, modeling, and visualization with open source software.\nThe geocompx project is a community-driven effort to provide resources for learning and teaching about geocomputation in multiple programming languages. It is about eXchanging information about geocomputation, cross (X) pollination of ideas from one programming language for working with geographic data to another, and hosting additional content on geocomputation with (X) other languages.\n\n\n\n\n\nGeocomputation with R\n\n\n\n\n\nGeocomputation with Python\n\n\n\nThe project and this website evolved from the website originally created to host content around Geocomputation with R. Since the publication of that book in 2019, the community around the book has grown and now includes a nascent ‘geocompy’ community and the associated Geocomputation with Python open source book project. It became clear that the ‘geocompr’ name was no longer appropriate for the more multi-lingual nature of the project."
  },
  {
    "objectID": "post/2023/ogh23/index.html",
    "href": "post/2023/ogh23/index.html",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "",
    "text": "In this blog post, we talk about our experience teaching R and Python for geocomputation. The focus of the blog post is on geographic vector data, meaning points, lines, polygons (and their ‘multi’ variants) and the attributes associated with them. Geographic data analysis is a broad topic and in a later post we will cover raster data, meaning gridded data such as satellite images.\n The context of this blog post is the OpenGeoHub Summer School 2023 which has courses on R, Python and Julia. The size and the diversity of the event has grown over the years. Noting that many events focus on just one language, and the advantages of diversity of languages and approaches, we wanted to follow-up in a blog post that could be useful to others.\nOpenGeoHub 2023 was also a unique chance for the authors of the in-progress open source book, Geocomputation with Python to meet in person: the first time we have all been in one place at the same time.\nThe post is based on the following lecture notes, which we recommend checking out for deeper dives into the R and Python implementations of geocomputation:\n\nTidy geographic data with sf, dplyr, ggplot2, geos and friends\nWorking with spatial data in Python"
  },
  {
    "objectID": "post/2023/ogh23/index.html#loading-packages",
    "href": "post/2023/ogh23/index.html#loading-packages",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "Loading packages",
    "text": "Loading packages\nWe will start by loading core packages for working with geographic vector and attribute data. See detailed description of R and Python implementations in the respective lecture note sections.\n\nPythonR\n\n\n\nimport pandas as pd\nfrom shapely import Point\nimport geopandas as gpd\n\n\n\n\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tmap)"
  },
  {
    "objectID": "post/2023/ogh23/index.html#creating-geographic-data",
    "href": "post/2023/ogh23/index.html#creating-geographic-data",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "Creating geographic data",
    "text": "Creating geographic data\nThe following commands create geographic datasets ‘from scratch’ representing coordinates of a the faculty where the Summer School takes place, and few hotels, in Poznan, Poland. Most projects start with pre-generated data, but it’s useful to create datasets to understand data structures.\n\nPythonR\n\n\n\npoi = gpd.GeoDataFrame([\n    {\"name\": \"Faculty\",        \"geometry\": Point(16.9418, 52.4643)},\n    {\"name\": \"Hotel ForZa\",    \"geometry\": Point(16.9474, 52.4436)},\n    {\"name\": \"Hotel Lechicka\", \"geometry\": Point(16.9308, 52.4437)},\n    {\"name\": \"FairPlayce\",     \"geometry\": Point(16.9497, 52.4604)},\n], crs=4326)\n\n\n\n\npoi_df = tribble(\n  ~name, ~lon, ~lat,\n  \"Faculty\",        16.9418, 52.4643,\n  \"Hotel ForZa\",    16.9474, 52.4436,\n  \"Hotel Lechicka\", 16.9308, 52.4437,\n  \"FairPlayce\",     16.9497, 52.4604\n)\npoi_sf = sf::st_as_sf(poi_df, coords = c(\"lon\", \"lat\"), crs = \"EPSG:4326\")\n\n\n\n\n\nDownloading data\nThe following commands download data from the internet.\n\nPythonR\n\n\n\nimport urllib.request\nimport zipfile\nimport os\nu = \"https://github.com/Robinlovelace/opengeohub2023/releases/download/data/data.zip\"\nf = os.path.basename(u)\nif not os.path.exists(\"data\"):\n    urllib.request.urlretrieve(u, f)\n\n('data.zip', &lt;http.client.HTTPMessage object at 0x7f379b2893d0&gt;)\n\n\n\n\n\nu = \"https://github.com/Robinlovelace/opengeohub2023/releases/download/data/data.zip\"\nf = basename(u)\nif (!dir.exists(\"data\")) {\n  download.file(u, f)\n}"
  },
  {
    "objectID": "post/2023/ogh23/index.html#unzipping-data",
    "href": "post/2023/ogh23/index.html#unzipping-data",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "Unzipping data",
    "text": "Unzipping data\nUnzip the zipfile in R and Python as follows.\n\nPythonR\n\n\n\nwith zipfile.ZipFile(f, 'r') as zip_ref:\n    zip_ref.extractall()\n\n\n\n\nunzip(f)"
  },
  {
    "objectID": "post/2023/ogh23/index.html#reading-and-printing-geographic-data",
    "href": "post/2023/ogh23/index.html#reading-and-printing-geographic-data",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "Reading and printing geographic data",
    "text": "Reading and printing geographic data\nAs shown below, Python and R code to import a shapefile are similar. Note: we recommend using open file formats such as GeoPackage (.gpkg), as outlined in Geocomputation with R and Geocomputation with Python.\nNote also that we read directly from the ZIP file thanks to the GDAL Virtual File System.\n\nPythonR\n\n\n\npol_all = gpd.read_file(\"zip://data.zip!data/osm/gis_osm_transport_a_free_1.shp\")\npol_all\n\n         osm_id  ...                                           geometry\n0      27923283  ...  POLYGON ((16.84088 52.42479, 16.84112 52.42511...\n1      28396243  ...  POLYGON ((16.96750 52.32743, 16.96802 52.32807...\n2      28396249  ...  POLYGON ((16.98029 52.32399, 16.98038 52.32409...\n3      28396250  ...  POLYGON ((16.97407 52.32418, 16.97420 52.32433...\n4      30164579  ...  POLYGON ((16.71011 53.16458, 16.71041 53.16478...\n..          ...  ...                                                ...\n277  1073517768  ...  POLYGON ((16.83966 51.60933, 16.83996 51.60941...\n278  1098015810  ...  POLYGON ((16.90579 52.43677, 16.90589 52.43672...\n279  1101813658  ...  POLYGON ((16.52227 52.34424, 16.52230 52.34425...\n280  1146503680  ...  POLYGON ((16.90591 52.42877, 16.90593 52.42878...\n281  1157127550  ...  POLYGON ((18.07060 51.74824, 18.07062 51.74826...\n\n[282 rows x 5 columns]\n\n\n\n\n\npol_all = sf::read_sf(\"/vsizip/data.zip/data/osm/gis_osm_transport_a_free_1.shp\")\npol_all\n\nSimple feature collection with 282 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 15.76877 ymin: 51.42587 xmax: 18.51031 ymax: 53.52821\nGeodetic CRS:  WGS 84\n# A tibble: 282 × 5\n   osm_id    code fclass          name                                  geometry\n   &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;           &lt;chr&gt;                            &lt;POLYGON [°]&gt;\n 1 27923283  5656 apron           &lt;NA&gt;                 ((16.84088 52.42479, 16.…\n 2 28396243  5656 apron           &lt;NA&gt;                 ((16.9675 52.32743, 16.9…\n 3 28396249  5656 apron           &lt;NA&gt;                 ((16.98029 52.32399, 16.…\n 4 28396250  5656 apron           &lt;NA&gt;                 ((16.97407 52.32418, 16.…\n 5 30164579  5656 apron           &lt;NA&gt;                 ((16.71011 53.16458, 16.…\n 6 32225811  5601 railway_station Czerwonak            ((16.9798 52.46868, 16.9…\n 7 36204378  5622 bus_station     &lt;NA&gt;                 ((16.95469 52.40964, 16.…\n 8 50701732  5651 airport         Lądowisko Poznań-Be… ((17.19788 52.53491, 17.…\n 9 55590985  5622 bus_station     Dworzec PKS-stanowi… ((17.20243 52.80927, 17.…\n10 56064358  5651 airport         Port lotniczy Zielo… ((15.76877 52.13175, 15.…\n# ℹ 272 more rows\n\n\n\n\n\nNote: in both R and Python, you can read-in the dataset from the URL in a single line of code without first downloading the zip file, although the syntax is different, with R using GDAL’s /vsicurl/ character string and Python using the zip+ syntax."
  },
  {
    "objectID": "post/2023/ogh23/index.html#subsetting-by-attributes",
    "href": "post/2023/ogh23/index.html#subsetting-by-attributes",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "Subsetting by attributes",
    "text": "Subsetting by attributes\nThe following commands select a subset of the data based on attribute values (looking for a specific string in the name column).\n\nPythonR\n\n\n\npol = pol_all[pol_all['name'].str.contains('Port*.+Poz', na=False)]\npol\n\n        osm_id  ...                                           geometry\n116  342024881  ...  POLYGON ((16.80040 52.42494, 16.80060 52.42533...\n\n[1 rows x 5 columns]\n\n\n\n\n\npol = pol_all |&gt;\n  filter(str_detect(name, \"Port*.+Poz\"))\npol\n\nSimple feature collection with 1 feature and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 16.8004 ymin: 52.41373 xmax: 16.85458 ymax: 52.42736\nGeodetic CRS:  WGS 84\n# A tibble: 1 × 5\n  osm_id     code fclass  name                                          geometry\n* &lt;chr&gt;     &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;                                    &lt;POLYGON [°]&gt;\n1 342024881  5651 airport Port Lotniczy Poznań-Ławica… ((16.8004 52.42494, 16.8…"
  },
  {
    "objectID": "post/2023/ogh23/index.html#basic-plotting",
    "href": "post/2023/ogh23/index.html#basic-plotting",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "Basic plotting",
    "text": "Basic plotting\nThe following commands plot the data. Note that by default, R’s plot() method for {sf} objects creates a plot for each column in the data (up to 9 by default). \n\nPythonR\n\n\n\npol.plot();\n\n\n\n\n\n\n\n\n\n\n\nplot(pol)\n\n\n\n\n\n\n\n\n\n\n\nThe arguments needed to change the colour of the fill and border are different in R and Python, but the results are similar.\n\nPythonR\n\n\n\npol.plot(color='none', edgecolor='black');\n\n\n\n\n\n\n\n\n\n\n\nplot(st_geometry(pol), col = \"white\", border = \"black\")"
  },
  {
    "objectID": "post/2023/ogh23/index.html#creating-geographic-data-frames-from-a-csv-file",
    "href": "post/2023/ogh23/index.html#creating-geographic-data-frames-from-a-csv-file",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "Creating geographic data frames from a CSV file",
    "text": "Creating geographic data frames from a CSV file\nThe following commands create a geographic data frame from a CSV file. Note that two steps—creating the geometry column and combining it with the original table, hereby combined into one complex expression—are needed to convert a DataFrame to a GeoDataFrame in Python, whereas in R the sf::st_as_sf() function can be used to convert a data.frame to a spatial data frame directly.\n\nPythonR\n\n\n\n# Unzip the data.zip file:\nwith zipfile.ZipFile(f, 'r') as zip_ref:\n    zip_ref.extractall(\"data\")\nstops = pd.read_csv(\"data/gtfs/stops.txt\")\nstops = gpd.GeoDataFrame(\n    stops.drop(columns=['stop_lon', 'stop_lat', 'stop_code']),\n    geometry = gpd.points_from_xy(stops.stop_lon, stops.stop_lat),\n    crs = 4326)\nstops\n\n      stop_id  ...                   geometry\n0        2186  ...  POINT (17.04263 52.32684)\n1         355  ...  POINT (16.86888 52.46234)\n2        4204  ...  POINT (16.78629 52.47810)\n3        3885  ...  POINT (16.72401 52.47590)\n4         494  ...  POINT (16.93085 52.43616)\n...       ...  ...                        ...\n2916     2099  ...  POINT (16.65026 52.47006)\n2917     3915  ...  POINT (16.98360 52.38233)\n2918     3876  ...  POINT (16.52949 52.49770)\n2919      594  ...  POINT (16.80900 52.43642)\n2920     1190  ...  POINT (16.99819 52.44124)\n\n[2921 rows x 4 columns]\n\n\n\n\n\nstops = read_csv(\"data/gtfs/stops.txt\") |&gt;\n  select(-stop_code) |&gt;\n  st_as_sf(coords = c(\"stop_lon\", \"stop_lat\"), crs = \"EPSG:4326\")\nstops\n\nSimple feature collection with 2921 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 16.48119 ymin: 52.15166 xmax: 17.27693 ymax: 52.58723\nGeodetic CRS:  WGS 84\n# A tibble: 2,921 × 4\n   stop_id stop_name              zone_id            geometry\n *   &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt;           &lt;POINT [°]&gt;\n 1    2186 Żerniki/Skrzyżowanie   B       (17.04263 52.32684)\n 2     355 Sucholeska             A       (16.86888 52.46234)\n 3    4204 Pawłowicka             A        (16.78629 52.4781)\n 4    3885 Kobylniki/Karolewska   B        (16.72401 52.4759)\n 5     494 Połabska               A       (16.93085 52.43616)\n 6    2040 Tarnowo Pdg/Karolewo I C       (16.68462 52.46915)\n 7    3736 Komorniki/Kryształowa  B        (16.78291 52.3478)\n 8    3932 Unii Lubelskiej        A        (16.9497 52.37239)\n 9    2795 Potasze/Jodłowa        C       (17.02994 52.52445)\n10    3861 Miękowo/Stokrotkowa    C       (16.98954 52.49024)\n# ℹ 2,911 more rows"
  },
  {
    "objectID": "post/2023/ogh23/index.html#plotting-attributes-and-layers",
    "href": "post/2023/ogh23/index.html#plotting-attributes-and-layers",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "Plotting attributes and layers",
    "text": "Plotting attributes and layers\nThe following commands plot the bus stops loaded in the previous step. Note that the tmap package is hereby used in R to create these more advanced plots, as it also supports interactive mapping (see below).\n\nPythonR\n\n\n\nstops.plot(markersize=1, column='zone_id', legend=True);\n\n\n\n\n\n\n\n\n\n\n\ntm_shape(stops) +\n  tm_symbols(size = 0.1, col = \"zone_id\")\n\n\n\n\n\n\n\n\n\n\n\nWe can add basic overlays in both languages as follows.\n\nPythonR\n\n\n\nbase = stops.plot(markersize=0.1)\npoi.plot(ax=base, color='red');\n\n\n\n\n\n\n\n\n\n\n\n\nplot(stops$geometry, col = \"grey\", pch = 20, cex = 0.5)\nplot(poi_sf$geometry, col = \"red\", add = TRUE)"
  },
  {
    "objectID": "post/2023/ogh23/index.html#interactive-plots",
    "href": "post/2023/ogh23/index.html#interactive-plots",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "Interactive plots",
    "text": "Interactive plots\nThe following commands create interactive plots, in Python and R respectively. The Python code requires the folium and mapclassify packages, which are not installed by default when you install geopandas. Note that with tmap, you can use the same code to create static and interactive plots, by changing the tmap_mode().\n\nPythonR\n\n\n\nstops.explore(column='zone_id', legend=True, cmap='Dark2')\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n\n\ntmap_mode(\"view\")\ntm_shape(stops) +\n  tm_symbols(size = 0.1, col = \"zone_id\")"
  },
  {
    "objectID": "post/2023/ogh23/index.html#reprojecting-data",
    "href": "post/2023/ogh23/index.html#reprojecting-data",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "Reprojecting data",
    "text": "Reprojecting data\nThe following commands reproject the data to a local projected Coordinate Reference System (CRS).\n\nPythonR\n\n\n\npoi.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\npoi_projected = poi.to_crs(2180)\nstops_projected = stops.to_crs(2180)\n\n\n\n\nst_crs(poi_sf)\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n        MEMBER[\"World Geodetic System 1984 (Transit)\"],\n        MEMBER[\"World Geodetic System 1984 (G730)\"],\n        MEMBER[\"World Geodetic System 1984 (G873)\"],\n        MEMBER[\"World Geodetic System 1984 (G1150)\"],\n        MEMBER[\"World Geodetic System 1984 (G1674)\"],\n        MEMBER[\"World Geodetic System 1984 (G1762)\"],\n        MEMBER[\"World Geodetic System 1984 (G2139)\"],\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]],\n        ENSEMBLEACCURACY[2.0]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\npoi_projected = st_transform(poi_sf, 2180)\nstops_projected = st_transform(stops, 2180)"
  },
  {
    "objectID": "post/2023/ogh23/index.html#buffers",
    "href": "post/2023/ogh23/index.html#buffers",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "Buffers",
    "text": "Buffers\nThe following commands create buffers around the points. Note that R allows buffer to be created directly from a spatial data frame with geographic (lon/lot) coordinates thanks to its integration with Google’s S2 spherical geometry engine, as outlined in Geocomputation with R. For buffer operations to work in Python you must reproject the data first (which we did, see above) (although there are plans for geopandas to support a spherical geometry backend at some point, as discussed in issue #2098).\n\nPythonR\n\n\n\nTo create a new vector layers named poi_buffer, in both languages, we can do the following.\n\npoi_buffer = poi.copy()\npoi_buffer.geometry = poi_projected.buffer(150).to_crs(4326)\n\n\n\n\npoi_buffer = st_buffer(poi_sf, 150)"
  },
  {
    "objectID": "post/2023/ogh23/index.html#calculating-distances-and-areas",
    "href": "post/2023/ogh23/index.html#calculating-distances-and-areas",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "Calculating distances and areas",
    "text": "Calculating distances and areas\nAn interesting difference between R and Python is that the former uses the units package to store units, making it easy to convert between them, as outlined in the buffers section of the R lecture notes.\n\nPythonR\n\n\n\npoi_buffer.to_crs(2180).area\n\n0    70572.341037\n1    70572.341037\n2    70572.341037\n3    70572.341037\ndtype: float64\n\n\n\n\n\nst_area(poi_buffer)\n\nUnits: [m^2]\n[1] 71656.68 71644.28 71656.92 71667.15"
  },
  {
    "objectID": "post/2023/ogh23/index.html#spatial-subsetting",
    "href": "post/2023/ogh23/index.html#spatial-subsetting",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "Spatial subsetting",
    "text": "Spatial subsetting\nCode to subset the bus stops within the buffered poi points is shown below. The R code is more concise because there is a special [ notation for the specific case of subsetting by intersection. In Python you must undertake the explicit steps, which are applicable to any predicate in both languages:\n\nTake the unary union of the buffered points before subsetting\nCreate a boolean Series object with the .intersects or other method, and use the boolean Series to subset the data (rather than another geographic object)\n\n\nPythonR\n\n\n\npoi_union = poi_buffer.unary_union\nsel = stops.intersects(poi_union)\nstops_in_b = stops[sel]\nstops_in_b\n\n      stop_id              stop_name zone_id                   geometry\n295       418  UAM Wydział Geografii       A  POINT (16.94108 52.46419)\n681       467             Umultowska       A  POINT (16.92882 52.44426)\n1724      468             Umultowska       A  POINT (16.93039 52.44307)\n1861      417  UAM Wydział Geografii       A  POINT (16.94161 52.46530)\n\n\n\n\n\nstops_in_b = stops[poi_buffer, ]\nstops_in_b\n\nSimple feature collection with 4 features and 3 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 16.92882 ymin: 52.44307 xmax: 16.94161 ymax: 52.4653\nGeodetic CRS:  WGS 84\n# A tibble: 4 × 4\n  stop_id stop_name             zone_id            geometry\n    &lt;dbl&gt; &lt;chr&gt;                 &lt;chr&gt;           &lt;POINT [°]&gt;\n1     418 UAM Wydział Geografii A       (16.94108 52.46419)\n2     467 Umultowska            A       (16.92882 52.44426)\n3     468 Umultowska            A       (16.93039 52.44307)\n4     417 UAM Wydział Geografii A        (16.94161 52.4653)"
  },
  {
    "objectID": "post/2023/ogh23/index.html#spatial-joins",
    "href": "post/2023/ogh23/index.html#spatial-joins",
    "title": "Geographic data analysis in R and Python: comparing code and outputs for vector data",
    "section": "Spatial joins",
    "text": "Spatial joins\nSpatial joins are implemented with similar functions in R and Python and the outputs are the same. See the Python and R tutorials, and in Geocomputation with R Section 4.2.4 and Geocomputation with Python 3.3.4 for more details.\n\nPythonR\n\n\n\npoi_buffer.sjoin(stops, how='left')\n\n             name  ... zone_id\n0         Faculty  ...       A\n0         Faculty  ...       A\n1     Hotel ForZa  ...     NaN\n2  Hotel Lechicka  ...       A\n2  Hotel Lechicka  ...       A\n3      FairPlayce  ...     NaN\n\n[6 rows x 6 columns]\n\n\n\n\n\nst_join(poi_buffer, stops)\n\nSimple feature collection with 6 features and 4 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 16.92856 ymin: 52.44223 xmax: 16.95195 ymax: 52.46567\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 5\n  name                                        geometry stop_id stop_name zone_id\n* &lt;chr&gt;                                  &lt;POLYGON [°]&gt;   &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  \n1 Faculty        ((16.93959 52.46441, 16.93957 52.464…     418 UAM Wydz… A      \n2 Faculty        ((16.93959 52.46441, 16.93957 52.464…     417 UAM Wydz… A      \n3 Hotel ForZa    ((16.94759 52.44224, 16.94762 52.442…      NA &lt;NA&gt;      &lt;NA&gt;   \n4 Hotel Lechicka ((16.93275 52.44435, 16.93275 52.444…     467 Umultows… A      \n5 Hotel Lechicka ((16.93275 52.44435, 16.93275 52.444…     468 Umultows… A      \n6 FairPlayce     ((16.9477 52.461, 16.94765 52.46092,…      NA &lt;NA&gt;      &lt;NA&gt;"
  },
  {
    "objectID": "post/2023/geocompy-bp1/index.html",
    "href": "post/2023/geocompy-bp1/index.html",
    "title": "Say hi to ‘Geocomputation with Python’",
    "section": "",
    "text": "Geocomputation with Python, also known as geocompy, is an open-source book on geographic data analysis with Python.1 It is written by Michael Dorman, Anita Graser, Robin Lovelace, and me with contributions from others. You can find it online at py.geocompx.org.\nWe started working on this project at the beginning of 2022, and since then, we have been making steady progress. The book is currently in the late stages of development, and we hope to release the first edition in 2024.2 This blog post briefly introduces the project, its goals, and its current status. It also serves as an invitation for contributions and feedback."
  },
  {
    "objectID": "post/2023/geocompy-bp1/index.html#footnotes",
    "href": "post/2023/geocompy-bp1/index.html#footnotes",
    "title": "Say hi to ‘Geocomputation with Python’",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe book is also a sibling project of Geocomputation with R (geocompr), a book on geographic data analysis, visualization, and modeling using the R programming language. The geocompy follows the style and structure of the first part of geocompr.↩︎\nAs a physical book with CRC Press.↩︎"
  },
  {
    "objectID": "post/2024/rsi-bp1/index.html",
    "href": "post/2024/rsi-bp1/index.html",
    "title": "An overview of the rsi R package for retrieving satellite imagery and calculating spectral indices",
    "section": "",
    "text": "Introduction\nrsi is a recent R package developed by Michael Mahoney and funded by Permian Global Research. It offers features that simplify the process of acquiring spatial data from STAC (SpatioTemporal Asset Catalog) and calculating spectral indices based on such data. A unique feature of this package is its source for the indices. Instead of providing a static list of available formulas, rsi obtains them from Awesome Spectral Indices: a curated repository of over 200 indices covering multiple application domains. The combination of satellite imagery access through STAC and a constantly expanding spectral indices repository significantly simplifies remote sensing processes and creates new opportunities, including the automation of calculating spectral indices over a wide span of time and area.\n\n\nSet up\nrsi is available in development version on GitHub, which can be installed with pak.\n\npak::pak(\"Permian-Global-Research/rsi\")\n\nTo acquire the newest Awesome Spectral Indices dataset, we can call spectral_indices(). Calling it without passing any arguments will result in using cached version of the tibble, which is updated automatically once a day. This tibble contains column with indices formulas, which are using standardized band names.\n\nlibrary(rsi)\nasi = spectral_indices(download_indices = TRUE)\nasi\n\n# A tibble: 231 × 9\n   application_domain bands     contributor   date_of_addition formula long_name\n   &lt;chr&gt;              &lt;list&gt;    &lt;chr&gt;         &lt;chr&gt;            &lt;chr&gt;   &lt;chr&gt;    \n 1 vegetation         &lt;chr [2]&gt; https://gith… 2021-11-17       (N - 0… Aerosol …\n 2 vegetation         &lt;chr [2]&gt; https://gith… 2021-11-17       (N - 0… Aerosol …\n 3 water              &lt;chr [6]&gt; https://gith… 2022-09-22       (B + G… Augmente…\n 4 vegetation         &lt;chr [2]&gt; https://gith… 2021-09-20       (1 / G… Anthocya…\n 5 vegetation         &lt;chr [3]&gt; https://gith… 2022-04-08       N * ((… Anthocya…\n 6 vegetation         &lt;chr [4]&gt; https://gith… 2021-05-11       (N - (… Atmosphe…\n 7 vegetation         &lt;chr [4]&gt; https://gith… 2021-05-14       sla * … Adjusted…\n 8 vegetation         &lt;chr [2]&gt; https://gith… 2022-04-08       (N * (… Advanced…\n 9 water              &lt;chr [4]&gt; https://gith… 2021-09-18       4.0 * … Automate…\n10 water              &lt;chr [5]&gt; https://gith… 2021-09-18       B + 2.… Automate…\n# ℹ 221 more rows\n# ℹ 3 more variables: platforms &lt;list&gt;, reference &lt;chr&gt;, short_name &lt;chr&gt;\n\n\n\n\nAcquiring STAC data\nrsi provides get_stac_data() function, built around the rstac package. It allows to connect to specified STAC source, and access data, based on selected attributes, like geospatial area of interest, date of acquisition of imagery, collection name, etc. For users convenience, rsi comes with additional functions that facilitate get_stac_data() function, providing necessary parameters, and limiting users input to specifying area of interest and a time frame of imagery. Those functions allow to access Landsat, Sentinel 1-2 imagery and digital elevation models (DEMs), available on Microsoft’s Planetary Computer.\nLet’s start with creating our area of interest. get_stac_data() accepts sf objects from which it extracts boundaries, so we can load in already existing data source, or create our own. For this example, we will create area of interest of 5000 meters around a point in San Antonio.\n\nlibrary(sf)\nsan_antonio = st_point(c(-98.491142, 29.424349))\nsan_antonio = st_sfc(san_antonio, crs = \"EPSG:4326\")\nsan_antonio = st_buffer(st_transform(san_antonio, \"EPSG:3081\"), 5000)\n\nHaving specified the area of interest, we can create our query. We will start with downloading Landsat data for September and October 2023 and save it to a temporary file.\n\nsa_landsat = get_landsat_imagery(\n    san_antonio,\n    start_date = \"2023-09-01\",\n    end_date = \"2023-10-31\",\n    output_filename = tempfile(fileext = \".tif\")\n)\nsa_landsat\n\n[1] \"/tmp/Rtmppind2Y/file501082fbc26c0.tif\"\n\n\nget_stac_data() and its child functions return the path to downloaded image. As a default, they create the composite image out of median values of all avaiable images for specified time span. We can specify other composite functions, like mean, sum, min or max. We can also not use composite function at all (composite_function = NULL), and obtain all available images for specified time span. Another default argument, passed in for mask_band, automatically masks clouds with NAs.\n\nsa_sentinel2 = get_sentinel2_imagery(\n    san_antonio,\n    start_date = \"2023-09-01\",\n    end_date = \"2023-10-31\",\n    output_filename = tempfile(fileext = \".tif\")\n)\n\nIn this example, we download Sentinel 2 images for September and October 2023, mask out clouds, and create a composite image.\nLet’s also obtain the DEM of our area.\n\nsa_dem = get_dem(\n    san_antonio,\n    output_filename = tempfile(fileext = \".tif\")\n)\n\nAfter downloading the images, we can load them into SpatRaster objects using rast(). rsi automatically assigns proper band names to it, which follow Awesome Spectral Indices standard.\n\nlibrary(terra)\nsa_landsat_rast = terra::rast(sa_landsat)\nsa_sentinel2_rast = terra::rast(sa_sentinel2)\nsa_dem_rast = terra::rast(sa_dem)\nsa_landsat_rast\n\nclass       : SpatRaster \ndimensions  : 333, 333, 8  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 1141170, 1151160, 803238.9, 813228.9  (xmin, xmax, ymin, ymax)\ncoord. ref. : NAD83 / Texas State Mapping System (EPSG:3081) \nsource      : file501082fbc26c0.tif \nnames       : A, B, G, R, N, S1, ... \n\n\nWe can plot the rasters together to compare Landsat and Sentinel images and visualize DEM.\n\npar(mfrow = c(1, 3))\nterra::plotRGB(sa_landsat_rast, r = 4, g = 3, b = 2, stretch = \"lin\")\nterra::plotRGB(sa_sentinel2_rast, r = 4, g = 3, b = 2, stretch = \"lin\")\nterra::plot(sa_dem_rast)\n\n\n\n\n\n\n\n\n\n\nCalculating spectral indices\nHaving downloaded images, we can use them to calculate spectral indices. In this example, we will observe the change in vegetation between two dates for the Swarzędz area in Poland. Let’s start by creating a new area of interest.\n\nswarzedz = st_point(c(17.108174, 52.405725))\nswarzedz = st_set_crs(st_sfc(swarzedz), \"EPSG:4326\")\nswarzedz = st_buffer(st_transform(swarzedz, \"EPSG:2180\"), 5000)\n\nFor the most accurate results, we will use Sentinel 2 data. The date span is set from July to September 2023. We will also pass in composite_function = NULL, so that all available images will be downloaded separately.\n\nswarzedz_sentinel2_sep = get_sentinel2_imagery(\n    swarzedz,\n    start_date = \"2023-07-01\",\n    end_date = \"2023-09-30\",\n    output_filename = tempfile(fileext = \".tif\"),\n    composite_function = NULL\n)\n\nswarzedz_sentinel2_sep\n\n [1] \"/tmp/Rtmppind2Y/file5010818963908_2023-09-27T10:00:31.024000Z.tif\"\n [2] \"/tmp/Rtmppind2Y/file5010818963908_2023-09-22T09:56:49.024000Z.tif\"\n [3] \"/tmp/Rtmppind2Y/file5010818963908_2023-09-17T10:00:31.024000Z.tif\"\n [4] \"/tmp/Rtmppind2Y/file5010818963908_2023-09-12T09:56:09.024000Z.tif\"\n [5] \"/tmp/Rtmppind2Y/file5010818963908_2023-09-07T10:00:31.024000Z.tif\"\n [6] \"/tmp/Rtmppind2Y/file5010818963908_2023-09-02T09:55:59.024000Z.tif\"\n [7] \"/tmp/Rtmppind2Y/file5010818963908_2023-08-28T10:00:31.025000Z.tif\"\n [8] \"/tmp/Rtmppind2Y/file5010818963908_2023-08-23T09:55:59.024000Z.tif\"\n [9] \"/tmp/Rtmppind2Y/file5010818963908_2023-08-18T10:00:31.024000Z.tif\"\n[10] \"/tmp/Rtmppind2Y/file5010818963908_2023-08-13T09:55:59.024000Z.tif\"\n[11] \"/tmp/Rtmppind2Y/file5010818963908_2023-08-08T10:00:31.024000Z.tif\"\n[12] \"/tmp/Rtmppind2Y/file5010818963908_2023-08-03T09:55:59.024000Z.tif\"\n[13] \"/tmp/Rtmppind2Y/file5010818963908_2023-07-29T10:00:31.024000Z.tif\"\n[14] \"/tmp/Rtmppind2Y/file5010818963908_2023-07-19T10:00:31.024000Z.tif\"\n[15] \"/tmp/Rtmppind2Y/file5010818963908_2023-07-14T09:55:59.024000Z.tif\"\n[16] \"/tmp/Rtmppind2Y/file5010818963908_2023-07-09T10:00:31.024000Z.tif\"\n[17] \"/tmp/Rtmppind2Y/file5010818963908_2023-07-04T09:55:59.024000Z.tif\"\n\n\nrsi allows for creating custom query functions using CQL2, which we can use to filter out products that do not meet our criteria. A common use case is filtering items based on their cloud coverage. For instance, if we want to download Sentinel-2 imagery with cloud cover below 25%, we can define our query function as follows:\n\nsentinel2_25cc_qf &lt;- function(bbox, stac_source, start_date, end_date, limit, ...) {\n    geom &lt;- rstac::cql2_bbox_as_geojson(bbox)\n    datetime &lt;- rstac::cql2_interval(start_date, end_date)\n    request &lt;- rstac::ext_filter(\n        rstac::stac(stac_source),\n        collection == \"sentinel-2-l2a\" &&\n        t_intersects(datetime, {{datetime}}) &&\n        s_intersects(geom, {{geom}}) &&\n        platform == \"Sentinel-2B\" &&\n        `eo:cloud_cover` &lt; 25\n        )\n    rstac::items_fetch(rstac::post_request(request))\n}\n\nWe could pass it in using the code get_stac_data(query_function = sentinel2_25cc_qf). This is an optional argument, as rsi provides a default query function. We can then continue our work with the results stored in swarzedz_sentinel2_sep, selecting two images that have low cloud coverage.\n\nswarzedz_sentinel2_07_09 = terra::rast(swarzedz_sentinel2_sep[16])\nswarzedz_sentinel2_09_27 = terra::rast(swarzedz_sentinel2_sep[1])\n\nrsi’s calculate_indices() requires a data frame of indices to calculate. Since we want to calculate NDVI, all we need to do is extract the row with NDVI’s formula. We can use the short_name column for that.\n\nndvi = asi[asi$short_name == \"NDVI\", ]\nswarzedz_sentinel2_07_09_ndvi = calculate_indices(\n    swarzedz_sentinel2_07_09,\n    ndvi,\n    output_filename = tempfile(fileext = \".tif\")\n)\nswarzedz_sentinel2_09_27_ndvi = calculate_indices(\n    swarzedz_sentinel2_09_27,\n    ndvi,\n    output_filename = tempfile(fileext = \".tif\")\n)\nswarzedz_sentinel2_07_09_ndvi\n\n[1] \"/tmp/Rtmppind2Y/file501084d439b94.tif\"\n\n\nWe now can plot both rasters to see the values that were calculated for both dates.\n\npar(mfrow = c(1, 2))\nswarzedz_sentinel2_07_09_ndvi_rast = terra::rast(swarzedz_sentinel2_07_09_ndvi)\nswarzedz_sentinel2_09_27_ndvi_rast = terra::rast(swarzedz_sentinel2_09_27_ndvi)\nterra::plot(swarzedz_sentinel2_07_09_ndvi_rast, range = c(-1, 1))\nterra::plot(swarzedz_sentinel2_09_27_ndvi_rast, range = c(-1, 1))\n\n\n\n\n\n\n\n\nTo better visualize the change in NDVI over time, we can create a difference raster. Values above 0 indicate that NDVI was higher on October 27 than on July 9. Values below 0 indicate that the NDVI values decreased on October 27, compared to July 9. Values equal to 0 mean that NDVI values haven’t changed over time.\n\npar(mfrow = c(1, 2))\ndif = swarzedz_sentinel2_09_27_ndvi_rast - swarzedz_sentinel2_07_09_ndvi_rast\nterra::plot(dif)\nhist(dif, main = \"\", xlab = \"NDVI\")\n\n\n\n\n\n\n\n\nWe can save both rasters as one, using stack_rasters() function.\n\nstack = stack_rasters(\n  c(\n    swarzedz_sentinel2_07_09_ndvi,\n    swarzedz_sentinel2_09_27_ndvi\n  ),\n  tempfile(fileext = \".vrt\")\n)\nstack_rast = terra::rast(stack)\nnames(stack_rast) = c(\"NDVI 07.09\", \"NDVI 09.27\")\nstack_rast\n\nclass       : SpatRaster \ndimensions  : 1000, 1000, 2  (nrow, ncol, nlyr)\nresolution  : 10, 10  (x, y)\nextent      : 366347.1, 376347.1, 501106.3, 511106.3  (xmin, xmax, ymin, ymax)\ncoord. ref. : ETRF2000-PL / CS92 (EPSG:2180) \nsource      : file5010876ee0652.vrt \nnames       : NDVI 07.09, NDVI 09.27 \nmin values  : -0.1237687, -0.5265973 \nmax values  :  0.7251614,  0.7040647 \n\n\n\nterra::plot(stack_rast, range = c(-1, 1))\n\n\n\n\n\n\n\n\n\n\nConclusion\nThe goal of this article was to demonstrate the capabilities of rsi package on the most common examples in remote sensing tasks. It’s current state already shows huge potential in many remote sensing applications. rsi provides functions which simplify processes of downloading satellite imagery and calculating spectral indices. If you want to learn more about rsi package, you can visit GitHub repository of the project.\nWhile working on this article, I found many posibble features that could be added into rsi. An useful feature would be adding methods of simpler product filtering, based on cloud coverage and other parameters. Other feature could be focused on time series analysis, by calculating indices for each available item in selected time span.\nI’m looking forward into the future developements of rsi.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{rydzik2024,\n  author = {Rydzik, Mateusz},\n  title = {An Overview of the Rsi {R} Package for Retrieving Satellite\n    Imagery and Calculating Spectral Indices},\n  date = {2024-01-10},\n  url = {https://geocompx.org/post/2024/rsi-bp1/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nRydzik, Mateusz. 2024. “An Overview of the Rsi R Package for\nRetrieving Satellite Imagery and Calculating Spectral Indices.”\nJanuary 10, 2024. https://geocompx.org/post/2024/rsi-bp1/."
  },
  {
    "objectID": "post/2024/spData-bp1/index.html",
    "href": "post/2024/spData-bp1/index.html",
    "title": "Shapefile must… be replaced",
    "section": "",
    "text": "The ESRI Shapefile format is possibly the most popular data format for storing spatial vector data. It is well-established, widely supported, and can be used in most GIS software. At the same time, it has many shortcomings and limitations: it is not a single file, but a collection of files; it has a limit of 2GB per file; it does not support more than one geometry type per file, and more.1 In the last few years, a few improved alternatives to the Shapefile format have been developed, such as the GeoPackage and the FlatGeobuf format.2\nIn 2017, together with Roger Bivand and Robin Lovelace, we created the spData R package – a collection of spatial data both as R objects and external file formats. The package has two main goals: to provide easy access to spatial data for teaching purposes and to provide datasets that could be used as examples in the documentation of other packages. Many of the datasets in the package are stored in the ESRI Shapefile format, which is not ideal.\nThus, after some discussions, we decided to replace the Shapefile format with the GeoPackage format in the spData package (version number: 2.3.1). We hope that this change will serve as a good example and encourage others to use modern spatial data formats. At the time of writing (June 2024), we added GeoPackage versions of all the datasets in the package, but we also plan to remove the ESRI Shapefile versions in the near future. Thus, if you use the spData package, be aware of this change and update your code accordingly. Also, if you have any comments or suggestions, please let us know in the issue on the package’s GitHub page at https://github.com/Nowosad/spData/issues/62."
  },
  {
    "objectID": "post/2024/spData-bp1/index.html#footnotes",
    "href": "post/2024/spData-bp1/index.html#footnotes",
    "title": "Shapefile must… be replaced",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMore about its limitations can be found at the Switch from Shapefile website and in the Geocomputation with R book.↩︎\nAnd more alternatives are being developed, such as GeoParquet.↩︎"
  },
  {
    "objectID": "post/2021/spatial-classes-conversion/index.html",
    "href": "post/2021/spatial-classes-conversion/index.html",
    "title": "Conversions between different spatial classes in R",
    "section": "",
    "text": "The R programming language has, over the past two decades, evolved substantial spatial data analysis capabilities, and is now one of the most powerful environments for undertaking geographic research using a reproducible command line interface. Currently, dedicated R packages allow to read spatial data and apply a plethora of different kinds of spatial methods in a reproducible fashion.\nThere are two main1 spatial data models - spatial vector data and spatial raster data. Natively R does not support spatial data and does not have a definition of spatial classes. Therefore, there had been a need to create R tools able to represent spatial vector and raster data. Spatial classes are slightly different from regular R objects, such as data frames or matrices, as they need to not only store values, but also information about spatial locations and their coordinate reference systems.\nNowadays, the most prominent packages to represent spatial vector data are sf (Pebesma 2021a) and its predecessor sp (Pebesma and Bivand 2021), however, the terra (Hijmans 2021b) package also has its own spatial class for vector data. Spatial raster data can be stored as objects from terra (Hijmans 2021b) and its predecessor raster (Hijmans 2021a), or alternatively the stars package (Pebesma 2021b).\nAs you could see in our Why R? webinar talk, the spatial capabilities of R constantly expand, but also evolve. New packages are being developed, while old ones are modified or superseded. In this process, new methods are created, higher performance code is added, and possible workflows are expanded. Alternative approaches allow for a (hopefully) healthy competition, resulting in better packages. Of course, having more than one package (with its own spatial class/es) for a vector or raster data model could be problematic, especially for new or inexperienced users.\nFirst, it takes time to understand how different spatial classes are organized. To illustrate this, let’s read the same spatial data, srtm.tif from the spDataLarge package (Nowosad and Lovelace 2021), using raster and stars. The raster object:\nraster_file_path = system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nlibrary(raster)\nsrtm_raster = raster(raster_file_path)\nsrtm_raster\n\nclass      : RasterLayer \ndimensions : 457, 465, 212505  (nrow, ncol, ncell)\nresolution : 0.0008333333, 0.0008333333  (x, y)\nextent     : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\ncrs        : +proj=longlat +datum=WGS84 +no_defs \nsource     : srtm.tif \nnames      : srtm \nvalues     : 1024, 2892  (min, max)\nThe stars object:\nlibrary(stars)\nsrtm_stars = read_stars(raster_file_path)\nsrtm_stars\n\nstars object with 2 dimensions and 1 attribute\nattribute(s):\n          Min. 1st Qu. Median     Mean 3rd Qu. Max.\nsrtm.tif  1024    1535   1837 1842.548    2114 2892\ndimension(s):\n  from  to  offset        delta refsys point x/y\nx    1 465 -113.24  0.000833333 WGS 84 FALSE [x]\ny    1 457 37.5129 -0.000833333 WGS 84 FALSE [y]\nSecondly, other packages with methods we want to use, could only accept one specific spatial class, but not the other. For example, the current version of the sabre package (Nowosad and Stepinski 2019) (0.3.2) accepts objects from the raster package, but not ones from terra or stars2. The partitions1 and partitions2 objects are of the RasterLayer class from raster, so the vmeasure_calc() function works correctly.\nlibrary(sabre)\nlibrary(raster)\ndata(\"partitions1\")\ndata(\"partitions2\")\nvmeasure_calc(partitions1, partitions2)\n\nThe SABRE results:\n\n V-measure: 0.36 \n Homogeneity: 0.32 \n Completeness: 0.42 \n\n The spatial objects can be retrieved with:\n $map1 - the first map\n $map2 - the second map\nHowever, when the input object (representing the same spatial data!) is of the SpatRaster class from terra, the calculation results in error.\nSome packages, such as tmap (Tennekes 2021), accept many R spatial classes, however, this takes a lot of effort from package creators to make it possible and to maintain it. Gladly, a number of functions exist that allow to convert between different R spatial classes. Using them, we can work with our favorite spatial data representation, switch to some other representation just for a certain calculation, and then convert the result back into our class. The next two sections showcase how to move between different spatial vector and raster data classes in R."
  },
  {
    "objectID": "post/2021/spatial-classes-conversion/index.html#spatial-vector-data",
    "href": "post/2021/spatial-classes-conversion/index.html#spatial-vector-data",
    "title": "Conversions between different spatial classes in R",
    "section": "Spatial vector data",
    "text": "Spatial vector data\nThe world.gpkg file from the spData (Bivand, Nowosad, and Lovelace 2020) contains spatial vector data with world countries.\n\nworld_path = system.file(\"shapes/world.gpkg\", package = \"spData\")\n\nNow, we can read this file, for example, as an sf object, and convert it into other spatial vector data classes.\n\nlibrary(sf)\nlibrary(sp)\nlibrary(terra)\n\n# read as sf\nworld = read_sf(world_path)\n\n# sf to sp\nworld_sp1 = as(world, \"Spatial\")\n\n# sf to terra vect\nworld_terra1 = vect(world)\n\n# sp to terra vect\nworld_terra2 = vect(world_sp1)\n\n# sp to sf\nworld_sf2 = st_as_sf(world_sp1)\n\n# terra vect to sf\nworld_sf3 = st_as_sf(world_terra1)\n\n# terra vect to sp\nworld_sp2 = as(world_terra1, \"Spatial\")\n\nIn summary, st_as_sf() converts other classes into sf, vect() transform other classes into terra’s SpatVector, and with as(x, \"Spatial\") it is possible to get sp’s vectors.\n\n\n\n\n\nFROM/TO\nsf\nsp\nterra\n\n\n\n\nsf\n\nas(x, \"Spatial\")\nvect()\n\n\nsp\nst_as_sf()\n\nvect()\n\n\nterra\nst_as_sf()\nas(x, \"Spatial\")"
  },
  {
    "objectID": "post/2021/spatial-classes-conversion/index.html#spatial-raster-data",
    "href": "post/2021/spatial-classes-conversion/index.html#spatial-raster-data",
    "title": "Conversions between different spatial classes in R",
    "section": "Spatial raster data",
    "text": "Spatial raster data\nThe srtm.tif file from the spDataLarge (Nowosad and Lovelace 2021) contains a raster elevation model for the Zion National Park in the USA.\n\nsrtm_path = system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\n\nNow, we can read this file, for example, as a raster object, and convert it into other spatial vector data classes.\n\nlibrary(raster)\nlibrary(stars)\nlibrary(terra)\n\nsrtm_raster1 = raster(srtm_path)\n\n# raster to terra\nsrtm_terra1 = rast(srtm_raster1)\n\n# terra to raster\nsrtm_raster2 = raster(srtm_terra1)\n\n# raster to stars\nsrtm_stars1 = st_as_stars(srtm_raster1)\n\n# stars to raster\nsrtm_raster2 = as(srtm_stars1, \"Raster\")\n\n# terra to stars\nsrtm_stars2 = st_as_stars(srtm_terra1)\n\n# stars to terra\nsrtm_terra1a = as(srtm_stars1, \"SpatRaster\")\n\nAs you can see - in most cases, we can just use one function to move from one class to another.\n\n\n\n\n\nFROM/TO\nraster\nterra\nstars\n\n\n\n\nraster\n\nrast()\nst_as_stars()\n\n\nterra\nraster()\n\nst_as_stars()\n\n\nstars\nraster()\nas(x, \"SpatRaster\")"
  },
  {
    "objectID": "post/2021/spatial-classes-conversion/index.html#summary",
    "href": "post/2021/spatial-classes-conversion/index.html#summary",
    "title": "Conversions between different spatial classes in R",
    "section": "Summary",
    "text": "Summary\nThis blog post summarizes how to move between different R spatial vector and raster classes. All of the functions mentioned above have one role: to change classes of input objects. They do not, however, change geometries or underlining values in the data.\nAdditionally, switching from the spatial vector data model to the spatial raster data model (and vice versa) is also possible. These operations are known as rasterization and vectorization, and they could impact spatial and nonspatial information in the input data. To learn more about them, read the Raster-Vector Interactions section in Geocomputation with R."
  },
  {
    "objectID": "post/2021/spatial-classes-conversion/index.html#footnotes",
    "href": "post/2021/spatial-classes-conversion/index.html#footnotes",
    "title": "Conversions between different spatial classes in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are also other spatial data models, including meshes or point clouds.↩︎\nI plan to improve that in a future release.↩︎"
  },
  {
    "objectID": "post/2020/r-spatial-demo-covid-19/index.html",
    "href": "post/2020/r-spatial-demo-covid-19/index.html",
    "title": "Demo of reproducible geographic data analysis: mapping Covid-19 data with R",
    "section": "",
    "text": "Introduction\nThe coronavirus pandemic is a global phenomenon that will affect the lives of the majority of the world’s population for years to come. Impacts range from physical distancing measures already affecting more than half of Earth’s population and knock-on impacts such as changes in air quality to potentially life threatening illness, with peer reviewed estimates of infection fatality rates showing the disease disproportionately affects the elderly and people with underlying health conditions.\nLike other global phenomena such as climate change, the impacts of the pandemic vary greatly by geographic location, with effective and early implementation of physical distancing measures and effective contact tracing associated with lower death rates, according to preliminary research, as illustrated in the animation below (source: Washington Post).\n\nThis article demonstrates how to download and map open data on the evolving coronavirus pandemic, using reproducible R code. The aim is not to provide scientific analysis of the data, but to demonstrate how ‘open science’ enables public access to important international datasets. It also provides an opportunity to demonstrate how techniques taught in Geocomputation with R can be applied to real-world datasets.\nBefore undertaking geographic analysis of ‘rate’ data, such as the number Covid-19 infections per unit area, it is worth acknowledging caveats at the outset. Simple graphics of complex phenomena can be misleading. This is well-illustrated in the figure below by Will Geary, which shows how the ecological fallacy can affect interpretations of geographical analysis of areal units such countries that we will be using in this research.\n\nThe post is intended more as a taster of geographic visualisation in R than as a gateway to scientific analysis of Covid-19 data. See resources such as the eRum2020 CovidR contest and lists of online resources for pointers on how to usefully contribute to data-driven efforts to tackle the crisis.\n\n\nSet-up\nTo reproduce the results presented in this article you will need to have an R installation with up-to-date versions of the following packages installed and loaded. (See the geocompr/docker repo and Installing R on Ubuntu article for more on setting up your computer to work with R).\n\nlibrary(sf)\n\nLinking to GEOS 3.11.0, GDAL 3.5.2, PROJ 9.0.1; sf_use_s2() is TRUE\n\n\n\nlibrary(tmap)\nlibrary(dplyr)\n\n\n\nGetting international Covid-19 data\nTo get data on official Covid-19 statistics, we will use the COVID19 R package.\n\nThis package provides daily updated data on a variety of variables related to the coronavirus pandemic at national, regional and city levels. Install it as follows:\n\ninstall.packages(\"COVID19\")\n\nAfter the package is installed, you can get up-to-date country-level data as follows:\n\nd = COVID19::covid19()\n\nTo minimise dependencies for reproducing the results in this article, we uploaded a copy of the data, which can be downloaded as follows (re-run the code above to get up-to-date data):\n\nd = readr::read_csv(\"https://git.io/covid-19-2020-04-23\")\nclass(d)\n\n[1] \"spec_tbl_df\" \"tbl_df\"      \"tbl\"         \"data.frame\" \n\n\nThe previous code chunk read a .csv file from online and confirmed, we have loaded a data frame (we will see how to join this with geographic data in the next section). We can get a sense of the contents of the data as follows:\n\nncol(d)\n\n[1] 24\n\nnrow(d)\n\n[1] 17572\n\nnames(d)\n\n [1] \"id\"             \"date\"           \"deaths\"         \"confirmed\"     \n [5] \"tests\"          \"recovered\"      \"hosp\"           \"icu\"           \n [9] \"vent\"           \"driving\"        \"walking\"        \"transit\"       \n[13] \"country\"        \"state\"          \"city\"           \"lat\"           \n[17] \"lng\"            \"pop\"            \"pop_14\"         \"pop_15_64\"     \n[21] \"pop_65\"         \"pop_age\"        \"pop_density\"    \"pop_death_rate\"\n\n\n\n\nGetting geographic data\nWe will use a dataset representing countries worldwide from the rnaturalearth package. Assuming you have the package installed you can get the geographic data as follows (see the subsequent code chunk if not):\n\nworld_rnatural = rnaturalearth::ne_download(returnclass = \"sf\")\n# names(world_iso) # variables available\nworld_iso = world_rnatural %&gt;% \n  select(NAME_LONG, ISO_A3_EH, POP_EST, GDP_MD_EST, CONTINENT)\n\nThe result of the previous code block, an object representing the world and containing only variables of interest, was uploaded to GitHub and can be loaded from a GeoJSON file as follows:\n\nworld_iso = sf::read_sf(\"https://git.io/JfICT\") \n\nTo see what’s in the world_iso object we can plot it, with the default setting in sf showing the geographic distribution of each variable:\n\nplot(world_iso)\n\n\n\n\n\n\n\n\n\n\nTransforming geographic data\nAn issue with the result from a data visualisation perspective is that this unprojected visualisation distorts the world: countries such as Greenland at high latitudes appear bigger than the actually are. To overcome this issue we will project the object as follows (see Chapter 6 of Geocomputation with R and a recent article on the r-spatial website for more on coordinate systems):\n\nworld_projected = world_iso %&gt;% \n  st_transform(\"+proj=moll\")\n\nWe can plot just the geometry of the updated object as follows, noting that the result is projected in a way that preserves the true area of countries (noting also that all projections introduce distortions):\n\nplot(st_geometry(world_projected))\n\n\n\n\n\n\n\n\n\n\nAttribute joins\nAs outlined in Chapter 3 of Geocomputation with R, attribute joins can be used to add additional variables to geographic data via a ‘key variable’ shared between the geographic and non-geographic objects. In this case the shared variables are ISO_A3_EH in the geographic object and id in the Covid-19 dataset d. We will be concise and call the dataset resulting from this join operation w.\n\nw = dplyr::left_join(world_projected, d, by = c(\"ISO_A3_EH\"= \"id\"))\nclass(w)\n\n[1] \"sf\"         \"tbl_df\"     \"tbl\"        \"data.frame\"\n\nnrow(w)\n\n[1] 14919\n\n\n\n\nCalculating area\nThe package sf provides a wide range of functions for calculating geographic variables such as object centroid, bounding boxes, lengths and, as demonstrated below, area. We use this area data to calculate the population density of each country as follows:\n\nw$Area_km = as.numeric(st_area(w)) / 1e6\nw$`Pop/km2` = as.numeric(w$POP_EST) / w$Area_km \n\n\n\nPlotting international Covid-19 data for a single day\nThe class of w shows that it has geometries for each row. Notice that it has many more rows of data than the original world object: geometries are repeated for every year. This is not an efficient way to store data, as it means lots of duplicate geometries. On a small dataset that doesn’t matter, but it’s something to be aware of. To check that the join has worked, we will take a subset of rows representing the global situation yesterday relative to the date of data access:\n\nw_yesterday = w %&gt;% \n  filter(date == max(date, na.rm = T) - 1)\nplot(w_yesterday)\n\n\n\n\n\n\n\n\nThe plot method for sf objects is fast and flexible, as documented in sf’s Plotting Simple Features vignette, which can be accessed with vignette(\"sf5\") from the R console. We can set the breaks to better show the difference between countries with no reported deaths and countries with few reported deaths as follows:\n\nplot(w_yesterday[\"deaths\"])\n\n\n\n\n\n\n\nb = c(0, 10, 100, 1000, 10000, 100000)\nplot(w_yesterday[\"deaths\"], breaks = b)\n\n\n\n\n\n\n\n\nTo plot the other Covid-19 variables, reporting number of confirmed cases, number of tests and number of people who have recovered, we can subset the relevant variables and pipe the result to the plot() function (noting the caveat that code containing pipes may be hard to debug) as follows:\n\nw_yesterday %&gt;%\n  dplyr::select(deaths, confirmed, tests, recovered) %&gt;% \n  plot()\n\n\n\n\n\n\n\n\n\n\nMaking maps with tmap\nThe mapping chapter of Geocomputation with R shows how the tmap package enables publication-quality maps to be created with concise and relatively commands, such as:\n\ntm_shape(w_yesterday) +\n  tm_polygons(c(\"deaths\", \"recovered\"))\n\n\n\n\n\n\n\n\nWe can modify the palette and scale as follows:\n\ntm_shape(w_yesterday) +\n  tm_polygons(\n    c(\"deaths\", \"recovered\"),\n    palette = \"viridis\",\n    style = \"order\"\n    ) \n\n\n\n\n\n\n\n\nThe map can be further improved by adding graticules representing the curvature of the Earth, created as follows:\n\ng = st_graticule(w_yesterday)\n\nIt’s also worth moving the legend:\n\ntm_shape(g) +\n  tm_lines(col = \"grey\") +\n  tm_shape(w_yesterday) +\n  tm_polygons(\n    c(\"deaths\", \"recovered\"),\n    palette = \"viridis\",\n    style = \"order\"\n    ) +\n  tm_layout(legend.position = c(0.01, 0.25))\n\n\n\n\n\n\n\n\nA problem with choropleth maps is that they can under-represent small areas. To overcome this issue we can use dot size instead of color to represent number:\n\ntm_shape(g) +\n  tm_lines(col = \"grey\") +\n  tm_shape(w_yesterday) +\n  tm_polygons() +\n  tm_layout(legend.position = c(0.01, 0)) +\n  tm_shape(w_yesterday) +\n  tm_dots(\n    col = c(\"red\", \"green\"),\n    size = c(\"deaths\", \"recovered\"),\n    palette = \"viridis\"\n    )\n\n\n\n\n\n\n\n\nOne question I have here: make the size legend have style = \"log10_pretty\" also?\n\n\nMaking animated maps\nThe animation at the beginning of this article shows how dynamic graphics can communicate change effectively. Animated maps are therefore useful for showing evolving geographic phenomena, such as the spread of Covid-19 worldwide. As covered in section 8.3 of Geocomputation with R, animated maps can be created with tmap by extending the tm_facet() functionality. So let’s start by creating a facetted map showing the total number of deaths on the first day of each month in our data:\n\nw$Date = as.character(w$date)\ntm_shape(g) +\n  tm_lines(col = \"grey\") +\n  tm_shape(w_yesterday) +\n  tm_polygons(\n    \"Pop/km2\",\n    palette = \"viridis\",\n    style = \"log10_pretty\",\n    n = 3\n    ) +\n  tm_shape(w %&gt;% filter(grepl(pattern = \"01$\", date))) +\n  tm_dots(size = \"deaths\", col = \"red\") +\n  tm_facets(\"Date\", nrow = 1, free.scales.fill = FALSE) +\n  tm_layout(\n    legend.outside.position = \"bottom\",\n    legend.stack = \"horizontal\"\n    )\n\n\n\n\n\n\n\n\nTo create an animated map, following instructions in Chapter 8 of Geocomputation with R, we need to make some small changes to the code above:\n\nm = tm_shape(g) +\n  tm_lines(col = \"grey\") +\n  tm_shape(w_yesterday) +\n  tm_polygons(\n    \"Pop/km2\",\n    palette = \"viridis\",\n    style = \"log10_pretty\",\n    n = 3\n    ) +\n  tm_shape(w %&gt;% filter(grepl(pattern = \"01$\", date))) +\n  tm_dots(size = \"deaths\", col = \"red\") +\n  tm_facets(along = \"Date\", free.coords = FALSE) +\n  tm_layout(legend.outside = TRUE)\ntmap_animation(m, \"covid-19-animated-map-test.gif\", width = 800)\nbrowseURL(\"covid-19-animated-map-test.gif\")\n\n\n\n\n\n\n\n\n\n\nWe made an animated map! The first version is rarely the best though, and the map above clearly could benefit from some adjustments before we plot the results for the whole dataset:\n\nw$Date = paste0(\"Total deaths from 22nd January 2020 to \", w$date)\n\nm = tm_shape(g) +\n  tm_lines(col = \"grey\") +\n  tm_shape(w) +\n  tm_polygons(\n    \"Pop/km2\",\n    palette = \"viridis\",\n    style = \"log10_pretty\",\n    lwd = 0.5\n    ) +\n  tm_shape(w) +\n  tm_dots(size = \"deaths\", col = \"red\") +\n  tm_facets(along = \"Date\", free.coords = FALSE) +\n  tm_layout(\n    main.title.size = 0.5,\n    legend.outside = TRUE\n    )\ntmap_animation(m, \"covid-19-animated-map.gif\", width = 1400, height = 600)\nbrowseURL(\"covid-19-animated-map.gif\")\n\n\n\n\n\nConclusion\nThis article has demonstrated how to work with and map geographic data using the free and open source statistical programming language R. It demonstrates that by representing analysis in code, research can be made reproducible and more accessible to others, encouraging transparent and open science. This has multiple advantages, from education and citizen engagement with the evidence to increased trust in the evidence on which important, life-or-death, decisions are made.\nAlthough the research did not address any policy issues, it could be extended to do so, and we encourage readers to check-out the following resources for ideas for future research:\n\nA reproducible geographic analysis of Covid-19 data in Spain by Antonio Paez and others (challenge: reproduce their findings)\nThe eRum2020 CovidR competition (challenge: enter the contest!)\nTry downloading the the city-level data with this command and exploring the geographic distribution of the outbreak at the city level:\n\n\nd_city = COVID19::covid19(level = 3)\n\nFor further details on geographic data analysis in R in general, we recommend checkout out in-depth materials such as Geocomputation with R and the in-progress open source book Spatial Data Science.\nThere is also an online talk on the subject on YouTube.\n\n\nSession info\n\n\nsessioninfo::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.2.2 (2022-10-31)\n os       Fedora Linux 37 (Thirty Seven)\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  en_US.UTF-8\n ctype    en_US.UTF-8\n tz       Europe/Warsaw\n date     2022-12-20\n pandoc   2.19.2 @ /usr/lib/rstudio/bin/quarto/bin/tools/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n ! package      * version   date (UTC) lib source\n P abind          1.4-5     2016-07-21 [?] CRAN (R 4.2.2)\n P base64enc      0.1-3     2015-07-28 [?] CRAN (R 4.2.2)\n P bit            4.0.5     2022-11-15 [?] CRAN (R 4.2.2)\n P bit64          4.0.5     2020-08-30 [?] CRAN (R 4.2.2)\n P class          7.3-20    2022-01-16 [?] CRAN (R 4.2.2)\n P classInt       0.4-8     2022-09-29 [?] CRAN (R 4.2.2)\n P cli            3.4.1     2022-09-23 [?] CRAN (R 4.2.2)\n P codetools      0.2-18    2020-11-04 [?] CRAN (R 4.2.2)\n P crayon         1.5.2     2022-09-29 [?] CRAN (R 4.2.2)\n P crosstalk      1.2.0     2021-11-04 [?] CRAN (R 4.2.2)\n P curl           4.3.3     2022-10-06 [?] CRAN (R 4.2.2)\n P DBI            1.1.3     2022-06-18 [?] CRAN (R 4.2.2)\n P dichromat      2.0-0.1   2022-05-02 [?] CRAN (R 4.2.2)\n P digest         0.6.31    2022-12-11 [?] CRAN (R 4.2.2)\n P dplyr        * 1.0.10    2022-09-01 [?] CRAN (R 4.2.2)\n P e1071          1.7-12    2022-10-24 [?] CRAN (R 4.2.2)\n P ellipsis       0.3.2     2021-04-29 [?] CRAN (R 4.2.2)\n P evaluate       0.19      2022-12-13 [?] CRAN (R 4.2.2)\n P fansi          1.0.3     2022-03-24 [?] CRAN (R 4.2.2)\n P fastmap        1.1.0     2021-01-25 [?] CRAN (R 4.2.2)\n P generics       0.1.3     2022-07-05 [?] CRAN (R 4.2.2)\n P glue           1.6.2     2022-02-24 [?] CRAN (R 4.2.2)\n P hms            1.1.2     2022-08-19 [?] CRAN (R 4.2.2)\n P htmltools      0.5.4     2022-12-07 [?] CRAN (R 4.2.2)\n P htmlwidgets    1.6.0     2022-12-15 [?] CRAN (R 4.2.2)\n P jsonlite       1.8.4     2022-12-06 [?] CRAN (R 4.2.2)\n P KernSmooth     2.23-20   2021-05-03 [?] CRAN (R 4.2.2)\n P knitr          1.41      2022-11-18 [?] CRAN (R 4.2.2)\n P lattice        0.20-45   2021-09-22 [?] CRAN (R 4.2.2)\n P leafem         0.2.0     2022-04-16 [?] CRAN (R 4.2.2)\n P leaflet        2.1.1     2022-03-23 [?] CRAN (R 4.2.2)\n P leafsync       0.1.0     2019-03-05 [?] CRAN (R 4.2.2)\n P lifecycle      1.0.3     2022-10-07 [?] CRAN (R 4.2.2)\n P lwgeom         0.2-10    2022-11-19 [?] CRAN (R 4.2.2)\n P magrittr       2.0.3     2022-03-30 [?] CRAN (R 4.2.2)\n P pillar         1.8.1     2022-08-19 [?] CRAN (R 4.2.2)\n P pkgconfig      2.0.3     2019-09-22 [?] CRAN (R 4.2.2)\n P png            0.1-8     2022-11-29 [?] CRAN (R 4.2.2)\n P proxy          0.4-27    2022-06-09 [?] CRAN (R 4.2.2)\n P R6             2.5.1     2021-08-19 [?] CRAN (R 4.2.2)\n P raster         3.6-11    2022-11-28 [?] CRAN (R 4.2.2)\n P RColorBrewer   1.1-3     2022-04-03 [?] CRAN (R 4.2.2)\n P Rcpp           1.0.9     2022-07-08 [?] CRAN (R 4.2.2)\n P readr          2.1.3     2022-10-01 [?] CRAN (R 4.2.2)\n   renv           0.16.0    2022-09-29 [2] CRAN (R 4.2.2)\n P rlang          1.0.6     2022-09-24 [?] CRAN (R 4.2.2)\n P rmarkdown      2.19      2022-12-15 [?] CRAN (R 4.2.2)\n P rstudioapi     0.14      2022-08-22 [?] CRAN (R 4.2.2)\n P sessioninfo    1.2.2     2021-12-06 [?] CRAN (R 4.2.2)\n P sf           * 1.0-9     2022-11-08 [?] CRAN (R 4.2.2)\n P sp             1.5-1     2022-11-07 [?] CRAN (R 4.2.2)\n P stars          0.6-0     2022-11-21 [?] CRAN (R 4.2.2)\n P stringi        1.7.8     2022-07-11 [?] CRAN (R 4.2.2)\n P stringr        1.5.0     2022-12-02 [?] CRAN (R 4.2.2)\n P terra          1.6-47    2022-12-02 [?] CRAN (R 4.2.2)\n P tibble         3.1.8     2022-07-22 [?] CRAN (R 4.2.2)\n P tidyselect     1.2.0     2022-10-10 [?] CRAN (R 4.2.2)\n P tmap         * 3.3-3     2022-03-02 [?] CRAN (R 4.2.2)\n P tmaptools      3.1-1     2021-01-19 [?] CRAN (R 4.2.2)\n P tzdb           0.3.0     2022-03-28 [?] CRAN (R 4.2.2)\n P units          0.8-1     2022-12-10 [?] CRAN (R 4.2.2)\n P utf8           1.2.2     2021-07-24 [?] CRAN (R 4.2.2)\n P vctrs          0.5.1     2022-11-16 [?] CRAN (R 4.2.2)\n P viridisLite    0.4.1     2022-08-22 [?] CRAN (R 4.2.2)\n P vroom          1.6.0     2022-09-30 [?] CRAN (R 4.2.2)\n P withr          2.5.0     2022-03-03 [?] CRAN (R 4.2.2)\n P xfun           0.35      2022-11-16 [?] CRAN (R 4.2.2)\n P XML            3.99-0.13 2022-12-04 [?] CRAN (R 4.2.2)\n P yaml           2.3.6     2022-10-18 [?] CRAN (R 4.2.2)\n\n [1] /tmp/Rtmp2F9iQn/renv-library-6dbab52f30514\n [2] /home/jn/Science/geocompr/geocompr.github.io/renv/library/R-4.2/x86_64-redhat-linux-gnu\n [3] /home/jn/Science/geocompr/geocompr.github.io/renv/sandbox/R-4.2/x86_64-redhat-linux-gnu/60c4e220\n\n P ── Loaded and on-disk path mismatch.\n\n──────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{lovelace2020,\n  author = {Lovelace, Robin},\n  title = {Demo of Reproducible Geographic Data Analysis: Mapping\n    {Covid-19} Data with {R}},\n  date = {2020-04-23},\n  url = {https://geocompx.org/post/2020/r-spatial-demo-covid-19/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLovelace, Robin. 2020. “Demo of Reproducible Geographic Data\nAnalysis: Mapping Covid-19 Data with R.” April 23, 2020. https://geocompx.org/post/2020/r-spatial-demo-covid-19/."
  },
  {
    "objectID": "post/2019/tmap-grid/index.html",
    "href": "post/2019/tmap-grid/index.html",
    "title": "Grids and graticules in the tmap package",
    "section": "",
    "text": "This vignette builds on the making maps chapter of the Geocomputation with R book. Its goal is to demonstrate how to set and modify grids and graticules in the tmap package."
  },
  {
    "objectID": "post/2019/tmap-grid/index.html#prerequisites",
    "href": "post/2019/tmap-grid/index.html#prerequisites",
    "title": "Grids and graticules in the tmap package",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe examples below assume the following packages are attached:\n\nlibrary(spData) # example datasets\nlibrary(tmap)   # map creation (&gt;=2.3)\nlibrary(sf)     # spatial data classes"
  },
  {
    "objectID": "post/2019/crs-projections-transformations/index.html",
    "href": "post/2019/crs-projections-transformations/index.html",
    "title": "Geographic projections and transformations",
    "section": "",
    "text": "Introduction\nThis workbook outlines key concepts and functions related to map projections — also referred to as coordinate reference systems (CRSs) — and transformation of geographic data from one projection to another. It is based on the open source book Geocomputation with R, and Chapter 6 in particular.\nIt was developed for the ‘CASA Summer School’, or the Doctoral Summer School for Advanced Spatial Modelling: Skills Workshop and Hackathon, 21st to 23rd August 2019, for its full name! It should be of use to anyone interested in projections, beyond the summer school, so we posted it on our newly updated website for maximum benefit.\n\n\nPrerequisites\nBefore you get started, make sure you have the packages installed:\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(spData)\n\n\n\nIntroduction\nBefore we get started, why use R for geographic data?\nSimple answer: because it works, excels at spatial statistics and visualisation and has a huge user community.\nIt can be used for a wide range of things such as:\n\nBook on Geocomputation: https://geocompr.robinlovelace.net/\nPropensity to Cycle Tool: https://pct.bike/\n\nGeographic data relies on a frame of reference. There are two main types of CRS:\n\nGeographic, where the frame of reference is the globe and how many degrees north or east from the position (0, 0) you are\nProjected, where the frame of reference is a flat representation of part of the Earth’s surface\n\n\n\n\n\n\nIllustration of vector (point) data in which location of London (the red X) is represented with reference to an origin (the blue circle). The left plot represents a geographic CRS with an origin at 0° longitude and latitude. The right plot represents a projected CRS with an origin located in the sea west of the South West Peninsula.\n\n\n\n\n\n\n\nIllustration of vector (point) data in which location of London (the red X) is represented with reference to an origin (the blue circle). The left plot represents a geographic CRS with an origin at 0° longitude and latitude. The right plot represents a projected CRS with an origin located in the sea west of the South West Peninsula.\n\n\n\n\n\n\nTLDR\nThe ‘too long didn’t read’ (TLDR) take away messages from this text are:\n\nBe aware of projections\nDecide an appropriate CRS for your project and ensure everything is in that CRS\nUse a projected CRS when doing geometric operations\nEPSG codes such as 27700 and 4326 refer to specific coordinate systems\nIt is worth checking if there is an officially supported projection for the region — that is often a good option\n\nIn R, you can check, set and transform CRS with st_crs() and st_transform() as follows:\n\nzones_london = lnd\nst_crs(zones_london)                                         # find out the CRS\n#&gt; Coordinate Reference System:\n#&gt;   User input: EPSG:4326 \n#&gt;   wkt:\n#&gt; GEOGCS[\"WGS 84\",\n#&gt;     DATUM[\"WGS_1984\",\n#&gt;         SPHEROID[\"WGS 84\",6378137,298.257223563,\n#&gt;             AUTHORITY[\"EPSG\",\"7030\"]],\n#&gt;         AUTHORITY[\"EPSG\",\"6326\"]],\n#&gt;     PRIMEM[\"Greenwich\",0,\n#&gt;         AUTHORITY[\"EPSG\",\"8901\"]],\n#&gt;     UNIT[\"degree\",0.0174532925199433,\n#&gt;         AUTHORITY[\"EPSG\",\"9122\"]],\n#&gt;     AUTHORITY[\"EPSG\",\"4326\"]]\nzones_london_projected = st_transform(zones_london, 27700)   # transform CRS\nst_crs(zones_london) = NA                                    # set CRS\nst_crs(zones_london) = 4326                                  # set CRS\n\nIf you ignore CRSs, bad things can happen.\n\n\nWhy are projections needed?\nWithout a coordinate system, we have no context:\n\n\n\n\n\n\n\n\n\nWhich country is it?\nLocation on Earth is measured in degrees so, even when axes are equal, maps are highly distorted representations of reality far from the equator:\n\n\n\n\n\n\n\n\n\nAlong which axis is the image over-represented?\nEven when we compensate for this, the cylindrical projection is misleading:\n\n\n\n\n\n\n\n\n\nBy default, most software (including R) plots data with geographic lon/lat coordinates a cylindrical projection, leading to maps like this:\n\nplot(canada$geom)\n\n\n\n\n\n\n\nplot(world)\n#&gt; Warning: plotting the first 9 out of 10 attributes; use max.plot = 10 to plot\n#&gt; all\n\n\n\n\n\n\n\n\nAcross the whole world, the results look like this:\n\nplot(st_geometry(world), col = \"grey\")\n\n\n\n\n\n\n\n\nThere is no single ‘correct’ CRS that can represent everywhere well: it is physically impossible to ‘peal’ the surface of the Earth onto a flat screen (credit: Awar Jahfar):\n\n\n\n\n\n\n\n\n\n\nAt best we can comply with two out of three spatial properties (distance, area, direction). Therefore, the task at hand determines which projection to choose. For instance, if we are interested in a density (points per grid cell or inhabitants per grid cell), we should use an equal-area projection.\n\n\nThere is also a fourth property, shape.\n\n\n\nWhich projection to use?\nA range of CRSs is available:\n\nA Lambert azimuthal equal-area (LAEA) projection for a custom local projection (set lon_0 and lat_0 to the center of the study area), which is an equal-area projection at all locations but distorts shapes beyond thousands of kilometers.\nAzimuthal equidistant (AEQD) projections for a specifically accurate straight-line distance between a point and the center point of the local projection.\nLambert conformal conic (LCC) projections for regions covering thousands of kilometers, with the cone set to keep distance and area properties reasonable between the secant lines.\nStereographic (STERE) projections for polar regions, but taking care not to rely on area and distance calculations thousands of kilometers from the center.\n\nThis is how it works in R:\n\nworld_laea1_g = world %&gt;%\n  st_transform(\"+proj=laea +x_0=0 +y_0=0 +lon_0=0 +lat_0=0\") %&gt;% \n  st_geometry()\nplot(world_laea1_g)\n\n\n\n\n\n\n\n\n\nworld %&gt;%\n  st_transform(\"+proj=aeqd +x_0=0 +y_0=0 +lon_0=0 +lat_0=0\") %&gt;% \n  st_geometry() %&gt;% \n  plot()\n\n\n\n\n\n\n\n\n\nworld %&gt;%\n  st_transform(\"+proj=moll\") %&gt;% \n  st_geometry() %&gt;% \n  plot()\n\n\n\n\n\n\n\n\nHow to add graticules?\n\nworld %&gt;%\n  st_transform(\"+proj=moll\") %&gt;% \n  st_geometry() %&gt;% \n  plot()\ng = st_graticule(x = world) %&gt;% \n  st_transform(\"+proj=moll\") %&gt;% \n  st_geometry()\nplot(g, add = TRUE)\n\n\n\n\n\n\n\n\n\ncanada_centroid = st_coordinates(st_centroid(canada))\n#&gt; Warning in st_centroid.sf(canada): st_centroid assumes attributes are constant\n#&gt; over geometries of x\ncanada_laea_crs = paste0(\"+proj=laea +x_0=0 +y_0=0 +lon_0=\",\n                         canada_centroid[1],\n                         \" +lat_0=\",\n                         canada_centroid[2])\ncanada_laea = st_transform(canada, crs = canada_laea_crs)\nworld_laea = st_transform(world, crs = canada_laea_crs)\nplot(st_geometry(canada_laea))\nplot(world_laea, add = TRUE)\n#&gt; Warning in plot.sf(world_laea, add = TRUE): ignoring all but the first attribute\n\n\n\n\n\n\n\n\n\ncanada_centroid = st_coordinates(st_centroid(canada))\n#&gt; Warning in st_centroid.sf(canada): st_centroid assumes attributes are constant\n#&gt; over geometries of x\ncanada_laea_crs = paste0(\"+proj=laea +x_0=0 +y_0=0 +lon_0=\",\n                         canada_centroid[1],\n                         \" +lat_0=\",\n                         canada_centroid[2])\ncanada_laea = st_transform(canada, crs = canada_laea_crs)\nworld_laea = st_transform(world, crs = canada_laea_crs)\nplot(st_geometry(canada_laea))\nplot(world_laea, add = TRUE)\n#&gt; Warning in plot.sf(world_laea, add = TRUE): ignoring all but the first attribute\n\n\n\n\n\n\n\n\n\n\nEPSG codes\nEPSG codes are standard codes for projections. See them in R with:\n\nepsg_codes = rgdal::make_EPSG()\n# View(epsg_codes) # open in interactive spreadsheet\n\nIn the UK, the EPSG code of official data is 27700.\n\n\nGeographic data in R\n\nlondon_df = data.frame(name = \"london\", population = 1e7,\n                       lon = -0.1, lat = 51.5)\nclass(london_df)\n#&gt; [1] \"data.frame\"\nlondon = st_as_sf(london_df, coords = c(\"lon\", \"lat\"))\nclass(london)\n#&gt; [1] \"sf\"         \"data.frame\"\nst_is_longlat(london)\n#&gt; [1] NA\nplot(zones_london_projected$geometry)\nplot(london$geometry, add = TRUE, pch = 9) # not there\n\n\n\n\n\n\n\n\n\n\nIssues with geometric operations\n\nlondon_buff1 = st_buffer(london, 0.1)\nplot(london_buff1)\n\n\n\n\n\n\n\nplot(zones_london$geometry)\nplot(london_buff1, add = T)\n#&gt; Warning in plot.sf(london_buff1, add = T): ignoring all but the first attribute\n\n\n\n\n\n\n\n\n\nst_crs(london) = 4326\nlondon_projected = st_transform(london, 27700)\nlondon_buff2 = st_buffer(london_projected, 10000)\nst_is_longlat(london_projected)\n#&gt; [1] FALSE\nplot(zones_london_projected$geometry)\nplot(london_buff2, add = TRUE)\n#&gt; Warning in plot.sf(london_buff2, add = TRUE): ignoring all but the first\n#&gt; attribute\n\n\n\n\n\n\n\n\n\n\nFurther reading\nIf you’re interested in learning more on this, check out Geocomputations with R.\n\nMore specific resources on projections include:\n\nExcellent tutorial on coordinate systems on the Manifold website: http://www.manifold.net/doc/mfd9/projections_tutorial.htm\nAn introduction to vector geographic data in Geocomputation with R (Section 2.2)\nAn introduction to CRSs in R (Section 2.4)\nThe contents and exercises of Chapter 6, solutions to which you can find at https://geocompr.github.io/geocompkg/articles/index.html\nFor a fun take on projections, see https://xkcd.com/977/\nChapter in upcoming book on CRSs by Edzer Pebesma and Roger Bivand: https://github.com/edzer/sdsr\n\nCheck out the questions in the exercises section of Chapter 6 of Geocomputation with R.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{lovelace2019,\n  author = {Lovelace, Robin},\n  title = {Geographic Projections and Transformations},\n  date = {2019-08-21},\n  url = {https://geocompx.org/post/2019/crs-projections-transformations/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLovelace, Robin. 2019. “Geographic Projections and\nTransformations.” August 21, 2019. https://geocompx.org/post/2019/crs-projections-transformations/."
  },
  {
    "objectID": "post/2022/geocompr-solutions/index.html",
    "href": "post/2022/geocompr-solutions/index.html",
    "title": "Automatic website for the Geocomputation with R solutions",
    "section": "",
    "text": "Geocomputation with R is a book on geographic data analysis, visualization, and modeling. Each chapter1 ends with a set of exercises allowing the readers to test how well they understand each presented concepts and approaches. However, exercises are often not enough without their solutions, which provides a way to compare readers’ answers with an expected one. This leads to technical questions: how to provide solutions to the exercises without making the book too long, and to minimise duplication of words/effort (for the questions and answers)? Also, how to test, at the same time, if the solutions are still working?\nFor the first edition of the book we added solutions as external set of vignettes. This approach had, however, several flaws. Most importantly, solutions in the vignettes were detached from the exercises in the book, meaning that if we decided to change an exercise, we would need to go to a different repository and update it there as well. Additionally, the solution vignettes had a different style from the book and were harder to find. We decided to try a different approach for the second edition of the book. This time, we created an external bookdown-based website for solutions, which you can find at https://geocompr.github.io/solutions/. The main goal of this blog post is to explain the new setup in some detail."
  },
  {
    "objectID": "post/2022/geocompr-solutions/index.html#new-solutions-website",
    "href": "post/2022/geocompr-solutions/index.html#new-solutions-website",
    "title": "Automatic website for the Geocomputation with R solutions",
    "section": "New solutions website",
    "text": "New solutions website\nFirstly, let’s take a look at the bigger picture. Exercises are shown at the end of the book chapters, for example, exercises for chapter 6 can be seen at https://geocompr.robinlovelace.net/raster-vector.html#exercises-4. You may notice that this exercises section starts with a description of prerequisites, and then several questions are listed. However, what to do when I want to find solutions to these problems or compare my results with those provided by the book authors? Then, we can visit the corresponding chapter in the solution website at https://geocompr.github.io/solutions/raster-vector.html. Note that the solution website contains the same content as the exercises section, and extends it with the solution code, code results, and newly created figures."
  },
  {
    "objectID": "post/2022/geocompr-solutions/index.html#solutions",
    "href": "post/2022/geocompr-solutions/index.html#solutions",
    "title": "Automatic website for the Geocomputation with R solutions",
    "section": "Solutions",
    "text": "Solutions\nNext, let’s dive deeper to see how it works. We can look at the source code of the solution website first – https://github.com/geocompr/solutions, and solutions for chapter 6 are at https://github.com/geocompr/solutions/blob/main/06-raster-vector.Rmd. If you open the last link, you should see a code looking like that:\n```{r setup, echo=FALSE, results='hide'}`r ''`\nex = tempfile(fileext = \".Rmd\")\non.exit(unlink(ex))\ndownload.file(\"https://raw.githubusercontent.com/Robinlovelace/geocompr/main/_06-ex.Rmd\", ex)\n```\n\n```{r, echo=FALSE, results='asis'}`r ''`\nres = knitr::knit_child(ex, quiet = TRUE, options = list(include = TRUE))\ncat(res, sep = '\\n')\n```\nWhat is going on here? In short, we have two code chunks. The first (hidden) code chunk downloads a _06-ex.Rmd file from the Robinlovelace/geocompr repository. Ok, but how does this _06-ex.Rmd document looks like? You can see its content at https://raw.githubusercontent.com/Robinlovelace/geocompr/main/_06-ex.Rmd. It is a mix of text (mainly exercises questions) and R code chunks. Note here that most code chunks do not have any code chunks options set, except the first one. We wanted the first code chunk to be visible on the main book and the solution websites, so we used include = TRUE.\nThe second code chunk takes the downloaded document and knits it with a new global option, include = TRUE. This option make sure that we will show all possible outputs from _06-ex.Rmd, including numerical results and plots."
  },
  {
    "objectID": "post/2022/geocompr-solutions/index.html#exercises",
    "href": "post/2022/geocompr-solutions/index.html#exercises",
    "title": "Automatic website for the Geocomputation with R solutions",
    "section": "Exercises",
    "text": "Exercises\nThe above section explained how we could see exercises and solutions on the solution website. However, how is it possible that we can only see the exercise text in the book? Let’s look at the bottom of the source code of the 6th chapter at https://github.com/Robinlovelace/geocompr/blob/main/06-raster-vector.Rmd:\n## Exercises\n\n```{r, echo=FALSE, results='asis'}`r ''`\nres = knitr::knit_child('_06-ex.Rmd', quiet = TRUE, options = list(include = FALSE, eval = FALSE))\ncat(res, sep = '\\n')\n```\n\nThere is only one short code chunk – it knits the same document, _06-ex.Rmd, however using different global options, include = FALSE, eval = FALSE. This assures that the book’s readers would not see any solutions’ code chunks or their outputs."
  },
  {
    "objectID": "post/2022/geocompr-solutions/index.html#workflow",
    "href": "post/2022/geocompr-solutions/index.html#workflow",
    "title": "Automatic website for the Geocomputation with R solutions",
    "section": "Workflow",
    "text": "Workflow\nNow we know all of the pieces of the puzzle. The last step is to connect them together with GitHub Actions.\nOur workflow is as follows:\n\nWe add/update/modify exercises and their solutions stored in a file per chapter, e.g., _06-ex.Rmd\nWe push the changes to the main book repository on GitHub (Robinlovelace/geocompr)\nThe book is automatically built using a GitHub Actions workflow, including exercises (but not solutions!)\nOnce a week, another GitHub Actions workflow runs in the solutions repository.\n\nThis approach ensures that the solutions are always up-to-date, reproducible, and in sync. with the manuscript. It creates a resources dedicated to the exercises and their solutions, helping people learn. It should also encourage others to get involved: can you think of another exercise or an alternative solution to existing exercises? You have all the info you need to contribute now. Let us know your comments or suggestions at https://github.com/geocompr/solutions! Pull Requests are also welcome!"
  },
  {
    "objectID": "post/2022/geocompr-solutions/index.html#footnotes",
    "href": "post/2022/geocompr-solutions/index.html#footnotes",
    "title": "Automatic website for the Geocomputation with R solutions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nExcept the last one.↩︎"
  },
  {
    "objectID": "post.html",
    "href": "post.html",
    "title": "Posts",
    "section": "",
    "text": "We welcome guest posts on geocomputation topics! If you have ideas to share, the best way to get in touch is by opening an issue at https://github.com/geocompx/geocompx.org/issues. We look forward to your contributions.\n Posts related to R are aggregated at R-bloggers and RWeekly \n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nShapefile must… be replaced\n\n\n\n\n\n\npackages\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJun 2, 2024\n\n\n2 min\n\n\n\n\n\n\n\nGeocomputation with R competition: book cover for the 2nd edition\n\n\n\n\n\n\nposts\n\n\nrstats\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nJan 17, 2024\n\n\n3 min\n\n\n\n\n\n\n\nAn overview of the rsi R package for retrieving satellite imagery and calculating spectral indices\n\n\n\n\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJan 10, 2024\n\n\n7 min\n\n\n\n\n\n\n\nSay hi to ‘Geocomputation with Python’\n\n\n\n\n\n\npython\n\n\ngeopython\n\n\ngeocompy\n\n\n\n\n\n\n\n\n\nDec 12, 2023\n\n\n3 min\n\n\n\n\n\n\n\nGeocomputation with R comptetition: book cover for the 2nd edition\n\n\n\n\n\n\nposts\n\n\nrstats\n\n\nannouncement\n\n\n\n\n\n\n\n\n\nDec 1, 2023\n\n\n2 min\n\n\n\n\n\n\n\nR-spatial beyond sf\n\n\n\n\n\n\nposts\n\n\nrstats\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\n9 min\n\n\n\n\n\n\n\nGeographic data analysis in R and Python: comparing code and outputs for vector data\n\n\n\n\n\n\nposts\n\n\nrstats\n\n\npython\n\n\n\n\n\n\n\n\n\nAug 30, 2023\n\n\n10 min\n\n\n\n\n\n\n\nUpcoming changes to popular R packages for spatial data: what you need to do\n\n\n\n\n\n\nposts\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJun 4, 2023\n\n\n10 min\n\n\n\n\n\n\n\nAutomatic website for the Geocomputation with R solutions\n\n\n\n\n\n\nvignette\n\n\nrstats\n\n\n\n\n\n\n\n\n\nFeb 24, 2022\n\n\n4 min\n\n\n\n\n\n\n\nProgress update: Geocomputation with R Second Edition Part 1\n\n\n\n\n\n\nupdates\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJan 27, 2022\n\n\n5 min\n\n\n\n\n\n\n\nGeocomputation with R: Second Edition feedback\n\n\n\n\n\n\nannouncement\n\n\nrstats\n\n\n\n\n\n\n\n\n\nSep 6, 2021\n\n\n2 min\n\n\n\n\n\n\n\nConversions between different spatial classes in R\n\n\n\n\n\n\nvignette\n\n\nrstats\n\n\n\n\n\n\n\n\n\nJun 17, 2021\n\n\n5 min\n\n\n\n\n\n\n\nRecent changes in R spatial and how to be ready for them\n\n\n\n\n\n\nannouncement\n\n\nrstats\n\n\n\n\n\n\n\n\n\nApr 25, 2020\n\n\n2 min\n\n\n\n\n\n\n\nDemo of reproducible geographic data analysis: mapping Covid-19 data with R\n\n\n\n\n\n\ndemo\n\n\nrstats\n\n\n\n\n\n\n\n\n\nApr 23, 2020\n\n\n10 min\n\n\n\n\n\n\n\nInstalling spatial R packages on Ubuntu\n\n\n\n\n\n\nsetup\n\n\nrstats\n\n\n\n\n\n\n\n\n\nMar 30, 2020\n\n\n12 min\n\n\n\n\n\n\n\nInset maps with ggplot2\n\n\n\n\n\n\nvignette\n\n\nrstats\n\n\n\n\n\n\n\n\n\nDec 8, 2019\n\n\n9 min\n\n\n\n\n\n\n\nMap coloring: the color scale styles available in the tmap package\n\n\n\n\n\n\nvignette\n\n\nrstats\n\n\n\n\n\n\n\n\n\nOct 17, 2019\n\n\n10 min\n\n\n\n\n\n\n\nGrids and graticules in the tmap package\n\n\n\n\n\n\nvignette\n\n\nrstats\n\n\n\n\n\n\n\n\n\nSep 4, 2019\n\n\n3 min\n\n\n\n\n\n\n\nGeographic projections and transformations\n\n\n\n\n\n\nvignette\n\n\nrstats\n\n\n\n\n\n\n\n\n\nAug 21, 2019\n\n\n7 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "static/presentations/libs/leaflet-binding/lib/leaflet-providers/rstudio_install.html",
    "href": "static/presentations/libs/leaflet-binding/lib/leaflet-providers/rstudio_install.html",
    "title": "Location",
    "section": "",
    "text": "Location\n\nfrom: github.com/schloerke/leaflet-providers@urlProtocol\nInspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‘old’ relative url protocols and to ‘upgrade’ them at js runtime.\n\n\n\nNotes…\n\nCopy/paste provider information into providers.json\n\nvar providers = L.TileLayer.Provider.providers;\nJSON.stringify(providers, null, \"  \");\n\n./data-raw/providerNames.R was re-ran to update to the latest providers\nSome providers had their protocols turned into ‘//’.\n\nThis allows browsers to pick the protocol\nTo stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize"
  },
  {
    "objectID": "static/workshops/libs/leaflet-binding/lib/leaflet-providers/rstudio_install.html",
    "href": "static/workshops/libs/leaflet-binding/lib/leaflet-providers/rstudio_install.html",
    "title": "Location",
    "section": "",
    "text": "Location\n\nfrom: github.com/schloerke/leaflet-providers@urlProtocol\nInspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‘old’ relative url protocols and to ‘upgrade’ them at js runtime.\n\n\n\nNotes…\n\nCopy/paste provider information into providers.json\n\nvar providers = L.TileLayer.Provider.providers;\nJSON.stringify(providers, null, \"  \");\n\n./data-raw/providerNames.R was re-ran to update to the latest providers\nSome providers had their protocols turned into ‘//’.\n\nThis allows browsers to pick the protocol\nTo stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize"
  },
  {
    "objectID": "static/workshops/erum2018/libs/leaflet-binding/lib/leaflet-providers/rstudio_install.html",
    "href": "static/workshops/erum2018/libs/leaflet-binding/lib/leaflet-providers/rstudio_install.html",
    "title": "Location",
    "section": "",
    "text": "Location\n\nfrom: github.com/schloerke/leaflet-providers@urlProtocol\nInspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‘old’ relative url protocols and to ‘upgrade’ them at js runtime.\n\n\n\nNotes…\n\nCopy/paste provider information into providers.json\n\nvar providers = L.TileLayer.Provider.providers;\nJSON.stringify(providers, null, \"  \");\n\n./data-raw/providerNames.R was re-ran to update to the latest providers\nSome providers had their protocols turned into ‘//’.\n\nThis allows browsers to pick the protocol\nTo stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize"
  },
  {
    "objectID": "guestbook.html#geocomputation-with-r",
    "href": "guestbook.html#geocomputation-with-r",
    "title": "Guestbook",
    "section": "Geocomputation with R",
    "text": "Geocomputation with R\n\nModules using the book\n\nCASA0005 Geographic Information Systems and Science, Andy MacLachlan and Adam Dennett, Centre for Advanced Spatial Analysis, Univeristy College London\nECS530 Analysing spatial data, Roger Bivand, Norwegian School of Economics\nGEOG 495 and 595: Geographic Data Analysis, University of Oregon\nESPM 288: Reproducible and Collaborative Data Science, Berkeley University of California \nENVR_SCI 390-0 “R Data Science” - Special Topics in Environmental Sciences Northwestern University\nPD-15 - R for Geospatial Analysis and Mapping, University Consortium for Geographic Information Science \nCRD 298: Spatial Methods in Community Research, Professor Noli Brazil, University of California, Davis\nCP6521 Advanced GIS, Yongsun Lee, Georgia Tech\nUrban Data Analytics in R, Nikhil Kaza, University of North Carolina at Chapel Hill\nGeospatial Analytics, taught by Dr Nick Tate as part of the MSc in Geographical Information Science at the Unversity of Leicester, UK.\nGEOG 28402 & 28403: Geographic Information Science II & III, Marynia Kolak at the Center for Spatial Data Science, Unversity of Chicago\n\n\n\nOther teaching courses/lectures/guides using the book\n\nData Science for Economists, EC 607\n11th International Summer School2019: Spatial and digital Epidemiology: Leveraging geo-referenced social media in the context of urban health\nData Carpentry: Introduction to Geospatial Raster and Vector Data with R\nR: Mapping and Geospatial guide, Duke University\n\n\n\nBook reviews\n\nI first read the first edition around 3 years ago and was astounded that everything I had been doing piecemeal in Q, Arc, and with some sf functions could be done in an orderly manner and finally the geospatial libraries were accessibly explained. Sadly at the time I did not get past the first part and thought “this is great! but need to move on!”. A month or so ago I sat down to reread the book in its entirety, with the move to terra etc.\n\n\nBeautiful book: A must-read if you’re into data analytics/science with R and do geospatial analysis.\n\nSee the rest of these reviews and maybe add your own review here.\n\n\nSocial media"
  },
  {
    "objectID": "py.html",
    "href": "py.html",
    "title": "Python resources",
    "section": "",
    "text": "Geocomputation with Python (geocompy) is motivated by the need for an introductory, yet rigorous and up-to-date, resource geographic data with the most popular programming language in the world. A unique selling point of the book is its cohesive and joined-up coverage of both vector and raster geographic data models and consistent learning curve. We aim to minimize surprises, with each section and chapter building on the previous. If you’re just starting out with Python for working with geographic data, this book is an excellent place to start.\nInspired by the Free and Open Source Software for Geospatial (FOSS4G) movement this is an open source book. Find the code underlying the geocompy project on GitHub, ensuring that the content is reproducible, transparent, and accessible. Making the book open source allows you or anyone else, to interact with the project by opening issues, making typo fixes and more, for the benefit of everyone.\nThe online version of the book is hosted at py.geocompx.org/."
  },
  {
    "objectID": "py.html#book",
    "href": "py.html#book",
    "title": "Python resources",
    "section": "",
    "text": "Geocomputation with Python (geocompy) is motivated by the need for an introductory, yet rigorous and up-to-date, resource geographic data with the most popular programming language in the world. A unique selling point of the book is its cohesive and joined-up coverage of both vector and raster geographic data models and consistent learning curve. We aim to minimize surprises, with each section and chapter building on the previous. If you’re just starting out with Python for working with geographic data, this book is an excellent place to start.\nInspired by the Free and Open Source Software for Geospatial (FOSS4G) movement this is an open source book. Find the code underlying the geocompy project on GitHub, ensuring that the content is reproducible, transparent, and accessible. Making the book open source allows you or anyone else, to interact with the project by opening issues, making typo fixes and more, for the benefit of everyone.\nThe online version of the book is hosted at py.geocompx.org/."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About us",
    "section": "",
    "text": "Robin Lovelace\nJannes Muenchow\nAnita Grasser\nOlivier Leroy\nMany contibutors to geocompr, geocompy, and geocompx\n\n\nJakub Nowosad\nMichael Dorman\nMireia Camacho\nBaba Yoshihiko\n\n\n\nLearn about the story behind the geocompx project in our presentation at FOSS4G Europe 2024. This talk explores the origins, development, and future directions of our open-source geospatial initiative."
  },
  {
    "objectID": "r.html",
    "href": "r.html",
    "title": "R resources",
    "section": "",
    "text": "Geocomputation with R (geocompr) is for people who want to analyze, visualize and model geographic data with open source software. It is based on R, a statistical programming language that has powerful data processing, visualization, and geospatial capabilities. The book equips you with the knowledge and skills to tackle a wide range of issues manifested in geographic data, including those with scientific, societal, and environmental implications. The online version of the book is hosted at r.geocompx.org/.\nYou can buy it from:\n\nRoutledge.com\nAmazon.com\nAmazon.co.uk"
  },
  {
    "objectID": "r.html#book",
    "href": "r.html#book",
    "title": "R resources",
    "section": "",
    "text": "Geocomputation with R (geocompr) is for people who want to analyze, visualize and model geographic data with open source software. It is based on R, a statistical programming language that has powerful data processing, visualization, and geospatial capabilities. The book equips you with the knowledge and skills to tackle a wide range of issues manifested in geographic data, including those with scientific, societal, and environmental implications. The online version of the book is hosted at r.geocompx.org/.\nYou can buy it from:\n\nRoutledge.com\nAmazon.com\nAmazon.co.uk"
  },
  {
    "objectID": "r.html#solutions",
    "href": "r.html#solutions",
    "title": "R resources",
    "section": "Solutions",
    "text": "Solutions\nYou can find all the solutions to the book’s exercises at r.geocompx.org/solutions."
  },
  {
    "objectID": "r.html#translations",
    "href": "r.html#translations",
    "title": "R resources",
    "section": "Translations",
    "text": "Translations\nThe Geocomputation with R book has a few community translations:\n\nGeocomputación con R (in Spanish). Translation lead: Mireia Camacho\nGeocomputation avec R (in French). Translation lead: Olivier Leroy\nGeocomputation with R (in Japanese). Translation lead: Yoshihiko Baba\n\nAdditionally, the first edition of the book has been officially translated into Japanese and Korean (check your local bookstore)."
  },
  {
    "objectID": "r.html#presentations",
    "href": "r.html#presentations",
    "title": "R resources",
    "section": "Presentations",
    "text": "Presentations\n\n“GIS and mapping ecosystem of R”\nEcole thématique SIGR2021, Saint-Pierre-d’Oléron, 2021-06-29\n\nslides\n\n\n\n“Recent changes in R spatial and how to be ready for them”\nWhy R? Webinar, remotely, 2020-04-23\n\nslides\nvideo\n\n\n\n“How to win friends and write an open-source book”\nuseR! 2019, Toulouse, France, 2019-07-12\n\nslides\n\n\n\n“Reproducible spatial data analysis: An example of transportation analysis for Bristol”\nCollegium Da Vinci. Poznan, Poland, 2019-01-29\n\nslides\n\n\n\n“Geocomputation with R: Data science, open source software and geo* data”\nNorthwest Universities R Day. Manchester, UK, 2018-10-31\n\nslides\n\n\n\n“Spatial data and the tidyverse”\nGEOSTAT2018. Prague, Czech Republic, 2018-08-22\n\nslides\n\n\n\n“Geocomputation with R: An overview of the field and introduction to the book”\nGEOSTAT2018. Prague, Czech Republic, 2018-08-20\n\nslides\n\n\n\n“Introduction to geocomputation with R”\nCinDay RUG. Mason, Ohio, 2018-05-22\n\nslides\n\n\n\n“Spatial data and the tidyverse”\nGeocomputation 2017. Leeds, UK, 2017-09-04\n\nslides"
  },
  {
    "objectID": "r.html#workshops",
    "href": "r.html#workshops",
    "title": "R resources",
    "section": "Workshops",
    "text": "Workshops\n\n“GIS and mapping ecosystem of R” and “Image processing and raster analysis with R”\nEcole thématique SIGR2021, Saint-Pierre-d’Oléron, 2021-06-29\n\nAll slides and additional information\n\n\n\nGeocomputation with R –\nEuropean Geosciences Union. Vienna, Austria, 2019-04-10\nTalk and practical workshop introducing packages for handling geographic data, based on chapters 2, 3 and 8 in the book\n\nIntroductory slides\nGeographic vector data in R\nGeographic raster data in R\nMaking maps with R: from static maps towards web applications\n\n\n\n“Geocomputation with R” –\neRum 2018. Budapest, Hungary, 2018-05-14\nSlides:\n\nBasics\nVector\nRaster\nViz\nrqgis\n\n\n\n“GIS with R: how to start?”\nGIS Learning Community. Cincinnati, Ohio, 2017-11-01\n\nGitHub repo"
  },
  {
    "objectID": "r.html#vignettes",
    "href": "r.html#vignettes",
    "title": "R resources",
    "section": "Vignettes",
    "text": "Vignettes\n\nThese vignettes provide extended examples for several methods and functions mentioned in Geocomputation with R\n\nSpatial Joins Extended\nMaking inset maps of the USA\nAlgorithms Extended\nPoint Pattern analysis, spatial interpolation and heatmaps\nDesire Lines Extended\nSpatial data and the tidyverse: pitfalls to avoid"
  },
  {
    "objectID": "r.html#other-recommened-books",
    "href": "r.html#other-recommened-books",
    "title": "R resources",
    "section": "Other recommened books",
    "text": "Other recommened books\n\nElegant and informative maps with tmap from Martijn Tennekes and Jakub Nowosad\nSpatial Data Science with applications in R from Edzer Pebesma and Roger Bivand; with solutions"
  },
  {
    "objectID": "static/workshops/erum2018/libs/leaflet-providers/rstudio_install.html",
    "href": "static/workshops/erum2018/libs/leaflet-providers/rstudio_install.html",
    "title": "Location",
    "section": "",
    "text": "Location\n\nfrom: github.com/schloerke/leaflet-providers@urlProtocol\nInspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‘old’ relative url protocols and to ‘upgrade’ them at js runtime.\n\n\n\nNotes…\n\nCopy/paste provider information into providers.json\n\nvar providers = L.TileLayer.Provider.providers;\nJSON.stringify(providers, null, \"  \");\n\n./data-raw/providerNames.R was re-ran to update to the latest providers\nSome providers had their protocols turned into ‘//’.\n\nThis allows browsers to pick the protocol\nTo stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize"
  },
  {
    "objectID": "static/workshops/libs/leaflet-providers/rstudio_install.html",
    "href": "static/workshops/libs/leaflet-providers/rstudio_install.html",
    "title": "Location",
    "section": "",
    "text": "Location\n\nfrom: github.com/schloerke/leaflet-providers@urlProtocol\nInspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‘old’ relative url protocols and to ‘upgrade’ them at js runtime.\n\n\n\nNotes…\n\nCopy/paste provider information into providers.json\n\nvar providers = L.TileLayer.Provider.providers;\nJSON.stringify(providers, null, \"  \");\n\n./data-raw/providerNames.R was re-ran to update to the latest providers\nSome providers had their protocols turned into ‘//’.\n\nThis allows browsers to pick the protocol\nTo stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize"
  },
  {
    "objectID": "static/presentations/libs/leaflet-providers/rstudio_install.html",
    "href": "static/presentations/libs/leaflet-providers/rstudio_install.html",
    "title": "Location",
    "section": "",
    "text": "Location\n\nfrom: github.com/schloerke/leaflet-providers@urlProtocol\nInspiration taken from https://github.com/leaflet-extras/leaflet-providers/commit/dea786a3219f9cc824b8e96903a17f46ca9a5afc to use the ‘old’ relative url protocols and to ‘upgrade’ them at js runtime.\n\n\n\nNotes…\n\nCopy/paste provider information into providers.json\n\nvar providers = L.TileLayer.Provider.providers;\nJSON.stringify(providers, null, \"  \");\n\n./data-raw/providerNames.R was re-ran to update to the latest providers\nSome providers had their protocols turned into ‘//’.\n\nThis allows browsers to pick the protocol\nTo stop files from the protocols staying as files, a ducktape patch was applied to L.TileLayer.prototype.initialize and L.TileLayer.WMS.prototype.initialize"
  },
  {
    "objectID": "static/img/book_cover_py_tmp.html",
    "href": "static/img/book_cover_py_tmp.html",
    "title": "geocompx",
    "section": "",
    "text": "https://www.oldbookillustrations.com/subjects/animals/page/9/ https://alexaanswers.amazon.com/question/wZ1u5YAK7xPaZCJxawr51"
  },
  {
    "objectID": "post/2022/geocompr2-bp2/index.html",
    "href": "post/2022/geocompr2-bp2/index.html",
    "title": "Progress update: Geocomputation with R Second Edition Part 1",
    "section": "",
    "text": "Geocomputation with R is a book on geographic data analysis, visualization, and modeling. The First Edition was first published as a physical book in 2019 and we have reached 500k people through via the website at https://geocompr.robinlovelace.net/ and the physical book since then.\nThe book has also become a key part of many university courses, lectures, and has been endorsed by many people who have found it vital for their work and for learning new geographic skills, as outlined in our guestbook at geocompr.github.io/guestbook/. Tools and methods for geographic data analysis are evolving quickly, especially in the R-Spatial ecosystem. Because of these changes, and demand from our readers and the wider community, we decided in September 2021 to start working on the second edition of the book. The second edition (work in progress) is available at https://geocompr.robinlovelace.net/, while you can find the first edition at https://bookdown.org/robinlovelace/geocompr/.\nAt the time of writing (January 2022), we have already made many changes to first part of the book called Foundations. In this post we list and explain the changes."
  },
  {
    "objectID": "post/2022/geocompr2-bp2/index.html#major-changes",
    "href": "post/2022/geocompr2-bp2/index.html#major-changes",
    "title": "Progress update: Geocomputation with R Second Edition Part 1",
    "section": "Major changes",
    "text": "Major changes\nSeveral major changes have happened to #rspatial since the release of the first version of the book. Most importantly:1\n\nPROJ library was refactored mainly, which have had an impact on a plethora of GIS software, including R spatial packages\nThe raster package, highlighted in the 1st edition, is now being replaced by its successor terra\n\n\nBoth of these changes are now addressed in Geocomputation with R. Chapter “Reprojecting geographic data” has undergone numerous modifications, including an explanation of the currently recommended WKT coordinate reference systems representation, information on how to get and set CRSs, or a much-improved section on creating custom projections. Similarly, we already replaced raster’s descriptions and functions with terra in the book’s first eight chapters."
  },
  {
    "objectID": "post/2022/geocompr2-bp2/index.html#changes-to-specific-chapters",
    "href": "post/2022/geocompr2-bp2/index.html#changes-to-specific-chapters",
    "title": "Progress update: Geocomputation with R Second Edition Part 1",
    "section": "Changes to specific chapters",
    "text": "Changes to specific chapters\nAdditionally, almost all chapters received significant edits. Section The history of R-spatial in 1st chapter was updated with summary of various new #rspatial developments, including terra, stars, lidR, rayshader, ggspatial, mapsf, etc. We also used this section to highlight that packages rgdal, rgeos, and maptools will be retired at the end of 2023 and that anyone still using these packages should “transition to sf/stars/terra functions using GDAL and PROJ at your earliest convenience”.”\nSecond chapter got an information about s2, an interface to spherical geometry library already used by the sf package, and a comparison between terra and stars.\nIn Chapter 4, we added improved communication of binary spatial predicates in Chapter 4 and a new section on the dimensionally extended 9 intersection model (DE-9IM).\n\nThe vector part of Chapter 5 received a new section on the links between subsetting and clipping, while the raster part got a much improved part on resampling methods. We also decided to split out the text related raster-vector interactions into a new 6th chapter.\nFinally, Chapter 8 got a bunch of improvements, including mentions of more data packages, alternative ways of reading-in OSM data, a part about geocoding, and information about Cloud Optimized GeoTIFF (COG). We also spent some time to show how to read just a part of vector data file using OGR SQL queries and WKT filters, and extract only a portion of a COG raster file."
  },
  {
    "objectID": "post/2022/geocompr2-bp2/index.html#book-infrastructure",
    "href": "post/2022/geocompr2-bp2/index.html#book-infrastructure",
    "title": "Progress update: Geocomputation with R Second Edition Part 1",
    "section": "Book infrastructure",
    "text": "Book infrastructure\nWe also decided that the 2nd edition is a good time to improve the book infrastructure, i.e., data packages, storage of the exercises and solutions, automatic book builds, etc.\nThe most visible change is the visual theme of the book website. Now it uses the Bootstrap 4 framework through the bookdown::bs4_book theme with slightly customized elements, such as font families or a text blocks style.\nNext, we created an external bookdown-based website for solutions of the Geocomputation with R exercises. You can find its early version at https://geocompr.github.io/solutions/, and we plan to describe it in more detail in a future blog post.\nThe book still uses two external packages, spData and spDataLarge, to store example datasets. However, we made an important change here – we saved some R spatial objects to files (e.g., raster objects to GeoTIFF) – this way, we can read data using raster/terra/stars, instead of just loading one package’s class object into R memory.\nWe refactored build settings, so the book builds on Docker images in the geocompr/docker repo, and improved the experience of using the book in Binder (ideal for trying out the code before installing or updating the necessary R packages), as documented in issue #691 (thanks to yuvipanda)."
  },
  {
    "objectID": "post/2022/geocompr2-bp2/index.html#minor-changes",
    "href": "post/2022/geocompr2-bp2/index.html#minor-changes",
    "title": "Progress update: Geocomputation with R Second Edition Part 1",
    "section": "Minor changes",
    "text": "Minor changes\nWe reworked several plots using the tmap package to improve the visual consistency of the book.\n\nThe book also received (almost) countless minor changes, including rephrasing many concepts and reordering of our prose to make some ideas easier to understand."
  },
  {
    "objectID": "post/2022/geocompr2-bp2/index.html#outro",
    "href": "post/2022/geocompr2-bp2/index.html#outro",
    "title": "Progress update: Geocomputation with R Second Edition Part 1",
    "section": "Outro",
    "text": "Outro\nOur plan now is to switch attention to the second part of the book called Extensions, while Part 1 is sent for peer review. We will work on the enhanced Making maps chapter, updates on Bridges to GIS software and Statistical learning.\nThe work and updates on the second edition of Geocomputation with R would not be possible without the interest and activity of our readers! We want to thank all of you, including authors of recent Pull Requests (ec-nebi, e-clin, Oliver Leroy defuneste, Ivan Dubrovin iod-ine, Floris Vanderhaeghe florisvdh, Hasmukh K Mistry hasukmistry, John S. Erickson olyerickson) and people raising new GitHub issues. We are still actively working on the book, and thus – let us know if you have any issues or suggestions and feel free to create any Pull Request from fixing typos, clarifying unclear sentences, to changes to the code and prose. You can also contact us on the Geocomputation with R discord channel."
  },
  {
    "objectID": "post/2022/geocompr2-bp2/index.html#footnotes",
    "href": "post/2022/geocompr2-bp2/index.html#footnotes",
    "title": "Progress update: Geocomputation with R Second Edition Part 1",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are also other significant changes, such as the retirement of RQGIS and mlr, but we plan to focus on them while updating the second part of the book.↩︎"
  },
  {
    "objectID": "post/2019/ggplot2-inset-maps/index.html",
    "href": "post/2019/ggplot2-inset-maps/index.html",
    "title": "Inset maps with ggplot2",
    "section": "",
    "text": "Inset maps enable multiple places to be shown in the same geographic data visualisation, as described in the Inset maps section (8.2.7) of our open source book Geocomputation with R. The topic of inset maps has gained attention and recently Enrico Spinielli asked inset maps could be created for data in unusual coordinate.\nR’s flexibility allows inset maps to be created in various ways, using different approaches and packages. However, the main idea stays the same: we need to create at least two maps: a larger one, called the main map, that shows the central story and a smaller one, called the inset map, that puts the main map in context.\nThis blog post shows how to create inset maps with ggplot2 for visualization. The approach also uses the sf package for spatial data reading and handling, cowplot to arrange inset maps, and rcartocolor for additional color palettes. To reproduce the results on your own computer, after installing them, these packages can be attached as follows:\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(cowplot)\nlibrary(rcartocolor)"
  },
  {
    "objectID": "post/2019/ggplot2-inset-maps/index.html#data-preparation",
    "href": "post/2019/ggplot2-inset-maps/index.html#data-preparation",
    "title": "Inset maps with ggplot2",
    "section": "Data preparation",
    "text": "Data preparation\nThe first step is to read and prepare the data we want to visualize. We use the us_states data from the spData package as the source of the inset map, and north_carolina from the sf package as the source of the main map.\n\nlibrary(spData)\ndata(\"us_states\", package = \"spData\")\nnorth_carolina = read_sf(system.file(\"shape/nc.shp\", package = \"sf\"))\n\nBoth objects should have the same coordinate reference system (crs). Here, we use crs = 2163, which represents the US National Atlas Equal Area projection.\n\nus_states_2163 = st_transform(us_states, crs = 2163)\nnorth_carolina_2163 = st_transform(north_carolina, crs = 2163)\n\nWe also need to have the borders of the area we want to highlight (use in the main map). This can be done by extracting the bounding box of our north_carolina_2163 object.\n\nnorth_carolina_2163_bb = st_as_sfc(st_bbox(north_carolina_2163))"
  },
  {
    "objectID": "post/2019/ggplot2-inset-maps/index.html#maps-creation",
    "href": "post/2019/ggplot2-inset-maps/index.html#maps-creation",
    "title": "Inset maps with ggplot2",
    "section": "Maps creation",
    "text": "Maps creation\nThe second step is to create both inset and main maps independently. The inset map should show the context (larger area) and highlight the area of interest.\n\nggm1 = ggplot() + \n  geom_sf(data = us_states_2163, fill = \"white\") + \n  geom_sf(data = north_carolina_2163_bb, fill = NA, color = \"red\", size = 1.2) +\n  theme_void()\n\nggm1\n\n\n\n\n\n\n\n\nThe main map’s role is to tell the story. Here we show the number of births between 1974 and 1978 in the North Carolina counties (the BIR74 variable) using the Mint color palette from the rcartocolor palette. We also customize the legend position and size - this way, the legend is a part of the map, instead of being somewhere outside the map frame.\n\nggm2 = ggplot() + \n  geom_sf(data = north_carolina_2163, aes(fill = BIR74)) +\n  scale_fill_carto_c(palette = \"Mint\") +\n  theme_void() +\n  theme(legend.position = c(0.4, 0.05),\n        legend.direction = \"horizontal\",\n        legend.key.width = unit(10, \"mm\"))\n\nggm2"
  },
  {
    "objectID": "post/2019/ggplot2-inset-maps/index.html#maps-joining",
    "href": "post/2019/ggplot2-inset-maps/index.html#maps-joining",
    "title": "Inset maps with ggplot2",
    "section": "Maps joining",
    "text": "Maps joining\nThe final step is to join two maps. This can be done using functions from the cowplot package. We create an empty ggplot layer using ggdraw(), fill it with out main map (draw_plot(ggm2)), and add an inset map by specifing its position and size:\n\ngg_inset_map1 = ggdraw() +\n  draw_plot(ggm2) +\n  draw_plot(ggm1, x = 0.05, y = 0.65, width = 0.3, height = 0.3)\n\ngg_inset_map1\n\n\n\n\n\n\n\n\nThe final map can be saved using the ggsave() function.\n\nggsave(filename = \"01_gg_inset_map.png\", \n       plot = gg_inset_map1,\n       width = 8, \n       height = 4,\n       dpi = 150)"
  },
  {
    "objectID": "post/2019/ggplot2-inset-maps/index.html#data-preparation-1",
    "href": "post/2019/ggplot2-inset-maps/index.html#data-preparation-1",
    "title": "Inset maps with ggplot2",
    "section": "Data preparation",
    "text": "Data preparation\nThis map will use the US states borders (states()) as the source of the inset map and the Kentucky Senate legislative districts (state_legislative_districts()) as the main map.\n\nlibrary(tigris)\noptions(tigris_use_cache = TRUE)\nus_states = states(cb = FALSE, class = \"sf\")\nky_districts = state_legislative_districts(\"KY\", house = \"upper\",\n                                           cb = FALSE, class = \"sf\")\n\nThe states() function, in addition to the 50 states, also returns the District of Columbia, Puerto Rico, American Samoa, the Commonwealth of the Northern Mariana Islands, Guam, and the US Virgin Islands. For our purpose, we are interested in the continental 48 states and the District of Columbia only; therefore, we remove the rest of the divisions using subset().\n\nus_states = subset(us_states, \n                   !NAME %in% c(\n                     \"United States Virgin Islands\",\n                     \"Commonwealth of the Northern Mariana Islands\",\n                     \"Guam\",\n                     \"American Samoa\",\n                     \"Puerto Rico\",\n                     \"Alaska\",\n                     \"Hawaii\"\n                   ))\n\nThe same as in the example above, we transform both objects to have the same projection.\n\nky_districts_2163 = st_transform(ky_districts, crs = 2163)\nus_states_2163 = st_transform(us_states, crs = 2163)\n\nWe also extract the bounding box of the main object here. However, instead of using it directly, we add a buffer of 10,000 meters around it. This output will be handy in both inset and main maps.\n\nky_districts_2163_bb = st_as_sfc(st_bbox(ky_districts_2163))\nky_districts_2163_bb = st_buffer(ky_districts_2163_bb, dist = 10000)\n\nThe ky_districts_2163 object does not have any interesting variables to visualize, so we create some random values here. However, we could also join the districts’ data with another dataset in this step.\n\nky_districts_2163$values = runif(nrow(ky_districts_2163))"
  },
  {
    "objectID": "post/2019/ggplot2-inset-maps/index.html#map-creation",
    "href": "post/2019/ggplot2-inset-maps/index.html#map-creation",
    "title": "Inset maps with ggplot2",
    "section": "Map creation",
    "text": "Map creation\nThe inset map should be as clear and simple as possible.\n\nggm3 = ggplot() + \n  geom_sf(data = us_states_2163, fill = \"white\", size = 0.2) + \n  geom_sf(data = ky_districts_2163_bb, fill = NA, color = \"blue\", size = 1.2) +\n  theme_void()\n\nggm3\n\n\n\n\n\n\n\n\nOn the other hand, the main map looks better when we provide some additional context to our data. One of the ways to achieve it is to add the borders of the neighboring states.\nImportantly, we also need to limit the extent of our main map to the range of the frame in the inset map. This can be done with the coord_sf() function.\n\nggm4 = ggplot() + \n  geom_sf(data = us_states_2163, fill = \"#F5F5DC\") +\n  geom_sf(data = ky_districts_2163, aes(fill = values)) +\n  scale_fill_carto_c(palette = \"Sunset\") +\n  theme_void() +\n  theme(legend.position = c(0.5, 0.07),\n        legend.direction = \"horizontal\",\n        legend.key.width = unit(10, \"mm\"),\n        plot.background = element_rect(fill = \"#BFD5E3\")) +\n  coord_sf(xlim = st_bbox(ky_districts_2163_bb)[c(1, 3)],\n           ylim = st_bbox(ky_districts_2163_bb)[c(2, 4)])\n\nggm4\n\n\n\n\n\n\n\n\nFinally, we draw two maps together, trying to find the best location and size for the inset map.\n\ngg_inset_map2 = ggdraw() +\n  draw_plot(ggm4) +\n  draw_plot(ggm3, x = 0.02, y = 0.65, width = 0.35, height = 0.35)\n\ngg_inset_map2\n\n\n\n\n\n\n\n\nThe final map can be saved using the ggsave() function.\n\nggsave(filename = \"02_gg_inset_map.png\", \n       plot = gg_inset_map2,\n       width = 7.05, \n       height = 4,\n       dpi = 150)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html",
    "href": "post/2019/tmap-styles/index.html",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "",
    "text": "This vignette builds on the making maps chapter of the Geocomputation with R book. Its goal is to demonstrate all possible map styles available in the tmap package."
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#prerequisites",
    "href": "post/2019/tmap-styles/index.html#prerequisites",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Prerequisites",
    "text": "Prerequisites\nThe examples below assume the following packages are attached:\n\nlibrary(spData) # example datasets\nlibrary(tmap)   # map creation\nlibrary(sf)     # spatial data reprojection\n\nThe world object containing a world map data from Natural Earth and information about countries’ names, regions, and subregions they belong to, areas, life expectancies, and populations. This object is in geographical coordinates using the WGS84 datum, however, for mapping purposes, the Mollweide projection is a better alternative (learn more in the modifying map projections section). The st_tranform function from the sf package allows for quick reprojection to the selected coordinate reference system (e.g., \"+proj=moll\" represents the Mollweide projection).\n\nworld_moll = st_transform(world, crs = \"+proj=moll\")"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#pretty",
    "href": "post/2019/tmap-styles/index.html#pretty",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Pretty",
    "text": "Pretty\nWhen the variable provided as the col argument is numeric, tmap will use the \"pretty\" style as a default. In other words, it runs tm_polygons(col = \"lifeExp\", style = \"pretty\") invisibly to the user. This style rounds breaks into whole numbers where possible and spaces them evenly.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE) \n\n\n\n\n\n\n\n\nA histogram is added using legend.hist = TRUE in this and several next examples to show how the selected map style relates to the distribution of values.\nIt is possible to indicate a preferred number of classes using the n argument. Importantly, not every n is possible depending on the range of the values in the data.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              legend.hist = TRUE,\n              n = 4) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#fixed",
    "href": "post/2019/tmap-styles/index.html#fixed",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Fixed",
    "text": "Fixed\nThe \"fixed\" style allows for a manual selection of the breaks in conjunction with the breaks argument.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\", \n              style = \"fixed\",\n              breaks = c(45, 60, 75, 90),\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE) \n\n\n\n\n\n\n\n\nAdditionally, the default labels can be overwritten using the labels argument.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\", \n              style = \"fixed\",\n              breaks = c(45, 60, 75, 90),\n              labels = c(\"low\", \"medium\", \"high\"),\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#breaks-based-on-the-standard-deviation-value",
    "href": "post/2019/tmap-styles/index.html#breaks-based-on-the-standard-deviation-value",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Breaks based on the standard deviation value",
    "text": "Breaks based on the standard deviation value\nThe \"sd\" style calculates a standard deviation of a given variable, and next use this value as the break width.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\", \n              style = \"sd\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#fisher-algorithm",
    "href": "post/2019/tmap-styles/index.html#fisher-algorithm",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Fisher algorithm",
    "text": "Fisher algorithm\nThe \"fisher\" style creates groups with maximalized homogeneity.1\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              style = \"fisher\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#jenks-natural-breaks",
    "href": "post/2019/tmap-styles/index.html#jenks-natural-breaks",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Jenks natural breaks",
    "text": "Jenks natural breaks\nThe \"jenks\" style identifies groups of similar values in the data and maximizes the differences between categories.2\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              style = \"jenks\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#hierarchical-clustering",
    "href": "post/2019/tmap-styles/index.html#hierarchical-clustering",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Hierarchical clustering",
    "text": "Hierarchical clustering\nIn the \"hclust\" style, breaks are created using hierarchical clustering.3\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              style = \"hclust\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#bagged-clustering",
    "href": "post/2019/tmap-styles/index.html#bagged-clustering",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Bagged clustering",
    "text": "Bagged clustering\nThe \"bclust\" style uses the bclust function to generate the breaks using bagged clustering.4\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              style = \"bclust\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE) \n\n\n\n\n\n\n\n\nCommittee Member: 1(1) 2(1) 3(1) 4(1) 5(1) 6(1) 7(1) 8(1) 9(1) 10(1)\nComputing Hierarchical Clustering"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#k-means-clustering",
    "href": "post/2019/tmap-styles/index.html#k-means-clustering",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "k-means clustering",
    "text": "k-means clustering\nThe \"kmeans\" style uses the kmeans function to generate the breaks.5\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\", \n              style = \"kmeans\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#quantile-breaks",
    "href": "post/2019/tmap-styles/index.html#quantile-breaks",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Quantile breaks",
    "text": "Quantile breaks\nThe \"quantile\" style creates breaks with an equal number of features (polygons).\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\", \n              style = \"quantile\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#equal-breaks",
    "href": "post/2019/tmap-styles/index.html#equal-breaks",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Equal breaks",
    "text": "Equal breaks\nThe \"equal\" style divides input values into bins of equal range and is appropriate for variables with a uniform distribution. It is not recommended for variables with a skewed distribution as the resulting map may end-up having little color diversity.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\", \n              style = \"equal\",\n              legend.hist = TRUE) +\n  tm_layout(legend.outside = TRUE) \n\n\n\n\n\n\n\n\nLearn more about the implementation of discrete scales in the classInt package’s documentation."
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#continuous",
    "href": "post/2019/tmap-styles/index.html#continuous",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Continuous",
    "text": "Continuous\nThe \"cont\" style presents a large number of colors over the continuous color field.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              style = \"cont\") +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#order",
    "href": "post/2019/tmap-styles/index.html#order",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Order",
    "text": "Order\nThe \"order\" style also presents a large number of colors over the continuous color field. However, this style is suited to visualize skewed distributions; notice that the values on the legend do not change linearly.\n\ntm_shape(world_moll) +\n  tm_polygons(col = \"lifeExp\",\n              style = \"order\") +\n  tm_layout(legend.outside = TRUE)"
  },
  {
    "objectID": "post/2019/tmap-styles/index.html#footnotes",
    "href": "post/2019/tmap-styles/index.html#footnotes",
    "title": "Map coloring: the color scale styles available in the tmap package",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.tandfonline.com/doi/abs/10.1080/01621459.1958.10501479↩︎\nhttps://en.wikipedia.org/wiki/Jenks_natural_breaks_optimization↩︎\nSee the ?hclust documentation for more details.↩︎\nSee the ?bclust documentation for more details.↩︎\nSee the ?kmeans documentation for more details.↩︎"
  },
  {
    "objectID": "post/2020/recent-changes-in-r-spatial-and-how-to-be-ready-for-them/index.html",
    "href": "post/2020/recent-changes-in-r-spatial-and-how-to-be-ready-for-them/index.html",
    "title": "Recent changes in R spatial and how to be ready for them",
    "section": "",
    "text": "Currently, hundreds of R packages are related to spatial data analysis. They range from ecology and earth observation, hydrology and soil science, to transportation and demography. These packages support various stages of analysis, including data preparation, visualization, modeling, or communicating the results. One common feature of most R spatial packages is that they are built upon some of the main representations of spatial data in R, available in key geographic R packages such as:\n\nsf, which replaces sp\nterra, which aims to replace raster\nstars\n\nThose packages are also not entirely independent. They are using external libraries, namely GEOS for spatial data operations, GDAL for reading and writing spatial data, and PROJ for conversions of spatial coordinates.\nTherefore, R spatial packages are interwoven with each other and depend partially on external software developments. This has several positives, including the ability to use cutting-edge features and algorithms. On the other hand, it also makes R spatial packages vulnerable to changes in the upstream packages and libraries.\nIn the first part of the talk, we showcase several recent advances in R packages. It includes the largest recent change related to the developments in the PROJ library. We explain why the changes happened and how they impact R users. The second part focus on how to prepare for the changes, including computer set-up and running R spatial packages using Docker (briefly covered in a previous post and outlined in the new geocompr/docker repo). We outline important considerations when setting-up operating systems for geographic R packages. To reduce set-up times you can use geographic R packages Docker, a flexible and scalable technology containerization technology. Docker can run on modern computers and on your browser via services such as Binder, greatly reducing set-up times. Discussing these set-up options, and questions of compatibility between geographic R packages and paradigms such as the tidyverse and data.table, ensure that after the talk everyone can empower themselves with open source software for geographic data analysis in a powerful and flexible statistical programming environment.\nYou can find the slides for the talk at https://nowosad.github.io/whyr_webinar004/.\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{nowosad,_robin_lovelace2020,\n  author = {Nowosad, Robin Lovelace, Jakub},\n  title = {Recent Changes in {R} Spatial and How to Be Ready for Them},\n  date = {2020-04-25},\n  url = {https://geocompx.org/post/2020/recent-changes-in-r-spatial-and-how-to-be-ready-for-them/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nNowosad, Robin Lovelace, Jakub. 2020. “Recent Changes in R Spatial\nand How to Be Ready for Them.” April 25, 2020. https://geocompx.org/post/2020/recent-changes-in-r-spatial-and-how-to-be-ready-for-them/."
  },
  {
    "objectID": "post/2020/installing-r-spatial-packages-linux/index.html",
    "href": "post/2020/installing-r-spatial-packages-linux/index.html",
    "title": "Installing spatial R packages on Ubuntu",
    "section": "",
    "text": "This post explains how to quickly get key R packages for geographic research installed on Ubuntu, a popular Linux distribution.\n\nA recent thread on the r-spatial GitHub organization alludes to many considerations when choosing a Linux set-up for work with geographic data, ranging from the choice of Linux distribution (distro) to the use of binary vs or compiled versions (binaries are faster to install). This post touches on some of these things. Its main purpose, though, is to provide advice on getting R’s key spatial packages up-and-running on a future-proof Linux operating system (Ubuntu).\nNow is a good time to be thinking about your R set-up because changes are in the pipeline and getting set-up (or preparing to get set-up) now could save hours in the future. These imminent changes include:\n\nThe next major release of R (4.0.0), scheduled for the 24th April (2020-04-24)\nThe next major release of Ubuntu (20.04), a Long Term Support (LTS) version that will be used by millions of servers and research computers worldwide for years to come. Coincidentally, Ubuntu 20.04 will be released a day earlier than R 4.0.0, on 23rd April (2020-04-23).\nOngoing changes to the OSGeo stack on which key geographic R packages depend, as documented in r-spatial repos and a recent blog post on how recent versions of PROJ enable more precise coordinate reference system definitions.\n\nTo keep-up with these changes, this post will be updated in late April when some of the dust has settled around these changes. However, the advice presented here should be future-proof. Upgrading Ubuntu is covered in the next section.\nThere many ways of getting Ubuntu set-up for spatial R packages. A benefit of Linux operating systems is that they offer choice and prevent ‘lock-in’. However, the guidance in the next section should reduce set-up time and improve maintainability (with updates managed by Ubuntu) compared with other ways of doing things, especially for beginners. If you’re planning to switch to Linux as the basis of your geographic work, this advice may be particularly useful. (The post was written in response to colleagues asking me how to set-up R on their new Ubuntu computers. If you would like a a computer running Ubuntu, check out companies that support open source operating systems and guides on installing Ubuntu on an existing machine).\nBy ‘key packages’ I mean the following, which enable the majority of day-to-day geographic data processing and visualization tasks:\n\nsf for reading, writing and working with a range geographic vector file formats and geometry types\nraster, a mature package for working with geographic raster data (see the terra for an in-development replacement for raster)\ntmap, a flexible package for making static and interactive maps\n\nThe focus is on Ubuntu because that’s what I’ve got most experience with and it is well supported by the community. Links for installing geographic R packages on other distros are provided in section 3.\n\n1. Installing spatial R packages on Ubuntu\n\n\nR’s spatial packages can be installed from source on the latest version of this popular operating system, once the appropriate repository has been set-up, meaning faster install times (only a few minutes including the installation of upstream dependencies). The following bash commands should install key geographic R packages on Ubuntu 19.10:\n# add a repository that ships the latest version of R:\nsudo add-apt-repository ppa:marutter/rrutter3.5\n# update the repositories so the software can be found:\nsudo apt update\n# install system dependencies:\nsudo apt install libudunits2-dev libgdal-dev libgeos-dev libproj-dev libfontconfig1-dev\n# binary versions of key R packages:\nsudo apt install r-base-dev r-cran-sf r-cran-raster r-cran-rjava\nTo test your installation of R has worked, try running R in an IDE such as RStudio or in the terminal by entering R. You should be able to run the following commands without problem:\nlibrary(sf)\n#&gt; Linking to GEOS 3.7.2, GDAL 2.4.2, PROJ 5.2.0\ninstall.packages(\"tmap\")\nIf you are using an older version of Ubuntu and don’t want to upgrade to 19.10, which will upgrade to (20.04) by the end of April 2020, see instructions at github.com/r-spatial/sf and detailed instructions on the blog rtask.thinkr.fr, which contains this additional shell command:\n# for Ubuntu 18.04\nsudo add-apt-repository ppa:marutter/c2d4u3.5\nThat adds a repository that ships hundreds of binary versions of R packages, meaning faster install times for packages (see the Binary package section of the open source book R Packages for more on binary packages). An updated repository, called c2d4u4.0 or similar, will be available for Ubuntu 20.04 in late April.\n\n\nIf you have issues with the instructions in this post here, you can find a wealth of answers on site such as StackOverflow, the sf issue tracker, r-sig-geo and Debian special interest group (SIG) email lists (the latter of which provided input into this blog post, thanks to Dirk Eddelbuettel and Michael Rutter).\n\n\n2. Updating R packages and upstream dependencies\nLinux operating systems allow you to customize your set-up in myriad ways. This can be enlightening but it can also be wasteful, so it’s worth considering the stability/cutting-edge continuum before diving into a particular set-up and potentially wasting time (if the previous section hasn’t already made-up your mind).\n\n\nA reliable way to keep close (but not too close) to the cutting edge on the R side on any operating system is simply to keep your packages up-to-date. Running the following command (or using the Tools menu in RStudio) every week or so will ensure you have up-to-date package versions:\n\nupdate.packages()\n\nKeeping system dependencies, software that R relies on but that is not maintained by R developers, is also important but can be tricky, especially for large and complex libraries like GDAL. On Ubuntu dependencies are managed by apt, and the following commands will update the ‘OSGeo stack’, composed of PROJ, GEOS and GDAL, if changes are detected in the default repositories (from 18.10 onwards):\nsudo apt update # see if things have changed\nsudo apt upgrade # install changes\nThe following commands will upgrade to a newer version of Ubuntu (it may be worth waiting until the point release of Ubuntu 20.04 — 20.04.1 — is released in summer before upgrading if you’re currently running Ubuntu 18.04 if high stability and low set-up times are priorities; also see instructions here):\napt dist-upgrade\nTo get more up-to-date upstream geographic libraries than provided in the default Ubuntu repositories, you can add the ubuntugis repository as follows. This is a pre-requisite on Ubuntu 18.04 and earlier but also works with later versions (warning, adding this repository could cause complications if you already have software such as QGIS that uses a particular version of GDAL installed):\nsudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable\nsudo apt update\nsudo apt upgrade\nThat will give you more up-to-date versions of GDAL, GEOS and PROJ which may offer some performance improvements. Note: if you do update dependencies such as GDAL you will need to re-install the relevant packages, e.g. with install.packages(\"sf\"). You can revert that change with the following little-known command:\nsudo add-apt-repository --remove ppa:ubuntugis/ubuntugis-unstable\nIf you also want the development versions of key R packages, e.g. to test new features and support development efforts, you can install them from GitHub, e.g. as follows:\n\nremotes::install_github(\"r-spatial/sf\")\nremotes::install_github(\"rspatial/raster\")\nremotes::install_github(\"mtennekes/tmaptools\") # required for dev version of tmap\nremotes::install_github(\"mtennekes/tmap\")\n\n\n\n3. Installing geographic R packages on other Linux operating systems\nIf you are in the fortunate position of switching to Linux and being able to choose the distribution that best fits your needs, it’s worth thinking about which distribution will be both user-friendly (more on that soon), performant and future-proof. Ubuntu is a solid choice, with a large user community and repositories such as ‘ubuntugis’ providing more up-to-date versions of upstream geographic libraries such as GDAL.\nQGIS is also well-supported on Ubuntu.\nHowever, you can install R and key geographic packages on other operating systems, although it may take longer. Useful links on installing R and geographic libraries are provided below for reference:\n\nInstalling R on Debian is covered on the CRAN website. Upstream dependencies such as GDAL can be installed on recent versions of Debian, such as buster, with commands such as apt install libgdal-dev as per instructions on the rocker/geospatial.\nInstalling R on Fedora/Red Hat is straightforward, as outlined on CRAN. GDAL and other spatial libraries can be installed from Fedora’s dnf package manager, e.g. as documented here for sf.\nArch Linux has a growing R community. Information on installing and setting-up R can be found on the ArchLinux wiki. Installing upstream dependencies such as GDAL on Arch is also relatively straightforward. There is also a detailed guide for installing R plus geographic packages by Patrick Schratz.\n\n\n\n4. Geographic R packages on Docker\n\n\n\n\n\n\n\nThe Ubuntu installation instructions outlined above provide such an easy and future-proof set-up. But if you want an even easier way to get the power of key geographic packages running on Linux, and have plenty of RAM and HD space, running R on the ‘Docker Engine’ may be an attractive option.\nAdvantages of using Docker include reproducibility (code will always run the same on any given image, and images can be saved), portability (Docker can run on Linux, Windows and Mac) and scalability (Docker provides a platform for scaling-up computations across multiple nodes).\nFor an introduction to using R/RStudio in Docker, see the Rocker project.\nUsing that approach, I recommend the following Docker images for using R as a basis for geographic research:\n\nrocker/geospatial which contains key geographic packages, including those listed above\nrobinlovelace/geocompr which contains all the packages needed to reproduce the contents of the book, and which you can run with the following command in a shell in which Docker is installed:\n\ndocker run -e PASSWORD=yourpassword --rm -p 8787:8787 robinlovelace/geocompr\nTo test-out the Ubuntu 19.10 set-up recommended above I created a Dockerfile and associated image on Dockerhub that you can test-out as follows:\ndocker run -it robinlovelace/geocompr:ubuntu-eoan\nR\nlibrary(sf)\n#&gt; Linking to GEOS 3.7.2, GDAL 2.4.2, PROJ 5.2.0\nlibrary(raster)\nlibrary(tmap) \nThe previous commands should take you to a terminal inside the docker container where you try out the Linux command line and R. If you want to use more cutting-edge versions of the geographic libraries, you can use the ubuntu-bionic image (note the more recent version numbers, with PROJ 7.0.0 for example):\nsudo docker run -it robinlovelace/geocompr:ubuntu-bionic\nR\nlibrary(sf)\n#&gt; Linking to GEOS 3.8.0, GDAL 3.0.4, PROJ 7.0.0\nThese images do not currently contain all the dependencies needed to reproduce the code in Geocomputation with R. \nHowever, as documented in issue 476 in the geocompr GitHub repo, there is a plan to provide Docker images with this full ‘R-spatial’ stack installed, building on strong foundations such as rocker/geospatial and the ubuntugis repositories, to support different versions of GDAL and other dependencies. We welcome any comments or tech support to help make this happen. Suggested changes to this post are also welcome, see the source code here.\n\n\n5. Fin\nR is an open-source language heavily inspired by Unix/Linux so it should come as no surprise that it runs well on a variety of Linux distributions, Ubuntu (covered in this post) in particular. The guidance in this post should get geographic R packages set-up quickly in a future-proof way. A sensible next step is to sharpen you system administration (sysadmin) and shell coding skills, e.g. with reference to Ubuntu wiki pages and Chapter 2 of the open source book Data Science at the Command Line.\nThis will take time but, building on OSGeo libraries, a well set-up Linux machine is an ideal platform to install, run and develop key geographic R packages in a performant, stable and future-proof way. \n\nBe the FOSS4G change you want to see in the world!\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{lovelace2020,\n  author = {Lovelace, Robin},\n  title = {Installing Spatial {R} Packages on {Ubuntu}},\n  date = {2020-03-30},\n  url = {https://geocompx.org/post/2020/installing-r-spatial-packages-linux/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLovelace, Robin. 2020. “Installing Spatial R Packages on\nUbuntu.” March 30, 2020. https://geocompx.org/post/2020/installing-r-spatial-packages-linux/."
  },
  {
    "objectID": "post/2021/geocompr2-bp1/index.html",
    "href": "post/2021/geocompr2-bp1/index.html",
    "title": "Geocomputation with R: Second Edition feedback",
    "section": "",
    "text": "TL;DR: Please help us by filling out the survey at https://forms.gle/nq9RmbxJyZXQgc948.\nIt’s been almost 3 years since the first edition of Geocomputation with R was published back in 2019. It’s been an amazing journey for this open source book on geographic data analysis, visualization and modeling since then. We have reached almost 500k people via the website at https://geocompr.robinlovelace.net/ and the physical book. The book now contributes to many university courses, lectures, and personal development as outlined at https://geocompr.github.io/guestbook/ - where you can add you own comments.\nBuilding on the success of the first edition, and motivated by the need for the material to adapt as spatial ecosystem evolves, we have decided that it is time for a Second Edition. We plan to start work on it over the next few months, aiming for publication around summer 2022. Second edition will be available at https://geocompr.robinlovelace.net/, while you can find the first edition at https://bookdown.org/robinlovelace/geocompr/.\nWe already have plenty of changes, updates, and improvements in mind, as documented in the book’s issue tracker. However, we want to get feedback from the community, to ensure we’re not miss something key and to find out what you most want from a 2nd edition. We’re asking for your help in guiding the future of Geocomputation with R!\nThe book is already much stronger thanks to community, with 50+ people contributing to the codebase already and many more supporting in the issue tracker and the guestbook. Please take a few minutes to let us know your thoughts on the current edition, and suggestions for the next one using the Google form.\nThanks from the Geocomputation with R team,\nRobin Lovelace, Jakub Nowosad, and Jannes Muenchow\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{lovelace,_jakub_nowosad,_jannes_muenchow2021,\n  author = {Lovelace, Jakub Nowosad, Jannes Muenchow, Robin},\n  title = {Geocomputation with {R:} {Second} {Edition} Feedback},\n  date = {2021-09-06},\n  url = {https://geocompx.org/post/2021/geocompr2-bp1/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLovelace, Jakub Nowosad, Jannes Muenchow, Robin. 2021.\n“Geocomputation with R: Second Edition Feedback.” September\n6, 2021. https://geocompx.org/post/2021/geocompr2-bp1/."
  },
  {
    "objectID": "post/2024/cover-competition-results/index.html",
    "href": "post/2024/cover-competition-results/index.html",
    "title": "Geocomputation with R competition: book cover for the 2nd edition",
    "section": "",
    "text": "Note\n\n\n\nThis is a guest blog post written by Benjamin Nowak. He is the winner of the Geocomputation with R book cover competition for the second edition of the book. Congratulations!\n\n\n\nOne page, many things to show\nLike Marco Sciaini, the creator of the cover of the previous edition, I started with the idea that seemed most logical to illustrate the cover of Geocomputation with R: illustrate various geocomputation and mapping techniques on the front page. The hexagons, reminiscent of R packages, were a good idea of Marco’s, but I had to come up with something new.\nThe cover competition arrived approximately at the same time as the release of Alberto Cairo’s The Art of Insight, and I must confess that the beautiful tryptic imagined by Nadieh Bremer for the cover first gave me the idea of trying out a layout with three different maps (one on top of the other). But it was just copying someone else.\nThe next idea then came to me quickly, as it fits well with the way graphics and maps are added as different layers in the same ggplot() object: adding different small maps on one big map.\n\n\nA tribute to useful ressources\nAnother idea I had with this cover was to highlight useful resources for the R community. For this purpose, I used a Tidy Tuesday dataset on energy production by country for the Dorling cartogram that covered Europe in my first proposal.\nAnother very important resource for me are the palette color packages, which save me so much time for my dataviz creation! So for this proposal I used Blake Robert Mills’ MoMAColors packages and scico from Fabio Crameri and Thomas Lin Pedersen.\nThis led me to the following proposal:\n\n\nProposal\n\n\n\n\n\nAlmost there…\nI was delighted when this proposal was chosen by Robin, Jakub and Jannes for the cover of the second edition of Geocomputation with R! But there were still a few elements missing to illustrate the diversity of cartographic objects and methods covered in the book. So, after some discussion and several iterations with the authors, we added:\n\na NDVI raster covering the eastern part of the map\na map of trades over Europe\n\nOn this last point, I had already had the opportunity to transform the FAOStat tables of coffee exports into line vectors, but I have to admit that the solution provided here by Robin, with the od package, was much faster! After one last try, a solution was found to keep a version of Dorling’s cartogram in the background, by keeping the circles on the centroids (which leads to superimpositions but is more consistent with the departures and arrivals of the flows).\nNow, the image contains vector and raster data (which is the focus of the 1st part of the book), shows some visualization techniques (i.e., cartograms; 2nd part of the book), and also shows some geocomputation techniques (i.e., origin-destination lines; 3rd part of the book).\n\n\nThe final version of the book cover image\n\n\n\nOverall, I enjoyed creating this cover, and even more so with the exchanges with the authors and the iterations that led to the final version of the cover. Many thanks to Robin, Jakub and Jannes for allowing me to make a small contribution to this great project!\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{nowak2024,\n  author = {Nowak, Benjamin},\n  title = {Geocomputation with {R} Competition: Book Cover for the 2nd\n    Edition},\n  date = {2024-01-17},\n  url = {https://geocompx.org/post/2024/cover-competition-results/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nNowak, Benjamin. 2024. “Geocomputation with R Competition: Book\nCover for the 2nd Edition.” January 17, 2024. https://geocompx.org/post/2024/cover-competition-results/."
  },
  {
    "objectID": "post/2023/rgdal-retirement/index.html",
    "href": "post/2023/rgdal-retirement/index.html",
    "title": "Upcoming changes to popular R packages for spatial data: what you need to do",
    "section": "",
    "text": "Three popular R packages for spatial data handling won’t be available on CRAN after October 2023.1 These packages are:\n\nrgdal: a package that provides bindings to the GDAL and PROJ libraries. In other words, it gives a capability of reading and writing spatial data formats, creating coordinate reference systems and projecting geometries.\nrgeos: a package that provides bindings to the GEOS library. It allows to perform various spatial operations, such as buffering, clipping, or unioning.\nmaptools: an older set of various tools for manipulating spatial data pre-dating rgdal and rgeos.\n\nAs you can see, two of these packages, rgdal and rgeos link and allow access to external (non-R) spatial libraries for reading and writing spatial data formats and performing spatial operations, such as reprojections and spatial joins.\nIf you are a user or developer of any of these packages, you need to be aware of the upcoming changes. From the user perspective, you won’t be able to install these packages from CRAN soon, and thus your workflows will be broken. On the other hand, if you are a developer and your package uses some of the above packages, it will stop working and you won’t be able to retain (or submit) it to CRAN. Very soon you will need to find some alternatives to these packages. Gladly, most of rgdal, rgeos, and maptools functionality is already available in modern R packages, such as sf and terra, as you will see below."
  },
  {
    "objectID": "post/2023/rgdal-retirement/index.html#the-issue",
    "href": "post/2023/rgdal-retirement/index.html#the-issue",
    "title": "Upcoming changes to popular R packages for spatial data: what you need to do",
    "section": "",
    "text": "Three popular R packages for spatial data handling won’t be available on CRAN after October 2023.1 These packages are:\n\nrgdal: a package that provides bindings to the GDAL and PROJ libraries. In other words, it gives a capability of reading and writing spatial data formats, creating coordinate reference systems and projecting geometries.\nrgeos: a package that provides bindings to the GEOS library. It allows to perform various spatial operations, such as buffering, clipping, or unioning.\nmaptools: an older set of various tools for manipulating spatial data pre-dating rgdal and rgeos.\n\nAs you can see, two of these packages, rgdal and rgeos link and allow access to external (non-R) spatial libraries for reading and writing spatial data formats and performing spatial operations, such as reprojections and spatial joins.\nIf you are a user or developer of any of these packages, you need to be aware of the upcoming changes. From the user perspective, you won’t be able to install these packages from CRAN soon, and thus your workflows will be broken. On the other hand, if you are a developer and your package uses some of the above packages, it will stop working and you won’t be able to retain (or submit) it to CRAN. Very soon you will need to find some alternatives to these packages. Gladly, most of rgdal, rgeos, and maptools functionality is already available in modern R packages, such as sf and terra, as you will see below."
  },
  {
    "objectID": "post/2023/rgdal-retirement/index.html#the-context",
    "href": "post/2023/rgdal-retirement/index.html#the-context",
    "title": "Upcoming changes to popular R packages for spatial data: what you need to do",
    "section": "The context",
    "text": "The context\nrgdal and maptools were first published on CRAN in 2003, and rgeos in 2011. These packages were developed when the spatial data analysis ecosystem in R was still very young, and they were subsequently used for many (ecological) data analysis workflows.\nWriting a popular, complex package is a lot of work, and maintaining it is even much more work. This can be seen, for example, by looking at the release versions of the rgdal package. There were 151 releases of this package between 2003 and 2023, which means that there was a new release every 7 weeks on average. Now, let’s consider that each release requires many hours of (usually voluntary) work.\nrgdal, maptools, and rgeos have a few things in common. While they are independent, they are also closely related to each other – all of them depend on the sp package for spatial data representation in R.2 Another thing they have in common is that currently we have more modern alternatives to them, such as sf (first released in 2016) and terra (2020). Finally, the main author of rgdal, maptools, and rgeos is the same person – Roger Bivand, who retired from his position at the Norwegian School of Economics in 2021.\nThus, given the workloads of maintaining these packages, the existence of modern alternatives, and the retirement of the main author, it was decided to retire these packages.3"
  },
  {
    "objectID": "post/2023/rgdal-retirement/index.html#how-to-test-if-your-code-is-affected",
    "href": "post/2023/rgdal-retirement/index.html#how-to-test-if-your-code-is-affected",
    "title": "Upcoming changes to popular R packages for spatial data: what you need to do",
    "section": "How to test if your code is affected?",
    "text": "How to test if your code is affected?\nThe most basic way to test if your code is affected by the upcoming changes is to check if you use rgdal, rgeos, and maptools in your scripts or as dependencies in your packages. If so – your workflows may be broken soon!\nThere is also a possibility that you use the sp package but not the affected packages. In this case, you may still be touched by the changes, because the sp package had interacted with rgdal and rgeos in the background. For example, if you run the spTransform() function from the sp package, it used the rgdal package in the background. Thus, you may add the following code to your script to check if it will work in the future:\n\noptions(\"sp_evolution_status\" = 2) # use sf instead of rgdal and rgeos in sp\nlibrary(sp)\n\nIf you get an error, it means that your code is affected by the changes.4\nThere is also another possible situation – you are using some packages that depend on the affected packages. This one is the most difficult to check, because you need to inspect the dependencies of all the packages you use.5"
  },
  {
    "objectID": "post/2023/rgdal-retirement/index.html#solutions-for-users",
    "href": "post/2023/rgdal-retirement/index.html#solutions-for-users",
    "title": "Upcoming changes to popular R packages for spatial data: what you need to do",
    "section": "Solutions for users",
    "text": "Solutions for users\nMy first advice would be: do yourself a favor, and just stop using the retired packages today. Write your new scripts using modern alternatives, such as sf or terra. In most cases they are easy to use, and you will be able to find a lot of examples and other resources online.\nIf you have some old scripts that use the retired packages, you have a few options and your decision should depend on your circumstances. If you do not plan to use them in the future, you can just leave them as they are. On the other hand, if you want to perform similar spatial operations, you should consider rewriting the scripts using modern alternatives – see the tables below for the equivalents of the most popular functions from rgdal, rgeos, and maptools.\nYou also may have old sp objects that you want to use in the future. Then, you can convert them to the modern equivalents with functions such as sf::st_as_sf() or terra::vect().6\nFinally, you may decide to still use the retired packages. You can do that by either not updating your R installation and packages, or by creating a docker image with the old R and packages versions. However, this is generally not a good idea, because rgdal, rgeos, and maptools will be archived on CRAN soon, and thus you (and other people) won’t be able to install them directly.7"
  },
  {
    "objectID": "post/2023/rgdal-retirement/index.html#solutions-for-package-developers",
    "href": "post/2023/rgdal-retirement/index.html#solutions-for-package-developers",
    "title": "Upcoming changes to popular R packages for spatial data: what you need to do",
    "section": "Solutions for package developers",
    "text": "Solutions for package developers\nLook at your package dependencies and check if you use any of the affected packages. If that is the case, you need to find alternatives to them. In general, there are two main alternative options8:\n\nsf package, which is a modern alternative to rgdal and rgeos, and also provides a spatial vector data representation in R.\nterra package, which gives support to working with spatial vector and raster data in R.\n\nYou just need to be aware that functions in these packages are not always identical to the ones in the affected packages. They may have different arguments, different defaults, expect different inputs, or return different outputs. For example, rgdal’s readOGR() function returns a Spatial*DataFrame object, while sf’s read_sf() returns an sf object and terra’s vect() returns a SpatVector object.9\n\n\n\n\n\nrgdal\nsf\nterra\n\n\n\n\nreadOGR()\nread_sf()\nvect()\n\n\nwriteOGR()\nwrite_sf()\nwriteVector()\n\n\n\n\n\nThe rgeos functions also have their equivalents in sf and terra.10\nInterestingly, the most often used maptools functions have been already deprecated for a long time and alternative tools for the same purposes existed, e.g., unionSpatialPolygons() could be replaced with rgeos::gUnaryUnion(), and maptools::spRbind() with sp::rbind(). The table below shows their modern substitutes.\n\n\n\n\n\nrgeos\nsf\nterra\n\n\n\n\ngArea()\nst_area()\nexpanse()\n\n\ngBuffer()\nst_buffer()\nbuffer()\n\n\ngCentroid()\nst_centroid()\ncentroids()\n\n\ngDistance()\nst_distance()\ndistance()\n\n\ngIntersection()\nst_intersection()\ncrop()\n\n\ngIntersects()\nst_intersects()\nrelate()\n\n\n\n\n\n\n\n\nmaptools\nsf\nterra\n\n\n\n\nunionSpatialPolygons()\nst_union()\naggregate()\n\n\nspRbind()\nrbind()\nrbind()\n\n\n\n\n\nThe sp package is not going to be removed from CRAN soon, but it is good to stop using it because it is no longer being actively developed. Here you can find some replacements for its basic functions:\n\n\n\n\n\nsp\nsf\nterra\n\n\n\n\nbbox()\nst_bbox()\next()\n\n\ncoordinates()\nst_coordinates()\ncrds()\n\n\nidenticalCRS()\nst_crs(x) == st_crs(y)\ncrs(x) == crs(y)\n\n\nover()\nst_intersects()\nrelate()\n\n\npoint.in.polygon()\nst_intersects()\nrelate()\n\n\nproj4string()\nst_crs()\ncrs()\n\n\nspsample()\nst_sample()\nspatSample()\n\n\nspTransform()\nst_transform()\nproject()\n\n\n\n\n\nYou also may want to visit the comparison table and the migration wiki to get more complete lists of alternative functions to the ones from rgdal, rgeos, and maptools.\nAdditionally, you may still want to accept sp objects as inputs and return them as outputs of your functions. In such case, you can use the sf::st_as_sf() or terra::vect() functions to convert sp objects to the modern equivalents, perform required spatial operations with sf/terra, and then convert the results back to sp objects with as(x, \"Spatial\"). Packages depending on sp would also need to add sf as their weak dependency.11"
  },
  {
    "objectID": "post/2023/rgdal-retirement/index.html#more-resources",
    "href": "post/2023/rgdal-retirement/index.html#more-resources",
    "title": "Upcoming changes to popular R packages for spatial data: what you need to do",
    "section": "More resources",
    "text": "More resources\nYou may use the evolution GitHub repository to ask questions about the retirement process.\nTo learn more about the rgdal, rgeos, and maptools retirement, you can check the following resources:\n\nThe announcement of their retirement\nA blog post series about the retirement process: part 1, part 2, part 3, part 4\nA video about this retirement process: https://youtu.be/TlpjIqTPMCA\nScripts converting code from the Applied Spatial Data Analysis with R 2nd edition book to run without retiring packages: https://github.com/rsbivand/sf_asdar2ed\n\nOn the other hand, if you want to learn more about their alternatives and how to use them, there are many resources available online:\n\nsf package’s official website\nterra package’s documentation\nThe Geocomputation with R book\nThe Spatial Data Science with applications in R book and its section on the retirement process\n\nBy exploring these resources, you will be able to find alternative workflows that suit your needs and continue your work with spatial data in R. Good luck!"
  },
  {
    "objectID": "post/2023/rgdal-retirement/index.html#acknowledgements",
    "href": "post/2023/rgdal-retirement/index.html#acknowledgements",
    "title": "Upcoming changes to popular R packages for spatial data: what you need to do",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nI would like to thank Roger Bivand and Maximilian H.K. Hesselbarth for their comments and suggestions that helped to improve this document."
  },
  {
    "objectID": "post/2023/rgdal-retirement/index.html#footnotes",
    "href": "post/2023/rgdal-retirement/index.html#footnotes",
    "title": "Upcoming changes to popular R packages for spatial data: what you need to do",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTheir retirement was announced almost two years ago.↩︎\nThis they share with the raster package, which was builtd on sp representations and used rgdal and rgeos under the hood. Since September 2022, raster uses terra, not rgdal or rgeos.↩︎\nMore details about the context of these changes may be found in the paper by Bivand (2021).↩︎\nBut if you make the sf package available, spTransform() will use it instead of rgdal. Similarly, many workflows using raster can make terra available, and remove any mention of rgdal or rgeos.↩︎\nHowever, maintainers of these packages have been actively contacted with the information of both the need to act, and suggesting ways to migrate from retiring packages to sf or terra. Many packages depended on rgdal and rgeos because raster used to depend on them; they can safely be replaced by the use by raster of terra “under the hood”.↩︎\nRead more about this in the Conversions between different spatial classes in R and Progress on R-spatial evolution, Apr 2023 blog posts.↩︎\nTheir installation will require downloading source packages from the CRAN package archive, and the providing the external software libraries needed to install rgdal and rgeos from source.↩︎\nThere are also other options, such as using the vapour or geos package, but they are not discussed here.↩︎\nOf course, you can convert these objects back to sp classes and carry on as before, if you choose. However, this choice is most suited to legacy settings rather than actively maintained packages.↩︎\nYou just need to be aware that they may return different values compared to rgeos. For example, sf may use a different algorithm to calculate the area of a polygon for data with a geographical coordinate reference system (CRS).↩︎\nSee the coercion section of the Progress on R-spatial evolution, Apr 2023 blog post.↩︎"
  },
  {
    "objectID": "post/2023/beyond-sf/index.html",
    "href": "post/2023/beyond-sf/index.html",
    "title": "R-spatial beyond sf",
    "section": "",
    "text": "sf has transformed the R ecosystem and helped pave the way for wider adoption of geospatial workflows by R users. The R-spatial ecosystem, however, is much bigger than just sf. There are lower level packages such as geos, wk, and s2 that provide access to geometries and algorithms outside of the context of sf. These packages are often more performant than sf but require a different frame of mind when using them.\nThe goal of this post is to demystify the way in which sf represents geometries and explain how other geometry libraries can be used alongside typical sf workflows."
  },
  {
    "objectID": "post/2023/beyond-sf/index.html#understanding-sf",
    "href": "post/2023/beyond-sf/index.html#understanding-sf",
    "title": "R-spatial beyond sf",
    "section": "Understanding sf",
    "text": "Understanding sf\nMost R users that do geospatial analysis are familiar with sf and understand spatial workflows in that context only. Many users of sf view it as this magical data frame that lets you do spatial analysis—and that is exactly how it feels! This means that sf has done a great job in making spatial analysis feel a lot easier for the vast majority of R users.\n\n\n\n\n\n\nNote\n\n\n\nRead more about sf in Spatial Data Science\n\n\nBut it is good to understand sf in a more fundamental way. sf is named after the Simple Feature Access Standard. Simple features are an agreed upon way to represent “geometric primitives”—things like Points, LineStrings, Polygons, and their Multi- types. The sf package builds a hierarchy off of these. We have sfg, sfc, and sf objects.\n\n\nsfg objects\nAt the core is the sfg class which is the representation of a simple feature geometry. sfg class objects are representations of a simple feature. They are a single geometry at a time. We can think of them as a scalar value (even though R does not have the concept of a scalar).\n\nlibrary(sf)\n#&gt; Linking to GEOS 3.11.1, GDAL 3.6.4, PROJ 9.1.1; sf_use_s2() is TRUE\n\n# create a point\npnt &lt;- st_point(c(0, 10))\n\n# create a line\nln &lt;- st_linestring(matrix(c(0, 1, 0, 0), ncol = 2))\n\nclass(pnt)\n#&gt; [1] \"XY\"    \"POINT\" \"sfg\"\nclass(ln)\n#&gt; [1] \"XY\"         \"LINESTRING\" \"sfg\"\n\nEach scalar value can be used independently which is useful in itself.\n\nst_length(ln)\n#&gt; [1] 1\n\nsfg objects are very simple objects constructed of numeric vectors, matrices, and lists of matrices. They also have no sense of a coordinate reference system (CRS). More often than not, though, we want to have a vector of geometries—one for each element in a dataset, for example. In sf, a vector of geometries is stored in an sfc object.\n\n\nsfc objects\nsfc is short for simple feature column. You might think that you could create a vector of geometries by combining them using c() but that would be wrong\n\nc(pnt, pnt)\n#&gt; MULTIPOINT ((0 10), (0 10))\nc(pnt, ln)\n#&gt; GEOMETRYCOLLECTION (POINT (0 10), LINESTRING (0 0, 1 0))\n\nsfg objects behave like scalars so combining them creates either a Multi- type of the sfg or a geometry collection (another type of simple feature).\nTo create a vector of geometries you must use st_sfc(). st_sfc() is the construct for creating a “simple feature geometry list column.” It lets you also set a number of attributes that are associated with the vector.\n\nst_sfc(pnt, pnt)\n#&gt; Geometry set for 2 features \n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 0 ymin: 10 xmax: 0 ymax: 10\n#&gt; CRS:           NA\n#&gt; POINT (0 10)\n#&gt; POINT (0 10)\n\nEach sfc object contains attributes such as a CRS (optional), bounding box, and precision.\n\nc(\n  st_sfc(pnt, pnt), \n  st_sfc(pnt),\n  st_sfc(ln)\n)\n#&gt; Geometry set for 4 features \n#&gt; Geometry type: GEOMETRY\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 0 ymin: 0 xmax: 1 ymax: 10\n#&gt; CRS:           NA\n#&gt; POINT (0 10)\n#&gt; POINT (0 10)\n#&gt; POINT (0 10)\n#&gt; LINESTRING (0 0, 1 0)\n\nUnder the hood, this is just a list of sfg objects. Remember, it’s not magic!\n\nunclass(st_sfc(pnt, pnt))\n#&gt; [[1]]\n#&gt; POINT (0 10)\n#&gt; \n#&gt; [[2]]\n#&gt; POINT (0 10)\n#&gt; \n#&gt; attr(,\"precision\")\n#&gt; [1] 0\n#&gt; attr(,\"bbox\")\n#&gt; xmin ymin xmax ymax \n#&gt;    0   10    0   10 \n#&gt; attr(,\"crs\")\n#&gt; Coordinate Reference System: NA\n#&gt; attr(,\"n_empty\")\n#&gt; [1] 0\n\nSince sfc objects are vectors, they can be included as a column in a data frame.\n\ndf &lt;- data.frame(\n  geo = st_sfc(pnt, pnt)\n)\n\ndf\n#&gt;       geometry\n#&gt; 1 POINT (0 10)\n#&gt; 2 POINT (0 10)\n\nclass(df)\n#&gt; [1] \"data.frame\""
  },
  {
    "objectID": "post/2023/beyond-sf/index.html#geometry-and-data.frames",
    "href": "post/2023/beyond-sf/index.html#geometry-and-data.frames",
    "title": "R-spatial beyond sf",
    "section": "Geometry and data.frames",
    "text": "Geometry and data.frames\nHaving geometry included in a data frame was a huge win for the R community because this means that attributes can be included along with geometries. Further, geometries in a data frame means that they are dplyr compatible.\nFor example we can create a vector of points from the x and y columns from the diamonds dataset in ggplot2.\n\ndata(diamonds, package = \"ggplot2\")\n\n1pnts &lt;- purrr::map2(\n  diamonds$x, \n  diamonds$y, \n2  function(.x, .y) st_point(c(.x, .y))\n) |&gt; \n3  st_sfc()\n\n\n1\n\nWe iterate over both the x and y columns in the diamonds dataset returning a list\n\n2\n\nEach value of x and y are accessed through the placeholder .x and .y based on their position in the map2() function. st_point() takes a length 2 numeric vector and returns a single sfg POINT object.\n\n3\n\nWe pass the list of sfg objects to create an sfc\n\n\n\n\nWe can included this in a data frame\n\nlibrary(dplyr, warn.conflicts = FALSE)\n\ndmnd &lt;- diamonds |&gt; \n  select(price, clarity) |&gt; \n  bind_cols(geometry = pnts)\n\nhead(dmnd)\n#&gt; # A tibble: 6 × 3\n#&gt;   price clarity    geometry\n#&gt;   &lt;int&gt; &lt;ord&gt;       &lt;POINT&gt;\n#&gt; 1   326 SI2     (3.95 3.98)\n#&gt; 2   326 SI1     (3.89 3.84)\n#&gt; 3   327 VS1     (4.05 4.07)\n#&gt; 4   334 VS2      (4.2 4.23)\n#&gt; 5   335 SI2     (4.34 4.35)\n#&gt; 6   336 VVS2    (3.94 3.96)\n\nNow we have a tibble with price, clarity, and geometry! Huge win! What if we wanted to calculate the average price by clarity and keep the geometries?\n\ndmnd |&gt; \n  group_by(clarity) |&gt; \n  summarise(avg_price = mean(price))\n#&gt; # A tibble: 8 × 2\n#&gt;   clarity avg_price\n#&gt;   &lt;ord&gt;       &lt;dbl&gt;\n#&gt; 1 I1          3924.\n#&gt; 2 SI2         5063.\n#&gt; 3 SI1         3996.\n#&gt; 4 VS2         3925.\n#&gt; 5 VS1         3839.\n#&gt; 6 VVS2        3284.\n#&gt; 7 VVS1        2523.\n#&gt; 8 IF          2865.\n\nUnfortunately, we lose the geometry just like we would for any other column that wasn’t included in the summarise() call. To keep it, we would need to perform an operation on the geometry itself.\n\ndmnd |&gt; \n  group_by(clarity) |&gt; \n  summarise(\n    avg_price = mean(price),\n    geometry = st_union(geometry)\n  )\n#&gt; # A tibble: 8 × 3\n#&gt;   clarity avg_price                                                     geometry\n#&gt;   &lt;ord&gt;       &lt;dbl&gt;                                                 &lt;MULTIPOINT&gt;\n#&gt; 1 I1          3924. ((4.33 4.29), (4.33 4.36), (4.36 4.33), (4.38 4.42), (4.39 …\n#&gt; 2 SI2         5063. ((0 0), (0 6.62), (3.79 3.75), (3.84 3.82), (3.87 3.85), (3…\n#&gt; 3 SI1         3996. ((3.88 3.84), (3.89 3.84), (3.9 3.85), (3.93 3.96), (3.95 3…\n#&gt; 4 VS2         3925. ((0 0), (3.73 3.68), (3.73 3.71), (3.74 3.71), (3.76 3.73),…\n#&gt; 5 VS1         3839. ((0 0), (3.83 3.85), (3.84 3.87), (3.86 3.89), (3.88 3.9), …\n#&gt; 6 VVS2        3284. ((3.83 3.86), (3.85 3.89), (3.85 3.9), (3.85 3.91), (3.86 3…\n#&gt; 7 VVS1        2523. ((0 0), (3.83 3.85), (3.87 3.9), (3.88 3.95), (3.88 3.99), …\n#&gt; 8 IF          2865. ((3.86 3.88), (3.89 3.9), (3.91 3.95), (3.92 3.94), (3.93 3…\n\nWouldn’t it be nice if the geometry knew to do that? Well, that is exactly what sf objects are."
  },
  {
    "objectID": "post/2023/beyond-sf/index.html#sf-objects",
    "href": "post/2023/beyond-sf/index.html#sf-objects",
    "title": "R-spatial beyond sf",
    "section": "sf objects",
    "text": "sf objects\nsf objects are just data frames with a geometry column that is sticky and smart. We can create an sf object if an sfc column is present in a data frame by using st_as_sf().\n\ndmnd_sf &lt;- st_as_sf(dmnd)\n\nDoing this creates an object of class sf. The two things that make an sf object so special are the class sf and the attribute sf_column.\n\n1attr(dmnd_sf, \"sf_column\")\n#&gt; [1] \"geometry\"\n\n\n1\n\nattr() lets us access a single attribute by name This attribute tells us which column is the geometry. Because we have this sf can implement its own methods for common functions like select(), mutate(), aggregate(), group_by(), etc which always keep the attr(x, \"sf_column\") attached to the data frame. Having the class and attribute allow methods like summarise() to be written for the class itself and handle the things like unioning geometry for us.\n\n\n\n\n\ndmnd_sf |&gt; \n  group_by(clarity) |&gt; \n  summarise(avg_price = mean(price))\n#&gt; Simple feature collection with 8 features and 2 fields\n#&gt; Geometry type: MULTIPOINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 0 ymin: 0 xmax: 10.74 ymax: 58.9\n#&gt; CRS:           NA\n#&gt; # A tibble: 8 × 3\n#&gt;   clarity avg_price                                                     geometry\n#&gt;   &lt;ord&gt;       &lt;dbl&gt;                                                 &lt;MULTIPOINT&gt;\n#&gt; 1 I1          3924. ((4.33 4.29), (4.33 4.36), (4.36 4.33), (4.38 4.42), (4.39 …\n#&gt; 2 SI2         5063. ((0 0), (0 6.62), (3.79 3.75), (3.84 3.82), (3.87 3.85), (3…\n#&gt; 3 SI1         3996. ((3.88 3.84), (3.89 3.84), (3.9 3.85), (3.93 3.96), (3.95 3…\n#&gt; 4 VS2         3925. ((0 0), (3.73 3.68), (3.73 3.71), (3.74 3.71), (3.76 3.73),…\n#&gt; 5 VS1         3839. ((0 0), (3.83 3.85), (3.84 3.87), (3.86 3.89), (3.88 3.9), …\n#&gt; 6 VVS2        3284. ((3.83 3.86), (3.85 3.89), (3.85 3.9), (3.85 3.91), (3.86 3…\n#&gt; 7 VVS1        2523. ((0 0), (3.83 3.85), (3.87 3.9), (3.88 3.95), (3.88 3.99), …\n#&gt; 8 IF          2865. ((3.86 3.88), (3.89 3.9), (3.91 3.95), (3.92 3.94), (3.93 3…\n\nNote that this is just like what we wrote earlier except that we didn’t have to handle the geometry column manually. If we compare these two approaches and ignore attribute difference like sf_column and class we can see that they are identical.\n\nx &lt;- dmnd |&gt; \n  group_by(clarity) |&gt; \n  summarise(\n    avg_price = mean(price),\n    geometry = st_union(geometry)\n  )\n\ny &lt;- dmnd_sf |&gt; \n  group_by(clarity) |&gt; \n  summarise(avg_price = mean(price))\n\n1all.equal(x, y, check.attributes = FALSE)\n#&gt; [1] TRUE\n\n\n1\n\nWe ignore attributes because they will be different because the classes are different."
  },
  {
    "objectID": "post/2023/beyond-sf/index.html#other-geometry-vectors",
    "href": "post/2023/beyond-sf/index.html#other-geometry-vectors",
    "title": "R-spatial beyond sf",
    "section": "Other geometry vectors",
    "text": "Other geometry vectors\nKnowing how sf works can be helpful for understanding how we can ease our reliance on it for all geospatial operations. Instead of thinking of the entire sf data frame as the thing that handles all geometry operations we now know that it is the sfc geometry column.\nIn R there are different ways of representing geometry besides sfc vectors. The packages s2, geos, wk, and rsgeo all provide different vectors of geometries that can be used.\nThese libraries are very handy for doing geometric operations. Each of these packages tend to be better at one thing than another, and each have their place. You might find big speed improvements if you opt to use one of these libraries instead of sf for certain things.\nTake for example calculating the length of linestrings on a geodesic. We can use the roxel dataset from {sfnetworks}.\n\ndata(roxel, package = \"sfnetworks\")\n\nTo illustrate, we can extract the geometry column and cast it as an rsgeo class object.\n\nlibrary(rsgeo)\ngeo &lt;- roxel$geometry\nrs &lt;- as_rsgeo(geo)\n\nWe can then use the functions st_length() and length_haversine() respectively to compute the length of linestrings.\n\nbench::mark(\n  st_length(geo),\n  length_haversine(rs),\n1  check = FALSE\n)\n#&gt; # A tibble: 2 × 6\n#&gt;   expression                min   median `itr/sec` mem_alloc `gc/sec`\n#&gt;   &lt;bch:expr&gt;           &lt;bch:tm&gt; &lt;bch:tm&gt;     &lt;dbl&gt; &lt;bch:byt&gt;    &lt;dbl&gt;\n#&gt; 1 st_length(geo)         6.34ms    6.8ms      140.    2.84MB     2.09\n#&gt; 2 length_haversine(rs) 457.01µs  515.3µs     1659.     8.8KB     0\n\n\n1\n\ncheck = FALSE because the distances are ever so slightly different likely due to floating point rounding or the radius of the earth that is used.\n\n\n\n\nThis is markedly faster when using rsgeo. We also do not have to extract the vector and work with it as its own object. Any vector can be included in a data frame, remember?\n\nroxel |&gt; \n  as_tibble() |&gt; \n  mutate(\n    geometry = as_rsgeo(geometry),\n    length = length_haversine(geometry)\n    ) |&gt; \n  select(name, type, length, geometry)\n#&gt; # A tibble: 851 × 4\n#&gt;    name                  type        length\n#&gt;    &lt;chr&gt;                 &lt;fct&gt;        &lt;dbl&gt;\n#&gt;  1 Havixbecker Strasse   residential   28.8\n#&gt;  2 Pienersallee          secondary    108. \n#&gt;  3 Schulte-Bernd-Strasse residential   54.3\n#&gt;  4 &lt;NA&gt;                  path         155. \n#&gt;  5 Welsingheide          residential  209. \n#&gt;  6 &lt;NA&gt;                  footway       63.0\n#&gt;  7 &lt;NA&gt;                  footway       42.2\n#&gt;  8 &lt;NA&gt;                  path          45.3\n#&gt;  9 &lt;NA&gt;                  track        348. \n#&gt; 10 &lt;NA&gt;                  track        145. \n#&gt; # ℹ 841 more rows\n#&gt; # ℹ 1 more variable: geometry &lt;r_LINEST&gt;\n\nThis same approach can work for other libraries such as geos.\n\nlibrary(geos)\n\nroxel |&gt; \n  as_tibble() |&gt; \n  mutate(\n    geometry = as_geos_geometry(geometry),\n    length = geos_length(geometry)\n    ) |&gt; \n  select(name, type, length, geometry)\n#&gt; # A tibble: 851 × 4\n#&gt;    name                  type          length geometry                          \n#&gt;    &lt;chr&gt;                 &lt;fct&gt;          &lt;dbl&gt; &lt;geos_geom&gt;                       \n#&gt;  1 Havixbecker Strasse   residential 0.000331 &lt;LINESTRING (7.53372 51.95556, 7.…\n#&gt;  2 Pienersallee          secondary   0.00101  &lt;LINESTRING (7.53244 51.95422, 7.…\n#&gt;  3 Schulte-Bernd-Strasse residential 0.000505 &lt;LINESTRING (7.53271 51.95209, 7.…\n#&gt;  4 &lt;NA&gt;                  path        0.00201  &lt;LINESTRING [7.5382 51.945...7.54…\n#&gt;  5 Welsingheide          residential 0.00188  &lt;LINESTRING (7.53767 51.9475, 7.5…\n#&gt;  6 &lt;NA&gt;                  footway     0.000585 &lt;LINESTRING (7.54379 51.94733, 7.…\n#&gt;  7 &lt;NA&gt;                  footway     0.000408 &lt;LINESTRING (7.54012 51.94478, 7.…\n#&gt;  8 &lt;NA&gt;                  path        0.000628 &lt;LINESTRING (7.53822 51.94546, 7.…\n#&gt;  9 &lt;NA&gt;                  track       0.00430  &lt;LINESTRING [7.5401 51.945...7.54…\n#&gt; 10 &lt;NA&gt;                  track       0.00162  &lt;LINESTRING [7.5412 51.946...7.54…\n#&gt; # ℹ 841 more rows\n\nIf you plan on doing more than just one operation, it is best to convert to the desired geometry type only once and then use it in subsequent calls. Converting from one geometry type can take some time and add additional unwanted overhead if you’re doing it all the time.\n\nroxy_geos &lt;- roxel |&gt; \n1  st_transform(25832) |&gt;\n2  as_tibble() |&gt;\n3  mutate(geometry = as_geos_geometry(geometry))\n\n\n1\n\nWe project our dataset to use meters\n\n2\n\nWe remove the sf class so we can modify the geometry column\n\n3\n\nWe update the geometry from an sfc to a geos_geometry\n\n\n\n\nWith a geometry column that is a geos_geometry, you can use the functions from the geos package to perform your analyses. And, if doing larger scale analyses, it might be faster and more memory efficient!\n\ngeossf\n\n\n\nroxy_geos |&gt; \n  mutate(length = geos_length(geometry)) |&gt; \n  group_by(type) |&gt; \n  summarise(\n    geometry = geos_unary_union(geos_make_collection(geometry)),\n    avg_len = sum(length)\n  ) \n#&gt; # A tibble: 9 × 3\n#&gt;   type         geometry                                                  avg_len\n#&gt;   &lt;fct&gt;        &lt;geos_geom&gt;                                                 &lt;dbl&gt;\n#&gt; 1 cycleway     &lt;MULTILINESTRING [399010 5756791...400105 5757567]&gt;        2740. \n#&gt; 2 footway      &lt;MULTILINESTRING [398732 5755903...399949 5757266]&gt;        4212. \n#&gt; 3 path         &lt;MULTILINESTRING [398473 5755558...399961 5757747]&gt;        7525. \n#&gt; 4 pedestrian   &lt;LINESTRING (399313.41032 5756903.27258, 399269.10927 57…    44.8\n#&gt; 5 residential  &lt;MULTILINESTRING [398511 5755623...400125 5757523]&gt;       22495. \n#&gt; 6 secondary    &lt;MULTILINESTRING [398476 5755557...400104 5757548]&gt;        3946. \n#&gt; 7 service      &lt;MULTILINESTRING [398470 5755810...399995 5757577]&gt;        6600. \n#&gt; 8 track        &lt;MULTILINESTRING [399654 5755892...399975 5757362]&gt;         974. \n#&gt; 9 unclassified &lt;MULTILINESTRING [398655 5755557...399971 5757726]&gt;        1973.\n\n\n\n\nroxel |&gt; \n  mutate(length = st_length(geometry)) |&gt; \n  group_by(type) |&gt; \n  summarise(avg_len = sum(length)) \n#&gt; Simple feature collection with 9 features and 2 fields\n#&gt; Geometry type: GEOMETRY\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: 7.522594 ymin: 51.94151 xmax: 7.546705 ymax: 51.9612\n#&gt; Geodetic CRS:  WGS 84\n#&gt; # A tibble: 9 × 3\n#&gt;   type         avg_len                                                  geometry\n#&gt;   &lt;fct&gt;            [m]                                            &lt;GEOMETRY [°]&gt;\n#&gt; 1 cycleway      2735.  MULTILINESTRING ((7.535912 51.95764, 7.536254 51.95772, …\n#&gt; 2 footway       4206.  MULTILINESTRING ((7.543791 51.94733, 7.54369 51.94686, 7…\n#&gt; 3 path          7513.  MULTILINESTRING ((7.523003 51.95298, 7.523524 51.95306),…\n#&gt; 4 pedestrian      44.7         LINESTRING (7.534819 51.95371, 7.534176 51.95364)\n#&gt; 5 residential  22457.  MULTILINESTRING ((7.533722 51.95556, 7.533461 51.95576),…\n#&gt; 6 secondary     3939.  MULTILINESTRING ((7.532442 51.95422, 7.53236 51.95377, 7…\n#&gt; 7 service       6589.  MULTILINESTRING ((7.532238 51.95388, 7.532069 51.9535, 7…\n#&gt; 8 track          972.  MULTILINESTRING ((7.540063 51.94468, 7.540338 51.94468, …\n#&gt; 9 unclassified  1968.  MULTILINESTRING ((7.534918 51.94681, 7.535121 51.94688),…"
  },
  {
    "objectID": "post/2023/beyond-sf/index.html#continuing-your-r-spatial-journey",
    "href": "post/2023/beyond-sf/index.html#continuing-your-r-spatial-journey",
    "title": "R-spatial beyond sf",
    "section": "Continuing your R-spatial journey",
    "text": "Continuing your R-spatial journey\nWith this, I encourage you to look at other geometry libraries available in the R ecosystem. Some of the notable ones are:\n\ns2 for spherical geometry operations with google s2\ngeos bindings to the powerful libgeos C library\nrsgeo bindings to the GeoRust geo-types and geo crates\nwk for low level generic geometry interface with C and C++ APIs\n\nEach library can convert to and from sf geometries as well.\nHappy programming!"
  },
  {
    "objectID": "post/2023/map-cover-competition/index.html",
    "href": "post/2023/map-cover-competition/index.html",
    "title": "Geocomputation with R comptetition: book cover for the 2nd edition",
    "section": "",
    "text": "Introduction\nThe 2nd edition of Geocomputation with R is due to be published in 2024. Now, we’re looking for a new cover image and we’d like your help. The competition is open to all and we have some prizes (see below).\nWe’re launching this map competition on 1st December, the day after the #30DayMapChallenge finished, to keep the map-making momentum going (and to provide an outlet for anyone suffering from withdrawal symptoms)! Like the 30 Day Map Challenge this competition is set up in the spirit of sharing ideas and fun. Unlike the challenge you have up to 31 days to create a single map, set of maps or other visualisation that could go on the front cover of this popular [open source] and community-driven book (questions are welcome via GitHub issues/discussions and on the geocompx Discord server).\n\n\nA bit of history\nMarco Sciaini won the competition for the book cover of the first edition of Geocomputation with R. His winning entry is shown on the left below. After some iteration on this winning idea, the concept was refined, resulting in a reproducible image (see its source code at github.com/geocompx/geocompr/) (center). Then, the image was incorporated into the official cover of the book’s first edition (right).\n\n\n\n\n\n\n\n\nSubmission\n\n\n\n\n\n\n\nFinal\n\n\n\n\n\n\n\nBook cover\n\n\n\n\n\n\n\n\nCompetition\nThe competition has the following rules:\n\nThe image must be (somewhat) related to Geocomputation with R\nThe winning image/concept must be based on reproducible code\nThe deadline for submissions is 31st December 2023\n\nSubmissions will be evaluated by us, the authors of Geocomputation with R, based on:\n\nHow good we think the image will look as a front cover for the second edition of Geocomputation with R\nCreativity and originality\nQuality of the code\n\n\n\nHow to enter\nTo enter, please submit your idea via GitHub (by sharing your idea as a comment in issue #980), by email or find us on social media: Robin, Jakub.\n\n\nPrizes\nThe winner will receive a $150 voucher for Taylor and Francis books, a copy of the second edition of Geocomputation with R and will be credited in the book. A runner-up will receive a copy of the book.\nWe look forward to seeing your ideas!\n\n\n\n\nReuseCC BY 4.0CitationBibTeX citation:@online{lovelace,_jakub_nowosad,_jannes_muenchow2023,\n  author = {Lovelace, Jakub Nowosad, Jannes Muenchow, Robin},\n  title = {Geocomputation with {R} Comptetition: Book Cover for the 2nd\n    Edition},\n  date = {2023-12-01},\n  url = {https://geocompx.org/post/2023/map-cover-competition/},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nLovelace, Jakub Nowosad, Jannes Muenchow, Robin. 2023.\n“Geocomputation with R Comptetition: Book Cover for the 2nd\nEdition.” December 1, 2023. https://geocompx.org/post/2023/map-cover-competition/."
  }
]